{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<module 'models.reg_cgan_model' from '/Users/zhongsheng/Documents/GitWorkspace/RegCGAN/models/reg_cgan_model.py'>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import dataset, metrics, plotting, config\n",
    "from models import reg_cgan_model\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(metrics)\n",
    "importlib.reload(plotting)\n",
    "importlib.reload(config)\n",
    "importlib.reload(reg_cgan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../figures/magical_sinus already exists replacing files in this notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_config = config.DatasetConfig(scenario=\"magical_sinus\", n_instance=250)\n",
    "\n",
    "assert(dataset_config.scenario == \"magical_sinus\"\n",
    "       or dataset_config.scenario == \"hdpe\"\n",
    "      )\n",
    "fig_dir = f\"../figures/{dataset_config.scenario}\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(fig_dir)\n",
    "    print(f\"Directory {fig_dir} created \") \n",
    "except FileExistsError:\n",
    "    print(f\"Directory {fig_dir} already exists replacing files in this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config = config.Config(\n",
    "    model=config.ModelConfig(activation=\"elu\", lr_gen=0.0001, lr_disc=0.001, optim_gen=\"Adam\",\n",
    "                             optim_disc=\"Adam\", z_input_size=5),\n",
    "    training=config.TrainingConfig(n_epochs=10000, batch_size=100, n_sampling=200),\n",
    "    dataset=dataset_config,\n",
    "    run=config.RunConfig(save_fig=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(exp_config.model.random_seed)\n",
    "random.seed(exp_config.model.random_seed)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(exp_config.model.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, _, _ = dataset.get_dataset(exp_config.dataset.n_instance, exp_config.dataset.scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfe7bc23d0134354bb1cf6a4a02083ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/GPy/kern/src/stationary.py:168: RuntimeWarning:overflow encountered in true_divide\n",
      " /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/GPy/kern/src/rbf.py:52: RuntimeWarning:overflow encountered in square\n",
      " /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/GPy/kern/src/rbf.py:76: RuntimeWarning:invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "import GPy\n",
    "\n",
    "noise = 4.3\n",
    "length = 16\n",
    "\n",
    "run_hyperopt_search = True\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=2, variance=noise, lengthscale=length)\n",
    "gpr = GPy.models.GPRegression(X_train, y_train, kernel)\n",
    "if run_hyperopt_search:\n",
    "    gpr.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Construct CGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Generator_input_x (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Generator_input_z (InputLayer)  (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           60          Generator_input_x[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 20)           120         Generator_input_z[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 40)           0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 60)           2460        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 60)           3660        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 60)           3660        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            61          dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,021\n",
      "Trainable params: 10,021\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discriminator_input_x (InputLay (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator_input_y (InputLay (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           60          Discriminator_input_x[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           40          Discriminator_input_y[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 60)           2460        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 60)           3660        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 60)           3660        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            61          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,882\n",
      "Trainable params: 9,941\n",
      "Non-trainable params: 9,941\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch: 0 / dLoss: 1.4127050638198853 / gLoss: 0.6287721991539001\n",
      "Epoch: 1 / dLoss: 1.144094705581665 / gLoss: 0.6757122278213501\n",
      "Epoch: 2 / dLoss: 0.9644745588302612 / gLoss: 0.7702928781509399\n",
      "Epoch: 3 / dLoss: 0.8449214696884155 / gLoss: 0.8490868210792542\n",
      "Epoch: 4 / dLoss: 0.7852710485458374 / gLoss: 0.9379416704177856\n",
      "Epoch: 5 / dLoss: 0.6832159757614136 / gLoss: 1.0646655559539795\n",
      "Epoch: 6 / dLoss: 0.6533118486404419 / gLoss: 1.175260066986084\n",
      "Epoch: 7 / dLoss: 0.6846393346786499 / gLoss: 1.2394654750823975\n",
      "Epoch: 8 / dLoss: 0.5337575674057007 / gLoss: 1.636114239692688\n",
      "Epoch: 9 / dLoss: 0.6505097150802612 / gLoss: 1.4730780124664307\n",
      "Epoch: 10 / dLoss: 0.5538200736045837 / gLoss: 1.835392951965332\n",
      "Epoch: 11 / dLoss: 0.6637328267097473 / gLoss: 1.739973783493042\n",
      "Epoch: 12 / dLoss: 0.6912514567375183 / gLoss: 1.8976553678512573\n",
      "Epoch: 13 / dLoss: 0.6086078882217407 / gLoss: 1.8522011041641235\n",
      "Epoch: 14 / dLoss: 0.6428689360618591 / gLoss: 1.9188884496688843\n",
      "Epoch: 15 / dLoss: 0.7241548299789429 / gLoss: 1.8655378818511963\n",
      "Epoch: 16 / dLoss: 0.7468894720077515 / gLoss: 1.9382274150848389\n",
      "Epoch: 17 / dLoss: 0.8165518045425415 / gLoss: 1.7459514141082764\n",
      "Epoch: 18 / dLoss: 0.907415509223938 / gLoss: 1.841736912727356\n",
      "Epoch: 19 / dLoss: 0.5822283029556274 / gLoss: 1.935461163520813\n",
      "Epoch: 20 / dLoss: 0.9667148590087891 / gLoss: 1.6273071765899658\n",
      "Epoch: 21 / dLoss: 0.8736690282821655 / gLoss: 1.6737045049667358\n",
      "Epoch: 22 / dLoss: 1.0794167518615723 / gLoss: 1.4160895347595215\n",
      "Epoch: 23 / dLoss: 1.1565742492675781 / gLoss: 1.2933250665664673\n",
      "Epoch: 24 / dLoss: 1.0453228950500488 / gLoss: 1.3632937669754028\n",
      "Epoch: 25 / dLoss: 0.9983484745025635 / gLoss: 1.347266435623169\n",
      "Epoch: 26 / dLoss: 1.2184289693832397 / gLoss: 1.187412977218628\n",
      "Epoch: 27 / dLoss: 1.1817747354507446 / gLoss: 1.2317067384719849\n",
      "Epoch: 28 / dLoss: 1.332564115524292 / gLoss: 1.0426472425460815\n",
      "Epoch: 29 / dLoss: 1.2471115589141846 / gLoss: 1.0495744943618774\n",
      "Epoch: 30 / dLoss: 1.261521339416504 / gLoss: 1.0627541542053223\n",
      "Epoch: 31 / dLoss: 1.2317707538604736 / gLoss: 0.974844217300415\n",
      "Epoch: 32 / dLoss: 1.3359577655792236 / gLoss: 0.9261133670806885\n",
      "Epoch: 33 / dLoss: 1.3100762367248535 / gLoss: 0.9082719683647156\n",
      "Epoch: 34 / dLoss: 1.3385095596313477 / gLoss: 0.9239448308944702\n",
      "Epoch: 35 / dLoss: 1.331476092338562 / gLoss: 0.9104775786399841\n",
      "Epoch: 36 / dLoss: 1.3202475309371948 / gLoss: 0.8844098448753357\n",
      "Epoch: 37 / dLoss: 1.4462549686431885 / gLoss: 0.8733047246932983\n",
      "Epoch: 38 / dLoss: 1.297890067100525 / gLoss: 0.8623961806297302\n",
      "Epoch: 39 / dLoss: 1.38022780418396 / gLoss: 0.8430805802345276\n",
      "Epoch: 40 / dLoss: 1.4392201900482178 / gLoss: 0.8025956749916077\n",
      "Epoch: 41 / dLoss: 1.3834189176559448 / gLoss: 0.8205335140228271\n",
      "Epoch: 42 / dLoss: 1.3585054874420166 / gLoss: 0.8264157772064209\n",
      "Epoch: 43 / dLoss: 1.4004898071289062 / gLoss: 0.7706871628761292\n",
      "Epoch: 44 / dLoss: 1.3751649856567383 / gLoss: 0.7832944989204407\n",
      "Epoch: 45 / dLoss: 1.429595947265625 / gLoss: 0.7196330428123474\n",
      "Epoch: 46 / dLoss: 1.4511091709136963 / gLoss: 0.7376566529273987\n",
      "Epoch: 47 / dLoss: 1.3523911237716675 / gLoss: 0.7694517374038696\n",
      "Epoch: 48 / dLoss: 1.4470641613006592 / gLoss: 0.7552691698074341\n",
      "Epoch: 49 / dLoss: 1.3836467266082764 / gLoss: 0.7673549652099609\n",
      "Epoch: 50 / dLoss: 1.412538766860962 / gLoss: 0.735046923160553\n",
      "Epoch: 51 / dLoss: 1.4252287149429321 / gLoss: 0.7273322343826294\n",
      "Epoch: 52 / dLoss: 1.386851191520691 / gLoss: 0.7353866100311279\n",
      "Epoch: 53 / dLoss: 1.4263619184494019 / gLoss: 0.7175206542015076\n",
      "Epoch: 54 / dLoss: 1.4039340019226074 / gLoss: 0.7339785695075989\n",
      "Epoch: 55 / dLoss: 1.4007527828216553 / gLoss: 0.7271240949630737\n",
      "Epoch: 56 / dLoss: 1.3835934400558472 / gLoss: 0.741504967212677\n",
      "Epoch: 57 / dLoss: 1.3822309970855713 / gLoss: 0.7465804219245911\n",
      "Epoch: 58 / dLoss: 1.3878920078277588 / gLoss: 0.7448387742042542\n",
      "Epoch: 59 / dLoss: 1.3772040605545044 / gLoss: 0.7341377139091492\n",
      "Epoch: 60 / dLoss: 1.380759835243225 / gLoss: 0.7243373394012451\n",
      "Epoch: 61 / dLoss: 1.374950885772705 / gLoss: 0.7303735613822937\n",
      "Epoch: 62 / dLoss: 1.349279761314392 / gLoss: 0.7454144358634949\n",
      "Epoch: 63 / dLoss: 1.3732497692108154 / gLoss: 0.741263747215271\n",
      "Epoch: 64 / dLoss: 1.3655619621276855 / gLoss: 0.7452918291091919\n",
      "Epoch: 65 / dLoss: 1.3687572479248047 / gLoss: 0.744926393032074\n",
      "Epoch: 66 / dLoss: 1.3441753387451172 / gLoss: 0.7517909407615662\n",
      "Epoch: 67 / dLoss: 1.3522167205810547 / gLoss: 0.7502272129058838\n",
      "Epoch: 68 / dLoss: 1.3605189323425293 / gLoss: 0.7416746020317078\n",
      "Epoch: 69 / dLoss: 1.3808294534683228 / gLoss: 0.7440956830978394\n",
      "Epoch: 70 / dLoss: 1.3479390144348145 / gLoss: 0.7468893527984619\n",
      "Epoch: 71 / dLoss: 1.3823174238204956 / gLoss: 0.7426596283912659\n",
      "Epoch: 72 / dLoss: 1.3853588104248047 / gLoss: 0.7346860766410828\n",
      "Epoch: 73 / dLoss: 1.3283774852752686 / gLoss: 0.7515207529067993\n",
      "Epoch: 74 / dLoss: 1.397024154663086 / gLoss: 0.7286694049835205\n",
      "Epoch: 75 / dLoss: 1.3559584617614746 / gLoss: 0.7288259267807007\n",
      "Epoch: 76 / dLoss: 1.3577709197998047 / gLoss: 0.7274133563041687\n",
      "Epoch: 77 / dLoss: 1.386881947517395 / gLoss: 0.7385694980621338\n",
      "Epoch: 78 / dLoss: 1.3557963371276855 / gLoss: 0.7332027554512024\n",
      "Epoch: 79 / dLoss: 1.385087013244629 / gLoss: 0.7357482314109802\n",
      "Epoch: 80 / dLoss: 1.3398687839508057 / gLoss: 0.7306935787200928\n",
      "Epoch: 81 / dLoss: 1.3784980773925781 / gLoss: 0.7118707895278931\n",
      "Epoch: 82 / dLoss: 1.354914665222168 / gLoss: 0.731557309627533\n",
      "Epoch: 83 / dLoss: 1.3453214168548584 / gLoss: 0.7441691756248474\n",
      "Epoch: 84 / dLoss: 1.3646708726882935 / gLoss: 0.7344592809677124\n",
      "Epoch: 85 / dLoss: 1.3692350387573242 / gLoss: 0.7221998572349548\n",
      "Epoch: 86 / dLoss: 1.3561325073242188 / gLoss: 0.7229217290878296\n",
      "Epoch: 87 / dLoss: 1.3776015043258667 / gLoss: 0.7174532413482666\n",
      "Epoch: 88 / dLoss: 1.3715040683746338 / gLoss: 0.7173957824707031\n",
      "Epoch: 89 / dLoss: 1.3767489194869995 / gLoss: 0.7096941471099854\n",
      "Epoch: 90 / dLoss: 1.4105144739151 / gLoss: 0.6877529621124268\n",
      "Epoch: 91 / dLoss: 1.3979430198669434 / gLoss: 0.6913341283798218\n",
      "Epoch: 92 / dLoss: 1.3909673690795898 / gLoss: 0.7107593417167664\n",
      "Epoch: 93 / dLoss: 1.369720220565796 / gLoss: 0.736586332321167\n",
      "Epoch: 94 / dLoss: 1.3897972106933594 / gLoss: 0.735721230506897\n",
      "Epoch: 95 / dLoss: 1.370385766029358 / gLoss: 0.731715977191925\n",
      "Epoch: 96 / dLoss: 1.3982230424880981 / gLoss: 0.7197539806365967\n",
      "Epoch: 97 / dLoss: 1.3732284307479858 / gLoss: 0.7253901958465576\n",
      "Epoch: 98 / dLoss: 1.3934581279754639 / gLoss: 0.7153211236000061\n",
      "Epoch: 99 / dLoss: 1.362474799156189 / gLoss: 0.729993999004364\n",
      "Epoch: 100 / dLoss: 1.4008358716964722 / gLoss: 0.7168484330177307\n",
      "Epoch: 101 / dLoss: 1.3675681352615356 / gLoss: 0.7283523678779602\n",
      "Epoch: 102 / dLoss: 1.4203176498413086 / gLoss: 0.688586950302124\n",
      "Epoch: 103 / dLoss: 1.3644485473632812 / gLoss: 0.7229255437850952\n",
      "Epoch: 104 / dLoss: 1.3887944221496582 / gLoss: 0.7118077278137207\n",
      "Epoch: 105 / dLoss: 1.4049745798110962 / gLoss: 0.7053688764572144\n",
      "Epoch: 106 / dLoss: 1.399172306060791 / gLoss: 0.7127458453178406\n",
      "Epoch: 107 / dLoss: 1.4038145542144775 / gLoss: 0.7182259559631348\n",
      "Epoch: 108 / dLoss: 1.3714978694915771 / gLoss: 0.7258022427558899\n",
      "Epoch: 109 / dLoss: 1.4000186920166016 / gLoss: 0.7073702812194824\n",
      "Epoch: 110 / dLoss: 1.3833460807800293 / gLoss: 0.7234426736831665\n",
      "Epoch: 111 / dLoss: 1.3928782939910889 / gLoss: 0.7126433849334717\n",
      "Epoch: 112 / dLoss: 1.393239974975586 / gLoss: 0.6904866695404053\n",
      "Epoch: 113 / dLoss: 1.3810555934906006 / gLoss: 0.7097077369689941\n",
      "Epoch: 114 / dLoss: 1.3810267448425293 / gLoss: 0.7023957967758179\n",
      "Epoch: 115 / dLoss: 1.378104567527771 / gLoss: 0.6946431994438171\n",
      "Epoch: 116 / dLoss: 1.3697733879089355 / gLoss: 0.7008121609687805\n",
      "Epoch: 117 / dLoss: 1.3737363815307617 / gLoss: 0.7033267021179199\n",
      "Epoch: 118 / dLoss: 1.375030279159546 / gLoss: 0.715735673904419\n",
      "Epoch: 119 / dLoss: 1.3761508464813232 / gLoss: 0.6971588134765625\n",
      "Epoch: 120 / dLoss: 1.3740730285644531 / gLoss: 0.7167668342590332\n",
      "Epoch: 121 / dLoss: 1.3653957843780518 / gLoss: 0.6975634694099426\n",
      "Epoch: 122 / dLoss: 1.3743469715118408 / gLoss: 0.7017083168029785\n",
      "Epoch: 123 / dLoss: 1.3728384971618652 / gLoss: 0.7160553932189941\n",
      "Epoch: 124 / dLoss: 1.3498939275741577 / gLoss: 0.7191658616065979\n",
      "Epoch: 125 / dLoss: 1.354200005531311 / gLoss: 0.7185981869697571\n",
      "Epoch: 126 / dLoss: 1.3449146747589111 / gLoss: 0.7190908193588257\n",
      "Epoch: 127 / dLoss: 1.348282814025879 / gLoss: 0.705751895904541\n",
      "Epoch: 128 / dLoss: 1.3526275157928467 / gLoss: 0.7025250792503357\n",
      "Epoch: 129 / dLoss: 1.344435691833496 / gLoss: 0.7020540833473206\n",
      "Epoch: 130 / dLoss: 1.3491840362548828 / gLoss: 0.7155233025550842\n",
      "Epoch: 131 / dLoss: 1.3474459648132324 / gLoss: 0.7097527384757996\n",
      "Epoch: 132 / dLoss: 1.3333019018173218 / gLoss: 0.7411970496177673\n",
      "Epoch: 133 / dLoss: 1.334120750427246 / gLoss: 0.7253910899162292\n",
      "Epoch: 134 / dLoss: 1.343961238861084 / gLoss: 0.7292844653129578\n",
      "Epoch: 135 / dLoss: 1.3505442142486572 / gLoss: 0.7172077894210815\n",
      "Epoch: 136 / dLoss: 1.3433563709259033 / gLoss: 0.7451235055923462\n",
      "Epoch: 137 / dLoss: 1.3640403747558594 / gLoss: 0.7148009538650513\n",
      "Epoch: 138 / dLoss: 1.339428424835205 / gLoss: 0.7274094223976135\n",
      "Epoch: 139 / dLoss: 1.322558045387268 / gLoss: 0.731049656867981\n",
      "Epoch: 140 / dLoss: 1.3566814661026 / gLoss: 0.7170330286026001\n",
      "Epoch: 141 / dLoss: 1.3454313278198242 / gLoss: 0.7094336152076721\n",
      "Epoch: 142 / dLoss: 1.3435678482055664 / gLoss: 0.7256522178649902\n",
      "Epoch: 143 / dLoss: 1.3498047590255737 / gLoss: 0.7373405694961548\n",
      "Epoch: 144 / dLoss: 1.3140764236450195 / gLoss: 0.7454013228416443\n",
      "Epoch: 145 / dLoss: 1.3464971780776978 / gLoss: 0.7288153171539307\n",
      "Epoch: 146 / dLoss: 1.3543338775634766 / gLoss: 0.7287921905517578\n",
      "Epoch: 147 / dLoss: 1.3068561553955078 / gLoss: 0.7713549137115479\n",
      "Epoch: 148 / dLoss: 1.3556456565856934 / gLoss: 0.7115355134010315\n",
      "Epoch: 149 / dLoss: 1.3626205921173096 / gLoss: 0.7072909474372864\n",
      "Epoch: 150 / dLoss: 1.3599828481674194 / gLoss: 0.6977844834327698\n",
      "Epoch: 151 / dLoss: 1.341741919517517 / gLoss: 0.7338861227035522\n",
      "Epoch: 152 / dLoss: 1.3431060314178467 / gLoss: 0.7153181433677673\n",
      "Epoch: 153 / dLoss: 1.3392724990844727 / gLoss: 0.7454084753990173\n",
      "Epoch: 154 / dLoss: 1.369995355606079 / gLoss: 0.7070066928863525\n",
      "Epoch: 155 / dLoss: 1.3471301794052124 / gLoss: 0.7385562062263489\n",
      "Epoch: 156 / dLoss: 1.3594748973846436 / gLoss: 0.7295878529548645\n",
      "Epoch: 157 / dLoss: 1.3619916439056396 / gLoss: 0.7003979682922363\n",
      "Epoch: 158 / dLoss: 1.3587387800216675 / gLoss: 0.7388948202133179\n",
      "Epoch: 159 / dLoss: 1.3638653755187988 / gLoss: 0.7132893204689026\n",
      "Epoch: 160 / dLoss: 1.3528931140899658 / gLoss: 0.7113208770751953\n",
      "Epoch: 161 / dLoss: 1.3559842109680176 / gLoss: 0.701056718826294\n",
      "Epoch: 162 / dLoss: 1.330552339553833 / gLoss: 0.733826756477356\n",
      "Epoch: 163 / dLoss: 1.3264672756195068 / gLoss: 0.7199608087539673\n",
      "Epoch: 164 / dLoss: 1.298380732536316 / gLoss: 0.7370768785476685\n",
      "Epoch: 165 / dLoss: 1.3534488677978516 / gLoss: 0.7221441864967346\n",
      "Epoch: 166 / dLoss: 1.3599553108215332 / gLoss: 0.7262745499610901\n",
      "Epoch: 167 / dLoss: 1.3473608493804932 / gLoss: 0.7237653136253357\n",
      "Epoch: 168 / dLoss: 1.3433942794799805 / gLoss: 0.7210072875022888\n",
      "Epoch: 169 / dLoss: 1.349245309829712 / gLoss: 0.7533888816833496\n",
      "Epoch: 170 / dLoss: 1.3244497776031494 / gLoss: 0.7304064035415649\n",
      "Epoch: 171 / dLoss: 1.353453278541565 / gLoss: 0.7291796207427979\n",
      "Epoch: 172 / dLoss: 1.3308892250061035 / gLoss: 0.7370179891586304\n",
      "Epoch: 173 / dLoss: 1.3243701457977295 / gLoss: 0.7061815857887268\n",
      "Epoch: 174 / dLoss: 1.329345464706421 / gLoss: 0.7233249545097351\n",
      "Epoch: 175 / dLoss: 1.3414957523345947 / gLoss: 0.7232333421707153\n",
      "Epoch: 176 / dLoss: 1.3300939798355103 / gLoss: 0.7231230139732361\n",
      "Epoch: 177 / dLoss: 1.3326895236968994 / gLoss: 0.7273895740509033\n",
      "Epoch: 178 / dLoss: 1.316146731376648 / gLoss: 0.7484354376792908\n",
      "Epoch: 179 / dLoss: 1.3192462921142578 / gLoss: 0.7361729145050049\n",
      "Epoch: 180 / dLoss: 1.3293933868408203 / gLoss: 0.7327611446380615\n",
      "Epoch: 181 / dLoss: 1.344590187072754 / gLoss: 0.7272270321846008\n",
      "Epoch: 182 / dLoss: 1.3596264123916626 / gLoss: 0.7432089447975159\n",
      "Epoch: 183 / dLoss: 1.2971432209014893 / gLoss: 0.7573075890541077\n",
      "Epoch: 184 / dLoss: 1.3190956115722656 / gLoss: 0.7429795861244202\n",
      "Epoch: 185 / dLoss: 1.300832748413086 / gLoss: 0.7293288707733154\n",
      "Epoch: 186 / dLoss: 1.3405513763427734 / gLoss: 0.7384090423583984\n",
      "Epoch: 187 / dLoss: 1.348905324935913 / gLoss: 0.7643928527832031\n",
      "Epoch: 188 / dLoss: 1.3281517028808594 / gLoss: 0.7685559988021851\n",
      "Epoch: 189 / dLoss: 1.3323830366134644 / gLoss: 0.7324287295341492\n",
      "Epoch: 190 / dLoss: 1.312860369682312 / gLoss: 0.7473368048667908\n",
      "Epoch: 191 / dLoss: 1.320319652557373 / gLoss: 0.7454211711883545\n",
      "Epoch: 192 / dLoss: 1.3041595220565796 / gLoss: 0.7442248463630676\n",
      "Epoch: 193 / dLoss: 1.334657907485962 / gLoss: 0.7846499085426331\n",
      "Epoch: 194 / dLoss: 1.3393597602844238 / gLoss: 0.748486340045929\n",
      "Epoch: 195 / dLoss: 1.2822279930114746 / gLoss: 0.8091589212417603\n",
      "Epoch: 196 / dLoss: 1.342787504196167 / gLoss: 0.7621487379074097\n",
      "Epoch: 197 / dLoss: 1.2949789762496948 / gLoss: 0.7794198393821716\n",
      "Epoch: 198 / dLoss: 1.3574252128601074 / gLoss: 0.742363452911377\n",
      "Epoch: 199 / dLoss: 1.334528923034668 / gLoss: 0.745978593826294\n",
      "Epoch: 200 / dLoss: 1.3344640731811523 / gLoss: 0.7482760548591614\n",
      "Epoch: 201 / dLoss: 1.3481230735778809 / gLoss: 0.7576274275779724\n",
      "Epoch: 202 / dLoss: 1.316688060760498 / gLoss: 0.7676859498023987\n",
      "Epoch: 203 / dLoss: 1.3161050081253052 / gLoss: 0.7953438758850098\n",
      "Epoch: 204 / dLoss: 1.2965779304504395 / gLoss: 0.7622442841529846\n",
      "Epoch: 205 / dLoss: 1.3075510263442993 / gLoss: 0.7906777262687683\n",
      "Epoch: 206 / dLoss: 1.2872726917266846 / gLoss: 0.7663236260414124\n",
      "Epoch: 207 / dLoss: 1.331531286239624 / gLoss: 0.7911490797996521\n",
      "Epoch: 208 / dLoss: 1.3231608867645264 / gLoss: 0.7603346109390259\n",
      "Epoch: 209 / dLoss: 1.3403525352478027 / gLoss: 0.7402528524398804\n",
      "Epoch: 210 / dLoss: 1.3242738246917725 / gLoss: 0.7457606792449951\n",
      "Epoch: 211 / dLoss: 1.3307445049285889 / gLoss: 0.740638017654419\n",
      "Epoch: 212 / dLoss: 1.3184447288513184 / gLoss: 0.8243924975395203\n",
      "Epoch: 213 / dLoss: 1.3249108791351318 / gLoss: 0.7779980301856995\n",
      "Epoch: 214 / dLoss: 1.307262659072876 / gLoss: 0.7633110284805298\n",
      "Epoch: 215 / dLoss: 1.3199350833892822 / gLoss: 0.7770955562591553\n",
      "Epoch: 216 / dLoss: 1.3193854093551636 / gLoss: 0.7644388675689697\n",
      "Epoch: 217 / dLoss: 1.309147834777832 / gLoss: 0.748126745223999\n",
      "Epoch: 218 / dLoss: 1.3154878616333008 / gLoss: 0.7484433054924011\n",
      "Epoch: 219 / dLoss: 1.3027023077011108 / gLoss: 0.7733854651451111\n",
      "Epoch: 220 / dLoss: 1.2914559841156006 / gLoss: 0.7516965270042419\n",
      "Epoch: 221 / dLoss: 1.3295618295669556 / gLoss: 0.7160217761993408\n",
      "Epoch: 222 / dLoss: 1.2983324527740479 / gLoss: 0.7676456570625305\n",
      "Epoch: 223 / dLoss: 1.2991461753845215 / gLoss: 0.7491114735603333\n",
      "Epoch: 224 / dLoss: 1.3344521522521973 / gLoss: 0.7182565331459045\n",
      "Epoch: 225 / dLoss: 1.2993183135986328 / gLoss: 0.813778281211853\n",
      "Epoch: 226 / dLoss: 1.318026065826416 / gLoss: 0.7680737972259521\n",
      "Epoch: 227 / dLoss: 1.3712224960327148 / gLoss: 0.7693672776222229\n",
      "Epoch: 228 / dLoss: 1.3012850284576416 / gLoss: 0.7651413083076477\n",
      "Epoch: 229 / dLoss: 1.3306338787078857 / gLoss: 0.7404779195785522\n",
      "Epoch: 230 / dLoss: 1.3203620910644531 / gLoss: 0.7988237142562866\n",
      "Epoch: 231 / dLoss: 1.3023011684417725 / gLoss: 0.7648410201072693\n",
      "Epoch: 232 / dLoss: 1.3458333015441895 / gLoss: 0.7376164197921753\n",
      "Epoch: 233 / dLoss: 1.3181345462799072 / gLoss: 0.7693963646888733\n",
      "Epoch: 234 / dLoss: 1.280602216720581 / gLoss: 0.7093136310577393\n",
      "Epoch: 235 / dLoss: 1.2963380813598633 / gLoss: 0.781018853187561\n",
      "Epoch: 236 / dLoss: 1.3238017559051514 / gLoss: 0.735358476638794\n",
      "Epoch: 237 / dLoss: 1.360159158706665 / gLoss: 0.7641599178314209\n",
      "Epoch: 238 / dLoss: 1.3042380809783936 / gLoss: 0.8304452300071716\n",
      "Epoch: 239 / dLoss: 1.3043293952941895 / gLoss: 0.7658519744873047\n",
      "Epoch: 240 / dLoss: 1.3468877077102661 / gLoss: 0.7871098518371582\n",
      "Epoch: 241 / dLoss: 1.3402076959609985 / gLoss: 0.7485955953598022\n",
      "Epoch: 242 / dLoss: 1.308215856552124 / gLoss: 0.7297718524932861\n",
      "Epoch: 243 / dLoss: 1.3068642616271973 / gLoss: 0.802949070930481\n",
      "Epoch: 244 / dLoss: 1.3620296716690063 / gLoss: 0.7569201588630676\n",
      "Epoch: 245 / dLoss: 1.3176157474517822 / gLoss: 0.7292565703392029\n",
      "Epoch: 246 / dLoss: 1.3010101318359375 / gLoss: 0.778892457485199\n",
      "Epoch: 247 / dLoss: 1.2964301109313965 / gLoss: 0.7529823184013367\n",
      "Epoch: 248 / dLoss: 1.2986022233963013 / gLoss: 0.7849810719490051\n",
      "Epoch: 249 / dLoss: 1.331278920173645 / gLoss: 0.7557345032691956\n",
      "Epoch: 250 / dLoss: 1.3280997276306152 / gLoss: 0.8075851202011108\n",
      "Epoch: 251 / dLoss: 1.306373119354248 / gLoss: 0.7342854142189026\n",
      "Epoch: 252 / dLoss: 1.3547611236572266 / gLoss: 0.7993360161781311\n",
      "Epoch: 253 / dLoss: 1.2830841541290283 / gLoss: 0.7823053598403931\n",
      "Epoch: 254 / dLoss: 1.3232753276824951 / gLoss: 0.8220974802970886\n",
      "Epoch: 255 / dLoss: 1.3234448432922363 / gLoss: 0.8355296850204468\n",
      "Epoch: 256 / dLoss: 1.3172461986541748 / gLoss: 0.7766134738922119\n",
      "Epoch: 257 / dLoss: 1.309328317642212 / gLoss: 0.7993575930595398\n",
      "Epoch: 258 / dLoss: 1.313786506652832 / gLoss: 0.8136128187179565\n",
      "Epoch: 259 / dLoss: 1.281339406967163 / gLoss: 0.7979910969734192\n",
      "Epoch: 260 / dLoss: 1.3334214687347412 / gLoss: 0.7754054069519043\n",
      "Epoch: 261 / dLoss: 1.3399913311004639 / gLoss: 0.7720612287521362\n",
      "Epoch: 262 / dLoss: 1.354090929031372 / gLoss: 0.8216376304626465\n",
      "Epoch: 263 / dLoss: 1.3279922008514404 / gLoss: 0.8536435961723328\n",
      "Epoch: 264 / dLoss: 1.3203381299972534 / gLoss: 0.8239216804504395\n",
      "Epoch: 265 / dLoss: 1.2903051376342773 / gLoss: 0.8428170680999756\n",
      "Epoch: 266 / dLoss: 1.3338990211486816 / gLoss: 0.7998907566070557\n",
      "Epoch: 267 / dLoss: 1.3073866367340088 / gLoss: 0.8033658862113953\n",
      "Epoch: 268 / dLoss: 1.315307378768921 / gLoss: 0.8283190131187439\n",
      "Epoch: 269 / dLoss: 1.291371464729309 / gLoss: 0.8030654191970825\n",
      "Epoch: 270 / dLoss: 1.280332088470459 / gLoss: 0.7940468788146973\n",
      "Epoch: 271 / dLoss: 1.2992284297943115 / gLoss: 0.7532269358634949\n",
      "Epoch: 272 / dLoss: 1.2901113033294678 / gLoss: 0.7902509570121765\n",
      "Epoch: 273 / dLoss: 1.2948179244995117 / gLoss: 0.82102370262146\n",
      "Epoch: 274 / dLoss: 1.3396131992340088 / gLoss: 0.7888486981391907\n",
      "Epoch: 275 / dLoss: 1.2968025207519531 / gLoss: 0.8004440069198608\n",
      "Epoch: 276 / dLoss: 1.3192170858383179 / gLoss: 0.8194429278373718\n",
      "Epoch: 277 / dLoss: 1.306283712387085 / gLoss: 0.8229132294654846\n",
      "Epoch: 278 / dLoss: 1.2795352935791016 / gLoss: 0.8307855725288391\n",
      "Epoch: 279 / dLoss: 1.3116077184677124 / gLoss: 0.7905535101890564\n",
      "Epoch: 280 / dLoss: 1.2833789587020874 / gLoss: 0.9082481265068054\n",
      "Epoch: 281 / dLoss: 1.2667007446289062 / gLoss: 0.819542407989502\n",
      "Epoch: 282 / dLoss: 1.2607982158660889 / gLoss: 0.8810430765151978\n",
      "Epoch: 283 / dLoss: 1.3352665901184082 / gLoss: 0.8175833821296692\n",
      "Epoch: 284 / dLoss: 1.2414281368255615 / gLoss: 0.8091386556625366\n",
      "Epoch: 285 / dLoss: 1.2826272249221802 / gLoss: 0.8498877882957458\n",
      "Epoch: 286 / dLoss: 1.274926781654358 / gLoss: 0.8417462706565857\n",
      "Epoch: 287 / dLoss: 1.2789018154144287 / gLoss: 0.9163362979888916\n",
      "Epoch: 288 / dLoss: 1.310469627380371 / gLoss: 0.8378795385360718\n",
      "Epoch: 289 / dLoss: 1.2955145835876465 / gLoss: 0.8110293745994568\n",
      "Epoch: 290 / dLoss: 1.2565380334854126 / gLoss: 0.8156326413154602\n",
      "Epoch: 291 / dLoss: 1.3143948316574097 / gLoss: 0.831788957118988\n",
      "Epoch: 292 / dLoss: 1.2742156982421875 / gLoss: 0.8311072587966919\n",
      "Epoch: 293 / dLoss: 1.323785424232483 / gLoss: 0.8294094204902649\n",
      "Epoch: 294 / dLoss: 1.2840921878814697 / gLoss: 0.7946116924285889\n",
      "Epoch: 295 / dLoss: 1.243464708328247 / gLoss: 0.7968772053718567\n",
      "Epoch: 296 / dLoss: 1.348137617111206 / gLoss: 0.8146644830703735\n",
      "Epoch: 297 / dLoss: 1.3887739181518555 / gLoss: 0.8398259878158569\n",
      "Epoch: 298 / dLoss: 1.2827363014221191 / gLoss: 0.8697482943534851\n",
      "Epoch: 299 / dLoss: 1.3131104707717896 / gLoss: 0.7741846442222595\n",
      "Epoch: 300 / dLoss: 1.2995468378067017 / gLoss: 0.8055089712142944\n",
      "Epoch: 301 / dLoss: 1.2999987602233887 / gLoss: 0.7802736759185791\n",
      "Epoch: 302 / dLoss: 1.323427677154541 / gLoss: 0.8048168420791626\n",
      "Epoch: 303 / dLoss: 1.3048374652862549 / gLoss: 0.7710160613059998\n",
      "Epoch: 304 / dLoss: 1.2723708152770996 / gLoss: 0.8295595645904541\n",
      "Epoch: 305 / dLoss: 1.2784013748168945 / gLoss: 0.8395006656646729\n",
      "Epoch: 306 / dLoss: 1.2304985523223877 / gLoss: 0.7694506049156189\n",
      "Epoch: 307 / dLoss: 1.3072550296783447 / gLoss: 0.8938055634498596\n",
      "Epoch: 308 / dLoss: 1.3119029998779297 / gLoss: 0.8571971654891968\n",
      "Epoch: 309 / dLoss: 1.321572184562683 / gLoss: 0.7769919633865356\n",
      "Epoch: 310 / dLoss: 1.3113725185394287 / gLoss: 0.8422974348068237\n",
      "Epoch: 311 / dLoss: 1.2908134460449219 / gLoss: 0.8422574400901794\n",
      "Epoch: 312 / dLoss: 1.2725521326065063 / gLoss: 0.8138970732688904\n",
      "Epoch: 313 / dLoss: 1.2740967273712158 / gLoss: 0.8870730400085449\n",
      "Epoch: 314 / dLoss: 1.25950026512146 / gLoss: 0.8646304607391357\n",
      "Epoch: 315 / dLoss: 1.2437679767608643 / gLoss: 0.8655861616134644\n",
      "Epoch: 316 / dLoss: 1.3199310302734375 / gLoss: 0.7888421416282654\n",
      "Epoch: 317 / dLoss: 1.2465612888336182 / gLoss: 0.8096672892570496\n",
      "Epoch: 318 / dLoss: 1.3150479793548584 / gLoss: 0.804675281047821\n",
      "Epoch: 319 / dLoss: 1.3261252641677856 / gLoss: 0.9067545533180237\n",
      "Epoch: 320 / dLoss: 1.353832483291626 / gLoss: 0.8172037601470947\n",
      "Epoch: 321 / dLoss: 1.278289794921875 / gLoss: 0.7974019646644592\n",
      "Epoch: 322 / dLoss: 1.2611017227172852 / gLoss: 0.8430562019348145\n",
      "Epoch: 323 / dLoss: 1.2409849166870117 / gLoss: 0.863125741481781\n",
      "Epoch: 324 / dLoss: 1.2633280754089355 / gLoss: 0.8710976243019104\n",
      "Epoch: 325 / dLoss: 1.268106460571289 / gLoss: 0.900429368019104\n",
      "Epoch: 326 / dLoss: 1.2883508205413818 / gLoss: 0.8836917281150818\n",
      "Epoch: 327 / dLoss: 1.314100980758667 / gLoss: 0.8502415418624878\n",
      "Epoch: 328 / dLoss: 1.2347688674926758 / gLoss: 0.8197891712188721\n",
      "Epoch: 329 / dLoss: 1.2726774215698242 / gLoss: 0.836101770401001\n",
      "Epoch: 330 / dLoss: 1.2643485069274902 / gLoss: 0.8142212629318237\n",
      "Epoch: 331 / dLoss: 1.2585126161575317 / gLoss: 0.8822805881500244\n",
      "Epoch: 332 / dLoss: 1.3232007026672363 / gLoss: 0.8091031908988953\n",
      "Epoch: 333 / dLoss: 1.2642810344696045 / gLoss: 0.866924524307251\n",
      "Epoch: 334 / dLoss: 1.2952852249145508 / gLoss: 0.81700599193573\n",
      "Epoch: 335 / dLoss: 1.2788457870483398 / gLoss: 0.7914949059486389\n",
      "Epoch: 336 / dLoss: 1.3514463901519775 / gLoss: 0.8320577144622803\n",
      "Epoch: 337 / dLoss: 1.2800085544586182 / gLoss: 0.8656354546546936\n",
      "Epoch: 338 / dLoss: 1.2976045608520508 / gLoss: 0.7995896339416504\n",
      "Epoch: 339 / dLoss: 1.2914276123046875 / gLoss: 0.8692981004714966\n",
      "Epoch: 340 / dLoss: 1.2730498313903809 / gLoss: 0.8434766530990601\n",
      "Epoch: 341 / dLoss: 1.3449161052703857 / gLoss: 0.8010534644126892\n",
      "Epoch: 342 / dLoss: 1.328364610671997 / gLoss: 0.7906020283699036\n",
      "Epoch: 343 / dLoss: 1.338435411453247 / gLoss: 0.868398904800415\n",
      "Epoch: 344 / dLoss: 1.2507050037384033 / gLoss: 0.9186960458755493\n",
      "Epoch: 345 / dLoss: 1.308288812637329 / gLoss: 0.8364284038543701\n",
      "Epoch: 346 / dLoss: 1.2734415531158447 / gLoss: 0.8701454401016235\n",
      "Epoch: 347 / dLoss: 1.224712610244751 / gLoss: 0.8506489396095276\n",
      "Epoch: 348 / dLoss: 1.215980052947998 / gLoss: 0.8665992617607117\n",
      "Epoch: 349 / dLoss: 1.352623701095581 / gLoss: 0.7992363572120667\n",
      "Epoch: 350 / dLoss: 1.2841883897781372 / gLoss: 0.9203395247459412\n",
      "Epoch: 351 / dLoss: 1.2854490280151367 / gLoss: 0.9011123180389404\n",
      "Epoch: 352 / dLoss: 1.2876167297363281 / gLoss: 0.7806993126869202\n",
      "Epoch: 353 / dLoss: 1.281679391860962 / gLoss: 0.782620906829834\n",
      "Epoch: 354 / dLoss: 1.3052771091461182 / gLoss: 0.8474387526512146\n",
      "Epoch: 355 / dLoss: 1.2986314296722412 / gLoss: 0.9109375476837158\n",
      "Epoch: 356 / dLoss: 1.3083399534225464 / gLoss: 0.778800368309021\n",
      "Epoch: 357 / dLoss: 1.3420350551605225 / gLoss: 0.8518484234809875\n",
      "Epoch: 358 / dLoss: 1.236649990081787 / gLoss: 0.944305419921875\n",
      "Epoch: 359 / dLoss: 1.2897284030914307 / gLoss: 0.8194454908370972\n",
      "Epoch: 360 / dLoss: 1.276829719543457 / gLoss: 0.8133470416069031\n",
      "Epoch: 361 / dLoss: 1.3203818798065186 / gLoss: 0.8001738786697388\n",
      "Epoch: 362 / dLoss: 1.3586370944976807 / gLoss: 0.8363267779350281\n",
      "Epoch: 363 / dLoss: 1.3082146644592285 / gLoss: 0.8139957189559937\n",
      "Epoch: 364 / dLoss: 1.288656234741211 / gLoss: 0.8282276391983032\n",
      "Epoch: 365 / dLoss: 1.325100064277649 / gLoss: 0.739777147769928\n",
      "Epoch: 366 / dLoss: 1.2687132358551025 / gLoss: 0.7609100341796875\n",
      "Epoch: 367 / dLoss: 1.2769116163253784 / gLoss: 0.8036851286888123\n",
      "Epoch: 368 / dLoss: 1.287827968597412 / gLoss: 0.791362464427948\n",
      "Epoch: 369 / dLoss: 1.2747917175292969 / gLoss: 0.7565863728523254\n",
      "Epoch: 370 / dLoss: 1.319314956665039 / gLoss: 0.8451154828071594\n",
      "Epoch: 371 / dLoss: 1.2630373239517212 / gLoss: 0.8485234975814819\n",
      "Epoch: 372 / dLoss: 1.2860932350158691 / gLoss: 0.7777056694030762\n",
      "Epoch: 373 / dLoss: 1.271141767501831 / gLoss: 0.775540292263031\n",
      "Epoch: 374 / dLoss: 1.288575291633606 / gLoss: 0.7972556352615356\n",
      "Epoch: 375 / dLoss: 1.300229549407959 / gLoss: 0.8625742197036743\n",
      "Epoch: 376 / dLoss: 1.262101411819458 / gLoss: 0.9005788564682007\n",
      "Epoch: 377 / dLoss: 1.2642607688903809 / gLoss: 0.8665757179260254\n",
      "Epoch: 378 / dLoss: 1.2997064590454102 / gLoss: 0.8160569667816162\n",
      "Epoch: 379 / dLoss: 1.2717771530151367 / gLoss: 0.8103442192077637\n",
      "Epoch: 380 / dLoss: 1.3415215015411377 / gLoss: 0.8330165147781372\n",
      "Epoch: 381 / dLoss: 1.2790751457214355 / gLoss: 0.836862325668335\n",
      "Epoch: 382 / dLoss: 1.2344160079956055 / gLoss: 0.8242470622062683\n",
      "Epoch: 383 / dLoss: 1.283569574356079 / gLoss: 0.8579019904136658\n",
      "Epoch: 384 / dLoss: 1.3099000453948975 / gLoss: 0.8440673351287842\n",
      "Epoch: 385 / dLoss: 1.2518634796142578 / gLoss: 0.807904839515686\n",
      "Epoch: 386 / dLoss: 1.3175182342529297 / gLoss: 0.823372483253479\n",
      "Epoch: 387 / dLoss: 1.253978967666626 / gLoss: 0.7268897294998169\n",
      "Epoch: 388 / dLoss: 1.2741734981536865 / gLoss: 0.8074945211410522\n",
      "Epoch: 389 / dLoss: 1.2972476482391357 / gLoss: 0.7530877590179443\n",
      "Epoch: 390 / dLoss: 1.255454659461975 / gLoss: 0.8110804557800293\n",
      "Epoch: 391 / dLoss: 1.3404533863067627 / gLoss: 0.7847386002540588\n",
      "Epoch: 392 / dLoss: 1.3290053606033325 / gLoss: 0.86053067445755\n",
      "Epoch: 393 / dLoss: 1.2145888805389404 / gLoss: 0.826485276222229\n",
      "Epoch: 394 / dLoss: 1.316915512084961 / gLoss: 0.8657088279724121\n",
      "Epoch: 395 / dLoss: 1.2686374187469482 / gLoss: 0.8078336119651794\n",
      "Epoch: 396 / dLoss: 1.290492057800293 / gLoss: 0.8285066485404968\n",
      "Epoch: 397 / dLoss: 1.2725306749343872 / gLoss: 0.8076967597007751\n",
      "Epoch: 398 / dLoss: 1.2608246803283691 / gLoss: 0.8057664632797241\n",
      "Epoch: 399 / dLoss: 1.322579264640808 / gLoss: 0.8117312788963318\n",
      "Epoch: 400 / dLoss: 1.2760980129241943 / gLoss: 0.8056608438491821\n",
      "Epoch: 401 / dLoss: 1.3166775703430176 / gLoss: 0.7799193859100342\n",
      "Epoch: 402 / dLoss: 1.2168712615966797 / gLoss: 0.8165546655654907\n",
      "Epoch: 403 / dLoss: 1.2412713766098022 / gLoss: 0.8101680278778076\n",
      "Epoch: 404 / dLoss: 1.3360731601715088 / gLoss: 0.8313870429992676\n",
      "Epoch: 405 / dLoss: 1.295973777770996 / gLoss: 0.836668074131012\n",
      "Epoch: 406 / dLoss: 1.3486659526824951 / gLoss: 0.8342328071594238\n",
      "Epoch: 407 / dLoss: 1.3179255723953247 / gLoss: 0.8163713216781616\n",
      "Epoch: 408 / dLoss: 1.286050796508789 / gLoss: 0.8166621327400208\n",
      "Epoch: 409 / dLoss: 1.3402233123779297 / gLoss: 0.8392083644866943\n",
      "Epoch: 410 / dLoss: 1.2632737159729004 / gLoss: 0.8360083699226379\n",
      "Epoch: 411 / dLoss: 1.3175950050354004 / gLoss: 0.8440995812416077\n",
      "Epoch: 412 / dLoss: 1.2978984117507935 / gLoss: 0.7869624495506287\n",
      "Epoch: 413 / dLoss: 1.3261010646820068 / gLoss: 0.793094277381897\n",
      "Epoch: 414 / dLoss: 1.3022115230560303 / gLoss: 0.8292182087898254\n",
      "Epoch: 415 / dLoss: 1.3397637605667114 / gLoss: 0.8364382386207581\n",
      "Epoch: 416 / dLoss: 1.3389264345169067 / gLoss: 0.8085261583328247\n",
      "Epoch: 417 / dLoss: 1.3303754329681396 / gLoss: 0.7562434673309326\n",
      "Epoch: 418 / dLoss: 1.2633380889892578 / gLoss: 0.7636468410491943\n",
      "Epoch: 419 / dLoss: 1.2917041778564453 / gLoss: 0.752277135848999\n",
      "Epoch: 420 / dLoss: 1.300612211227417 / gLoss: 0.7865278720855713\n",
      "Epoch: 421 / dLoss: 1.2587378025054932 / gLoss: 0.8055874705314636\n",
      "Epoch: 422 / dLoss: 1.3581693172454834 / gLoss: 0.7849767804145813\n",
      "Epoch: 423 / dLoss: 1.3638490438461304 / gLoss: 0.8129677772521973\n",
      "Epoch: 424 / dLoss: 1.296025276184082 / gLoss: 0.7492296099662781\n",
      "Epoch: 425 / dLoss: 1.4272708892822266 / gLoss: 0.7557988166809082\n",
      "Epoch: 426 / dLoss: 1.3441466093063354 / gLoss: 0.7735728621482849\n",
      "Epoch: 427 / dLoss: 1.2824711799621582 / gLoss: 0.8321201205253601\n",
      "Epoch: 428 / dLoss: 1.2873296737670898 / gLoss: 0.7851734757423401\n",
      "Epoch: 429 / dLoss: 1.3296399116516113 / gLoss: 0.8129823207855225\n",
      "Epoch: 430 / dLoss: 1.318406581878662 / gLoss: 0.815619945526123\n",
      "Epoch: 431 / dLoss: 1.2972097396850586 / gLoss: 0.8308234214782715\n",
      "Epoch: 432 / dLoss: 1.2900631427764893 / gLoss: 0.8084105849266052\n",
      "Epoch: 433 / dLoss: 1.3212110996246338 / gLoss: 0.8060309886932373\n",
      "Epoch: 434 / dLoss: 1.2943356037139893 / gLoss: 0.8254514932632446\n",
      "Epoch: 435 / dLoss: 1.247711420059204 / gLoss: 0.7630963325500488\n",
      "Epoch: 436 / dLoss: 1.3601608276367188 / gLoss: 0.8102095127105713\n",
      "Epoch: 437 / dLoss: 1.331484079360962 / gLoss: 0.7829616069793701\n",
      "Epoch: 438 / dLoss: 1.294510006904602 / gLoss: 0.7618755102157593\n",
      "Epoch: 439 / dLoss: 1.2692339420318604 / gLoss: 0.8680579662322998\n",
      "Epoch: 440 / dLoss: 1.2765692472457886 / gLoss: 0.858705997467041\n",
      "Epoch: 441 / dLoss: 1.312690019607544 / gLoss: 0.7791985869407654\n",
      "Epoch: 442 / dLoss: 1.4151201248168945 / gLoss: 0.828777015209198\n",
      "Epoch: 443 / dLoss: 1.3116984367370605 / gLoss: 0.7776231169700623\n",
      "Epoch: 444 / dLoss: 1.358693242073059 / gLoss: 0.7832423448562622\n",
      "Epoch: 445 / dLoss: 1.357142686843872 / gLoss: 0.798635721206665\n",
      "Epoch: 446 / dLoss: 1.353846788406372 / gLoss: 0.7902573347091675\n",
      "Epoch: 447 / dLoss: 1.3493568897247314 / gLoss: 0.7458341121673584\n",
      "Epoch: 448 / dLoss: 1.3670803308486938 / gLoss: 0.774152398109436\n",
      "Epoch: 449 / dLoss: 1.3608951568603516 / gLoss: 0.764548122882843\n",
      "Epoch: 450 / dLoss: 1.3658223152160645 / gLoss: 0.7816284894943237\n",
      "Epoch: 451 / dLoss: 1.3398425579071045 / gLoss: 0.7426603436470032\n",
      "Epoch: 452 / dLoss: 1.3626353740692139 / gLoss: 0.7641238570213318\n",
      "Epoch: 453 / dLoss: 1.3300387859344482 / gLoss: 0.7453184723854065\n",
      "Epoch: 454 / dLoss: 1.3201344013214111 / gLoss: 0.764742374420166\n",
      "Epoch: 455 / dLoss: 1.317847728729248 / gLoss: 0.8020471930503845\n",
      "Epoch: 456 / dLoss: 1.3393386602401733 / gLoss: 0.7542329430580139\n",
      "Epoch: 457 / dLoss: 1.3355082273483276 / gLoss: 0.7683730125427246\n",
      "Epoch: 458 / dLoss: 1.3431940078735352 / gLoss: 0.8000845313072205\n",
      "Epoch: 459 / dLoss: 1.2967522144317627 / gLoss: 0.8025327920913696\n",
      "Epoch: 460 / dLoss: 1.3142266273498535 / gLoss: 0.7849036455154419\n",
      "Epoch: 461 / dLoss: 1.3302183151245117 / gLoss: 0.7429872155189514\n",
      "Epoch: 462 / dLoss: 1.3444902896881104 / gLoss: 0.6898062825202942\n",
      "Epoch: 463 / dLoss: 1.338295817375183 / gLoss: 0.7773339748382568\n",
      "Epoch: 464 / dLoss: 1.3932534456253052 / gLoss: 0.7885968089103699\n",
      "Epoch: 465 / dLoss: 1.3420931100845337 / gLoss: 0.8192852139472961\n",
      "Epoch: 466 / dLoss: 1.3462910652160645 / gLoss: 0.7420241832733154\n",
      "Epoch: 467 / dLoss: 1.290341854095459 / gLoss: 0.7453308701515198\n",
      "Epoch: 468 / dLoss: 1.3601951599121094 / gLoss: 0.7826213836669922\n",
      "Epoch: 469 / dLoss: 1.2834956645965576 / gLoss: 0.7188155651092529\n",
      "Epoch: 470 / dLoss: 1.3219702243804932 / gLoss: 0.8033025860786438\n",
      "Epoch: 471 / dLoss: 1.2754660844802856 / gLoss: 0.7537645101547241\n",
      "Epoch: 472 / dLoss: 1.2951180934906006 / gLoss: 0.8002793192863464\n",
      "Epoch: 473 / dLoss: 1.301546335220337 / gLoss: 0.784342348575592\n",
      "Epoch: 474 / dLoss: 1.3431406021118164 / gLoss: 0.8036194443702698\n",
      "Epoch: 475 / dLoss: 1.3393412828445435 / gLoss: 0.8167383670806885\n",
      "Epoch: 476 / dLoss: 1.2924604415893555 / gLoss: 0.7776445746421814\n",
      "Epoch: 477 / dLoss: 1.3382422924041748 / gLoss: 0.8199388980865479\n",
      "Epoch: 478 / dLoss: 1.2931125164031982 / gLoss: 0.7628645300865173\n",
      "Epoch: 479 / dLoss: 1.2943609952926636 / gLoss: 0.7726424932479858\n",
      "Epoch: 480 / dLoss: 1.2939939498901367 / gLoss: 0.7508329749107361\n",
      "Epoch: 481 / dLoss: 1.2954154014587402 / gLoss: 0.7967804670333862\n",
      "Epoch: 482 / dLoss: 1.3362960815429688 / gLoss: 0.7938496470451355\n",
      "Epoch: 483 / dLoss: 1.3179395198822021 / gLoss: 0.8177807331085205\n",
      "Epoch: 484 / dLoss: 1.3856230974197388 / gLoss: 0.7720198035240173\n",
      "Epoch: 485 / dLoss: 1.2636946439743042 / gLoss: 0.7523701190948486\n",
      "Epoch: 486 / dLoss: 1.313288927078247 / gLoss: 0.7522274255752563\n",
      "Epoch: 487 / dLoss: 1.3385696411132812 / gLoss: 0.7058458924293518\n",
      "Epoch: 488 / dLoss: 1.346421241760254 / gLoss: 0.8133174777030945\n",
      "Epoch: 489 / dLoss: 1.3373668193817139 / gLoss: 0.7706025838851929\n",
      "Epoch: 490 / dLoss: 1.3125112056732178 / gLoss: 0.7859184741973877\n",
      "Epoch: 491 / dLoss: 1.4006649255752563 / gLoss: 0.8237571716308594\n",
      "Epoch: 492 / dLoss: 1.3034977912902832 / gLoss: 0.8357001543045044\n",
      "Epoch: 493 / dLoss: 1.3279139995574951 / gLoss: 0.8034136891365051\n",
      "Epoch: 494 / dLoss: 1.283273696899414 / gLoss: 0.8530029058456421\n",
      "Epoch: 495 / dLoss: 1.3725961446762085 / gLoss: 0.8230308294296265\n",
      "Epoch: 496 / dLoss: 1.3248178958892822 / gLoss: 0.8120264410972595\n",
      "Epoch: 497 / dLoss: 1.2864285707473755 / gLoss: 0.7327766418457031\n",
      "Epoch: 498 / dLoss: 1.3054090738296509 / gLoss: 0.7926856875419617\n",
      "Epoch: 499 / dLoss: 1.3437001705169678 / gLoss: 0.7896167039871216\n",
      "Epoch: 500 / dLoss: 1.279854655265808 / gLoss: 0.8562648892402649\n",
      "Epoch: 501 / dLoss: 1.2920050621032715 / gLoss: 0.8724420070648193\n",
      "Epoch: 502 / dLoss: 1.2606061697006226 / gLoss: 0.8760193586349487\n",
      "Epoch: 503 / dLoss: 1.2869744300842285 / gLoss: 0.8038205504417419\n",
      "Epoch: 504 / dLoss: 1.3268096446990967 / gLoss: 0.8400657773017883\n",
      "Epoch: 505 / dLoss: 1.3338215351104736 / gLoss: 0.7823605537414551\n",
      "Epoch: 506 / dLoss: 1.3491628170013428 / gLoss: 0.7849281430244446\n",
      "Epoch: 507 / dLoss: 1.3348839282989502 / gLoss: 0.8074079751968384\n",
      "Epoch: 508 / dLoss: 1.336248517036438 / gLoss: 0.7883226275444031\n",
      "Epoch: 509 / dLoss: 1.3215991258621216 / gLoss: 0.751656174659729\n",
      "Epoch: 510 / dLoss: 1.300138235092163 / gLoss: 0.7940219640731812\n",
      "Epoch: 511 / dLoss: 1.3115432262420654 / gLoss: 0.7803473472595215\n",
      "Epoch: 512 / dLoss: 1.301816701889038 / gLoss: 0.7891761660575867\n",
      "Epoch: 513 / dLoss: 1.298015832901001 / gLoss: 0.8259388208389282\n",
      "Epoch: 514 / dLoss: 1.3045248985290527 / gLoss: 0.7800281643867493\n",
      "Epoch: 515 / dLoss: 1.2792279720306396 / gLoss: 0.8083860874176025\n",
      "Epoch: 516 / dLoss: 1.3441612720489502 / gLoss: 0.8169727921485901\n",
      "Epoch: 517 / dLoss: 1.3456690311431885 / gLoss: 0.7717671394348145\n",
      "Epoch: 518 / dLoss: 1.2816643714904785 / gLoss: 0.7926554679870605\n",
      "Epoch: 519 / dLoss: 1.3161990642547607 / gLoss: 0.7816799879074097\n",
      "Epoch: 520 / dLoss: 1.2668681144714355 / gLoss: 0.8145826458930969\n",
      "Epoch: 521 / dLoss: 1.2907147407531738 / gLoss: 0.8207623362541199\n",
      "Epoch: 522 / dLoss: 1.3346894979476929 / gLoss: 0.801716148853302\n",
      "Epoch: 523 / dLoss: 1.3344333171844482 / gLoss: 0.8179569840431213\n",
      "Epoch: 524 / dLoss: 1.2820899486541748 / gLoss: 0.8135716915130615\n",
      "Epoch: 525 / dLoss: 1.3503689765930176 / gLoss: 0.8222886919975281\n",
      "Epoch: 526 / dLoss: 1.2863529920578003 / gLoss: 0.8331582546234131\n",
      "Epoch: 527 / dLoss: 1.2714664936065674 / gLoss: 0.8485387563705444\n",
      "Epoch: 528 / dLoss: 1.2815444469451904 / gLoss: 0.8136316537857056\n",
      "Epoch: 529 / dLoss: 1.3047809600830078 / gLoss: 0.8489349484443665\n",
      "Epoch: 530 / dLoss: 1.2542717456817627 / gLoss: 0.7648192644119263\n",
      "Epoch: 531 / dLoss: 1.2898083925247192 / gLoss: 0.8144776821136475\n",
      "Epoch: 532 / dLoss: 1.2952721118927002 / gLoss: 0.7735419273376465\n",
      "Epoch: 533 / dLoss: 1.2286080121994019 / gLoss: 0.7769739627838135\n",
      "Epoch: 534 / dLoss: 1.3568618297576904 / gLoss: 0.733449399471283\n",
      "Epoch: 535 / dLoss: 1.318392276763916 / gLoss: 0.7774066925048828\n",
      "Epoch: 536 / dLoss: 1.2889853715896606 / gLoss: 0.7810418009757996\n",
      "Epoch: 537 / dLoss: 1.2106437683105469 / gLoss: 0.790793240070343\n",
      "Epoch: 538 / dLoss: 1.301222801208496 / gLoss: 0.8246105909347534\n",
      "Epoch: 539 / dLoss: 1.241563320159912 / gLoss: 0.8210057616233826\n",
      "Epoch: 540 / dLoss: 1.3191649913787842 / gLoss: 0.8304435014724731\n",
      "Epoch: 541 / dLoss: 1.295320987701416 / gLoss: 0.7965198755264282\n",
      "Epoch: 542 / dLoss: 1.3069372177124023 / gLoss: 0.8626744151115417\n",
      "Epoch: 543 / dLoss: 1.2523536682128906 / gLoss: 0.8530796766281128\n",
      "Epoch: 544 / dLoss: 1.3153040409088135 / gLoss: 0.8191407918930054\n",
      "Epoch: 545 / dLoss: 1.2394747734069824 / gLoss: 0.8413525223731995\n",
      "Epoch: 546 / dLoss: 1.2772871255874634 / gLoss: 0.8215544819831848\n",
      "Epoch: 547 / dLoss: 1.2753928899765015 / gLoss: 0.8215994238853455\n",
      "Epoch: 548 / dLoss: 1.2703181505203247 / gLoss: 0.810981035232544\n",
      "Epoch: 549 / dLoss: 1.2739044427871704 / gLoss: 0.8416502475738525\n",
      "Epoch: 550 / dLoss: 1.2745633125305176 / gLoss: 0.8211361765861511\n",
      "Epoch: 551 / dLoss: 1.292529582977295 / gLoss: 0.7987311482429504\n",
      "Epoch: 552 / dLoss: 1.3060920238494873 / gLoss: 0.8692382574081421\n",
      "Epoch: 553 / dLoss: 1.2550411224365234 / gLoss: 0.7998533844947815\n",
      "Epoch: 554 / dLoss: 1.2644908428192139 / gLoss: 0.7995362281799316\n",
      "Epoch: 555 / dLoss: 1.283640742301941 / gLoss: 0.8405314087867737\n",
      "Epoch: 556 / dLoss: 1.3204028606414795 / gLoss: 0.8628742098808289\n",
      "Epoch: 557 / dLoss: 1.332679271697998 / gLoss: 0.812575101852417\n",
      "Epoch: 558 / dLoss: 1.328066349029541 / gLoss: 0.7914884686470032\n",
      "Epoch: 559 / dLoss: 1.3769080638885498 / gLoss: 0.7785265445709229\n",
      "Epoch: 560 / dLoss: 1.281049370765686 / gLoss: 0.7698972821235657\n",
      "Epoch: 561 / dLoss: 1.2925459146499634 / gLoss: 0.8411698937416077\n",
      "Epoch: 562 / dLoss: 1.2676148414611816 / gLoss: 0.8182916045188904\n",
      "Epoch: 563 / dLoss: 1.2916405200958252 / gLoss: 0.8195443153381348\n",
      "Epoch: 564 / dLoss: 1.267520546913147 / gLoss: 0.8751236200332642\n",
      "Epoch: 565 / dLoss: 1.2722830772399902 / gLoss: 0.8216596841812134\n",
      "Epoch: 566 / dLoss: 1.3046058416366577 / gLoss: 0.816483736038208\n",
      "Epoch: 567 / dLoss: 1.2582039833068848 / gLoss: 0.7927128076553345\n",
      "Epoch: 568 / dLoss: 1.3020025491714478 / gLoss: 0.8183247447013855\n",
      "Epoch: 569 / dLoss: 1.2461211681365967 / gLoss: 0.8147678375244141\n",
      "Epoch: 570 / dLoss: 1.228418231010437 / gLoss: 0.8665564060211182\n",
      "Epoch: 571 / dLoss: 1.2723822593688965 / gLoss: 0.8146064281463623\n",
      "Epoch: 572 / dLoss: 1.3108997344970703 / gLoss: 0.8575521111488342\n",
      "Epoch: 573 / dLoss: 1.2425074577331543 / gLoss: 0.8850294351577759\n",
      "Epoch: 574 / dLoss: 1.2939915657043457 / gLoss: 0.8855960965156555\n",
      "Epoch: 575 / dLoss: 1.31465744972229 / gLoss: 0.7964926362037659\n",
      "Epoch: 576 / dLoss: 1.2582807540893555 / gLoss: 0.905180037021637\n",
      "Epoch: 577 / dLoss: 1.3001445531845093 / gLoss: 0.8418668508529663\n",
      "Epoch: 578 / dLoss: 1.2220699787139893 / gLoss: 0.8880481719970703\n",
      "Epoch: 579 / dLoss: 1.2934521436691284 / gLoss: 0.8528897762298584\n",
      "Epoch: 580 / dLoss: 1.2373933792114258 / gLoss: 0.8546181321144104\n",
      "Epoch: 581 / dLoss: 1.2364168167114258 / gLoss: 0.8890419006347656\n",
      "Epoch: 582 / dLoss: 1.2740888595581055 / gLoss: 0.8128474950790405\n",
      "Epoch: 583 / dLoss: 1.2623960971832275 / gLoss: 0.8666722774505615\n",
      "Epoch: 584 / dLoss: 1.230540156364441 / gLoss: 0.9004696607589722\n",
      "Epoch: 585 / dLoss: 1.3314006328582764 / gLoss: 0.8889815807342529\n",
      "Epoch: 586 / dLoss: 1.3317663669586182 / gLoss: 0.8638055920600891\n",
      "Epoch: 587 / dLoss: 1.2968394756317139 / gLoss: 0.8827199339866638\n",
      "Epoch: 588 / dLoss: 1.3012704849243164 / gLoss: 0.8419447541236877\n",
      "Epoch: 589 / dLoss: 1.2666676044464111 / gLoss: 0.802776038646698\n",
      "Epoch: 590 / dLoss: 1.250779390335083 / gLoss: 0.8453441858291626\n",
      "Epoch: 591 / dLoss: 1.2156007289886475 / gLoss: 0.8377070426940918\n",
      "Epoch: 592 / dLoss: 1.2411400079727173 / gLoss: 0.8410064578056335\n",
      "Epoch: 593 / dLoss: 1.1669108867645264 / gLoss: 0.8624333143234253\n",
      "Epoch: 594 / dLoss: 1.2590563297271729 / gLoss: 0.9019815921783447\n",
      "Epoch: 595 / dLoss: 1.2714307308197021 / gLoss: 0.8922016620635986\n",
      "Epoch: 596 / dLoss: 1.294722318649292 / gLoss: 0.8095852732658386\n",
      "Epoch: 597 / dLoss: 1.2978930473327637 / gLoss: 0.8727923035621643\n",
      "Epoch: 598 / dLoss: 1.3164377212524414 / gLoss: 0.8875715732574463\n",
      "Epoch: 599 / dLoss: 1.293241024017334 / gLoss: 0.8434154391288757\n",
      "Epoch: 600 / dLoss: 1.2902827262878418 / gLoss: 0.8654597401618958\n",
      "Epoch: 601 / dLoss: 1.2262139320373535 / gLoss: 0.8481835126876831\n",
      "Epoch: 602 / dLoss: 1.2403743267059326 / gLoss: 0.8607422113418579\n",
      "Epoch: 603 / dLoss: 1.2791638374328613 / gLoss: 0.904978334903717\n",
      "Epoch: 604 / dLoss: 1.2900021076202393 / gLoss: 0.8826640248298645\n",
      "Epoch: 605 / dLoss: 1.2592463493347168 / gLoss: 0.9453859925270081\n",
      "Epoch: 606 / dLoss: 1.323337435722351 / gLoss: 0.9406347870826721\n",
      "Epoch: 607 / dLoss: 1.2252799272537231 / gLoss: 0.953037440776825\n",
      "Epoch: 608 / dLoss: 1.2942742109298706 / gLoss: 0.8969449400901794\n",
      "Epoch: 609 / dLoss: 1.2822961807250977 / gLoss: 0.8091505169868469\n",
      "Epoch: 610 / dLoss: 1.2608064413070679 / gLoss: 0.8121580481529236\n",
      "Epoch: 611 / dLoss: 1.2808029651641846 / gLoss: 0.8728258013725281\n",
      "Epoch: 612 / dLoss: 1.230954885482788 / gLoss: 0.8426205515861511\n",
      "Epoch: 613 / dLoss: 1.3266818523406982 / gLoss: 0.9021531939506531\n",
      "Epoch: 614 / dLoss: 1.3027220964431763 / gLoss: 0.8137038350105286\n",
      "Epoch: 615 / dLoss: 1.2895575761795044 / gLoss: 0.8304764032363892\n",
      "Epoch: 616 / dLoss: 1.1752102375030518 / gLoss: 0.9428253769874573\n",
      "Epoch: 617 / dLoss: 1.3111729621887207 / gLoss: 0.9226044416427612\n",
      "Epoch: 618 / dLoss: 1.2626298666000366 / gLoss: 0.8737776279449463\n",
      "Epoch: 619 / dLoss: 1.3477416038513184 / gLoss: 0.8889161944389343\n",
      "Epoch: 620 / dLoss: 1.2422950267791748 / gLoss: 0.8619393110275269\n",
      "Epoch: 621 / dLoss: 1.2583940029144287 / gLoss: 0.8233139514923096\n",
      "Epoch: 622 / dLoss: 1.2010695934295654 / gLoss: 0.8574046492576599\n",
      "Epoch: 623 / dLoss: 1.2351386547088623 / gLoss: 0.9256817102432251\n",
      "Epoch: 624 / dLoss: 1.2592315673828125 / gLoss: 0.7991886138916016\n",
      "Epoch: 625 / dLoss: 1.1604554653167725 / gLoss: 0.8851972222328186\n",
      "Epoch: 626 / dLoss: 1.302734613418579 / gLoss: 0.7946997880935669\n",
      "Epoch: 627 / dLoss: 1.2652274370193481 / gLoss: 0.7998015880584717\n",
      "Epoch: 628 / dLoss: 1.297224521636963 / gLoss: 0.8937292695045471\n",
      "Epoch: 629 / dLoss: 1.29109787940979 / gLoss: 0.87312912940979\n",
      "Epoch: 630 / dLoss: 1.1972899436950684 / gLoss: 0.8724969625473022\n",
      "Epoch: 631 / dLoss: 1.1877235174179077 / gLoss: 0.8805477023124695\n",
      "Epoch: 632 / dLoss: 1.2741690874099731 / gLoss: 0.902099072933197\n",
      "Epoch: 633 / dLoss: 1.2014224529266357 / gLoss: 0.926373302936554\n",
      "Epoch: 634 / dLoss: 1.3114407062530518 / gLoss: 0.8881937265396118\n",
      "Epoch: 635 / dLoss: 1.2162611484527588 / gLoss: 0.8720364570617676\n",
      "Epoch: 636 / dLoss: 1.2606844902038574 / gLoss: 0.8258315920829773\n",
      "Epoch: 637 / dLoss: 1.3203665018081665 / gLoss: 0.9020749926567078\n",
      "Epoch: 638 / dLoss: 1.261446475982666 / gLoss: 0.8097973465919495\n",
      "Epoch: 639 / dLoss: 1.2620413303375244 / gLoss: 0.8390309810638428\n",
      "Epoch: 640 / dLoss: 1.3069978952407837 / gLoss: 0.8829395771026611\n",
      "Epoch: 641 / dLoss: 1.2304661273956299 / gLoss: 1.0059996843338013\n",
      "Epoch: 642 / dLoss: 1.3159925937652588 / gLoss: 0.9220283031463623\n",
      "Epoch: 643 / dLoss: 1.2232465744018555 / gLoss: 0.8883154392242432\n",
      "Epoch: 644 / dLoss: 1.3074760437011719 / gLoss: 0.9589967131614685\n",
      "Epoch: 645 / dLoss: 1.2604910135269165 / gLoss: 0.875752329826355\n",
      "Epoch: 646 / dLoss: 1.2641518115997314 / gLoss: 0.8455868363380432\n",
      "Epoch: 647 / dLoss: 1.2789384126663208 / gLoss: 0.8805550336837769\n",
      "Epoch: 648 / dLoss: 1.3341046571731567 / gLoss: 0.9008908867835999\n",
      "Epoch: 649 / dLoss: 1.2531670331954956 / gLoss: 0.8792640566825867\n",
      "Epoch: 650 / dLoss: 1.274980068206787 / gLoss: 0.8803623914718628\n",
      "Epoch: 651 / dLoss: 1.2285630702972412 / gLoss: 0.9285978674888611\n",
      "Epoch: 652 / dLoss: 1.3099796772003174 / gLoss: 0.8908108472824097\n",
      "Epoch: 653 / dLoss: 1.240555763244629 / gLoss: 0.8909220099449158\n",
      "Epoch: 654 / dLoss: 1.3053474426269531 / gLoss: 0.8899305462837219\n",
      "Epoch: 655 / dLoss: 1.3480339050292969 / gLoss: 0.8678808808326721\n",
      "Epoch: 656 / dLoss: 1.3161070346832275 / gLoss: 0.9093371629714966\n",
      "Epoch: 657 / dLoss: 1.288475751876831 / gLoss: 0.8837555050849915\n",
      "Epoch: 658 / dLoss: 1.2608296871185303 / gLoss: 0.8648624420166016\n",
      "Epoch: 659 / dLoss: 1.220245361328125 / gLoss: 0.8407031297683716\n",
      "Epoch: 660 / dLoss: 1.240344524383545 / gLoss: 0.8822544813156128\n",
      "Epoch: 661 / dLoss: 1.2097680568695068 / gLoss: 0.973462700843811\n",
      "Epoch: 662 / dLoss: 1.2698285579681396 / gLoss: 0.8478853106498718\n",
      "Epoch: 663 / dLoss: 1.2194719314575195 / gLoss: 0.8566994667053223\n",
      "Epoch: 664 / dLoss: 1.201598882675171 / gLoss: 0.8863155841827393\n",
      "Epoch: 665 / dLoss: 1.1649549007415771 / gLoss: 0.9579878449440002\n",
      "Epoch: 666 / dLoss: 1.2001516819000244 / gLoss: 0.8864632248878479\n",
      "Epoch: 667 / dLoss: 1.322439432144165 / gLoss: 0.9640367031097412\n",
      "Epoch: 668 / dLoss: 1.1339099407196045 / gLoss: 0.9612960815429688\n",
      "Epoch: 669 / dLoss: 1.2669644355773926 / gLoss: 0.9330804347991943\n",
      "Epoch: 670 / dLoss: 1.3445050716400146 / gLoss: 0.9447583556175232\n",
      "Epoch: 671 / dLoss: 1.3019890785217285 / gLoss: 0.8916394114494324\n",
      "Epoch: 672 / dLoss: 1.3010005950927734 / gLoss: 0.9186742901802063\n",
      "Epoch: 673 / dLoss: 1.2597808837890625 / gLoss: 0.8746528625488281\n",
      "Epoch: 674 / dLoss: 1.2845678329467773 / gLoss: 0.7775470614433289\n",
      "Epoch: 675 / dLoss: 1.2660274505615234 / gLoss: 0.8795146346092224\n",
      "Epoch: 676 / dLoss: 1.3004953861236572 / gLoss: 0.8331954479217529\n",
      "Epoch: 677 / dLoss: 1.2710766792297363 / gLoss: 0.8974173665046692\n",
      "Epoch: 678 / dLoss: 1.2161142826080322 / gLoss: 0.8659014701843262\n",
      "Epoch: 679 / dLoss: 1.2995264530181885 / gLoss: 0.8354971408843994\n",
      "Epoch: 680 / dLoss: 1.2755928039550781 / gLoss: 0.8743866682052612\n",
      "Epoch: 681 / dLoss: 1.2260432243347168 / gLoss: 0.9094184637069702\n",
      "Epoch: 682 / dLoss: 1.2514183521270752 / gLoss: 0.9509971141815186\n",
      "Epoch: 683 / dLoss: 1.2292531728744507 / gLoss: 0.904986560344696\n",
      "Epoch: 684 / dLoss: 1.2127575874328613 / gLoss: 0.9560562372207642\n",
      "Epoch: 685 / dLoss: 1.228630781173706 / gLoss: 0.9130030274391174\n",
      "Epoch: 686 / dLoss: 1.2765345573425293 / gLoss: 0.9121270179748535\n",
      "Epoch: 687 / dLoss: 1.301450252532959 / gLoss: 0.9358237385749817\n",
      "Epoch: 688 / dLoss: 1.3113199472427368 / gLoss: 0.8754482865333557\n",
      "Epoch: 689 / dLoss: 1.22599458694458 / gLoss: 0.9143696427345276\n",
      "Epoch: 690 / dLoss: 1.2274863719940186 / gLoss: 0.9081417918205261\n",
      "Epoch: 691 / dLoss: 1.223256230354309 / gLoss: 0.978259801864624\n",
      "Epoch: 692 / dLoss: 1.2338712215423584 / gLoss: 0.8602311611175537\n",
      "Epoch: 693 / dLoss: 1.1566362380981445 / gLoss: 0.8898205757141113\n",
      "Epoch: 694 / dLoss: 1.3235604763031006 / gLoss: 0.8164916038513184\n",
      "Epoch: 695 / dLoss: 1.209219217300415 / gLoss: 0.9084979295730591\n",
      "Epoch: 696 / dLoss: 1.2127721309661865 / gLoss: 0.8039341568946838\n",
      "Epoch: 697 / dLoss: 1.2949678897857666 / gLoss: 0.9039214849472046\n",
      "Epoch: 698 / dLoss: 1.2759637832641602 / gLoss: 0.8910999298095703\n",
      "Epoch: 699 / dLoss: 1.2226684093475342 / gLoss: 0.8943741321563721\n",
      "Epoch: 700 / dLoss: 1.2659555673599243 / gLoss: 0.9404324889183044\n",
      "Epoch: 701 / dLoss: 1.2111079692840576 / gLoss: 0.8786661028862\n",
      "Epoch: 702 / dLoss: 1.184739589691162 / gLoss: 0.9439243078231812\n",
      "Epoch: 703 / dLoss: 1.2357532978057861 / gLoss: 0.9477618932723999\n",
      "Epoch: 704 / dLoss: 1.2558053731918335 / gLoss: 0.962892472743988\n",
      "Epoch: 705 / dLoss: 1.262581706047058 / gLoss: 0.8717817068099976\n",
      "Epoch: 706 / dLoss: 1.2703850269317627 / gLoss: 0.8753607869148254\n",
      "Epoch: 707 / dLoss: 1.1554458141326904 / gLoss: 0.9814311265945435\n",
      "Epoch: 708 / dLoss: 1.2055649757385254 / gLoss: 0.8842634558677673\n",
      "Epoch: 709 / dLoss: 1.2519079446792603 / gLoss: 0.8293150067329407\n",
      "Epoch: 710 / dLoss: 1.227876901626587 / gLoss: 0.9245561361312866\n",
      "Epoch: 711 / dLoss: 1.2883477210998535 / gLoss: 0.8745763897895813\n",
      "Epoch: 712 / dLoss: 1.2343127727508545 / gLoss: 0.8871394395828247\n",
      "Epoch: 713 / dLoss: 1.267176866531372 / gLoss: 0.8943472504615784\n",
      "Epoch: 714 / dLoss: 1.2792332172393799 / gLoss: 0.9752753973007202\n",
      "Epoch: 715 / dLoss: 1.2766844034194946 / gLoss: 0.9366297125816345\n",
      "Epoch: 716 / dLoss: 1.2701913118362427 / gLoss: 0.969527006149292\n",
      "Epoch: 717 / dLoss: 1.2769806385040283 / gLoss: 0.9829548597335815\n",
      "Epoch: 718 / dLoss: 1.2966846227645874 / gLoss: 0.9812922477722168\n",
      "Epoch: 719 / dLoss: 1.2262829542160034 / gLoss: 0.9110928177833557\n",
      "Epoch: 720 / dLoss: 1.2274284362792969 / gLoss: 0.8754119277000427\n",
      "Epoch: 721 / dLoss: 1.2324965000152588 / gLoss: 0.8804228901863098\n",
      "Epoch: 722 / dLoss: 1.3495851755142212 / gLoss: 0.8501236438751221\n",
      "Epoch: 723 / dLoss: 1.2135217189788818 / gLoss: 0.8661657571792603\n",
      "Epoch: 724 / dLoss: 1.2455694675445557 / gLoss: 0.7770344614982605\n",
      "Epoch: 725 / dLoss: 1.2541497945785522 / gLoss: 0.8998034596443176\n",
      "Epoch: 726 / dLoss: 1.2832396030426025 / gLoss: 0.9956256747245789\n",
      "Epoch: 727 / dLoss: 1.2109415531158447 / gLoss: 0.8900203108787537\n",
      "Epoch: 728 / dLoss: 1.2336949110031128 / gLoss: 0.9286619424819946\n",
      "Epoch: 729 / dLoss: 1.27219820022583 / gLoss: 0.9401038885116577\n",
      "Epoch: 730 / dLoss: 1.2563118934631348 / gLoss: 0.8923724293708801\n",
      "Epoch: 731 / dLoss: 1.2543342113494873 / gLoss: 0.9232174158096313\n",
      "Epoch: 732 / dLoss: 1.1660704612731934 / gLoss: 0.8608210682868958\n",
      "Epoch: 733 / dLoss: 1.1839118003845215 / gLoss: 0.8149781823158264\n",
      "Epoch: 734 / dLoss: 1.2540374994277954 / gLoss: 0.8670989871025085\n",
      "Epoch: 735 / dLoss: 1.2308626174926758 / gLoss: 0.9102671146392822\n",
      "Epoch: 736 / dLoss: 1.2248523235321045 / gLoss: 0.8815673589706421\n",
      "Epoch: 737 / dLoss: 1.3109009265899658 / gLoss: 0.8875350952148438\n",
      "Epoch: 738 / dLoss: 1.2372077703475952 / gLoss: 1.0120054483413696\n",
      "Epoch: 739 / dLoss: 1.2243869304656982 / gLoss: 0.9308364987373352\n",
      "Epoch: 740 / dLoss: 1.2551205158233643 / gLoss: 0.9954133629798889\n",
      "Epoch: 741 / dLoss: 1.2829945087432861 / gLoss: 0.8988950848579407\n",
      "Epoch: 742 / dLoss: 1.2335255146026611 / gLoss: 0.8932631015777588\n",
      "Epoch: 743 / dLoss: 1.2593746185302734 / gLoss: 0.9454084634780884\n",
      "Epoch: 744 / dLoss: 1.2545546293258667 / gLoss: 0.9309736490249634\n",
      "Epoch: 745 / dLoss: 1.2505130767822266 / gLoss: 0.9035634398460388\n",
      "Epoch: 746 / dLoss: 1.1968779563903809 / gLoss: 0.9470323920249939\n",
      "Epoch: 747 / dLoss: 1.2184131145477295 / gLoss: 0.9250806570053101\n",
      "Epoch: 748 / dLoss: 1.2551708221435547 / gLoss: 0.8849424123764038\n",
      "Epoch: 749 / dLoss: 1.2303065061569214 / gLoss: 0.8913991451263428\n",
      "Epoch: 750 / dLoss: 1.216651201248169 / gLoss: 0.9010495543479919\n",
      "Epoch: 751 / dLoss: 1.1800236701965332 / gLoss: 1.0328258275985718\n",
      "Epoch: 752 / dLoss: 1.289750099182129 / gLoss: 0.979457676410675\n",
      "Epoch: 753 / dLoss: 1.233616590499878 / gLoss: 0.978099524974823\n",
      "Epoch: 754 / dLoss: 1.2280123233795166 / gLoss: 0.9960759878158569\n",
      "Epoch: 755 / dLoss: 1.3351926803588867 / gLoss: 0.8677153587341309\n",
      "Epoch: 756 / dLoss: 1.1791551113128662 / gLoss: 0.8331874012947083\n",
      "Epoch: 757 / dLoss: 1.300992488861084 / gLoss: 0.8770963549613953\n",
      "Epoch: 758 / dLoss: 1.2035554647445679 / gLoss: 0.8927361965179443\n",
      "Epoch: 759 / dLoss: 1.2445850372314453 / gLoss: 0.9340239763259888\n",
      "Epoch: 760 / dLoss: 1.2996995449066162 / gLoss: 1.0393301248550415\n",
      "Epoch: 761 / dLoss: 1.232145071029663 / gLoss: 0.8211379051208496\n",
      "Epoch: 762 / dLoss: 1.2977163791656494 / gLoss: 0.8364506363868713\n",
      "Epoch: 763 / dLoss: 1.2632505893707275 / gLoss: 0.8664261698722839\n",
      "Epoch: 764 / dLoss: 1.2412099838256836 / gLoss: 0.866557240486145\n",
      "Epoch: 765 / dLoss: 1.2884379625320435 / gLoss: 0.9824355840682983\n",
      "Epoch: 766 / dLoss: 1.2345112562179565 / gLoss: 0.9892857074737549\n",
      "Epoch: 767 / dLoss: 1.1934049129486084 / gLoss: 0.9903782606124878\n",
      "Epoch: 768 / dLoss: 1.193349838256836 / gLoss: 1.0650889873504639\n",
      "Epoch: 769 / dLoss: 1.143045425415039 / gLoss: 0.9555776715278625\n",
      "Epoch: 770 / dLoss: 1.310985803604126 / gLoss: 0.9136134386062622\n",
      "Epoch: 771 / dLoss: 1.2090222835540771 / gLoss: 0.8647968769073486\n",
      "Epoch: 772 / dLoss: 1.3092918395996094 / gLoss: 0.8707387447357178\n",
      "Epoch: 773 / dLoss: 1.2390462160110474 / gLoss: 1.0090017318725586\n",
      "Epoch: 774 / dLoss: 1.175856113433838 / gLoss: 1.0010753870010376\n",
      "Epoch: 775 / dLoss: 1.1914805173873901 / gLoss: 0.9922347068786621\n",
      "Epoch: 776 / dLoss: 1.2920315265655518 / gLoss: 1.014593482017517\n",
      "Epoch: 777 / dLoss: 1.2659204006195068 / gLoss: 0.8565452694892883\n",
      "Epoch: 778 / dLoss: 1.2002861499786377 / gLoss: 0.9558417797088623\n",
      "Epoch: 779 / dLoss: 1.3122706413269043 / gLoss: 0.944670557975769\n",
      "Epoch: 780 / dLoss: 1.2929579019546509 / gLoss: 0.9911430478096008\n",
      "Epoch: 781 / dLoss: 1.2231855392456055 / gLoss: 1.0093114376068115\n",
      "Epoch: 782 / dLoss: 1.1963281631469727 / gLoss: 0.86295485496521\n",
      "Epoch: 783 / dLoss: 1.230923056602478 / gLoss: 0.8655461668968201\n",
      "Epoch: 784 / dLoss: 1.198418378829956 / gLoss: 0.8454576730728149\n",
      "Epoch: 785 / dLoss: 1.238983154296875 / gLoss: 0.8619144558906555\n",
      "Epoch: 786 / dLoss: 1.1859586238861084 / gLoss: 0.9232856631278992\n",
      "Epoch: 787 / dLoss: 1.1515603065490723 / gLoss: 0.9957038760185242\n",
      "Epoch: 788 / dLoss: 1.1399006843566895 / gLoss: 1.0207626819610596\n",
      "Epoch: 789 / dLoss: 1.2884244918823242 / gLoss: 0.9733218550682068\n",
      "Epoch: 790 / dLoss: 1.2646996974945068 / gLoss: 0.9916951060295105\n",
      "Epoch: 791 / dLoss: 1.2775468826293945 / gLoss: 0.9336539506912231\n",
      "Epoch: 792 / dLoss: 1.2198221683502197 / gLoss: 0.8504859805107117\n",
      "Epoch: 793 / dLoss: 1.2727495431900024 / gLoss: 1.0779047012329102\n",
      "Epoch: 794 / dLoss: 1.2196829319000244 / gLoss: 0.9377226233482361\n",
      "Epoch: 795 / dLoss: 1.2072558403015137 / gLoss: 0.9848090410232544\n",
      "Epoch: 796 / dLoss: 1.2626492977142334 / gLoss: 0.9206967949867249\n",
      "Epoch: 797 / dLoss: 1.1943213939666748 / gLoss: 0.9261231422424316\n",
      "Epoch: 798 / dLoss: 1.1707124710083008 / gLoss: 0.9060307145118713\n",
      "Epoch: 799 / dLoss: 1.2489045858383179 / gLoss: 1.0057297945022583\n",
      "Epoch: 800 / dLoss: 1.2541530132293701 / gLoss: 0.9580578804016113\n",
      "Epoch: 801 / dLoss: 1.2421753406524658 / gLoss: 1.044339656829834\n",
      "Epoch: 802 / dLoss: 1.1897594928741455 / gLoss: 0.8357530832290649\n",
      "Epoch: 803 / dLoss: 1.2037208080291748 / gLoss: 0.9452618956565857\n",
      "Epoch: 804 / dLoss: 1.2175700664520264 / gLoss: 0.9485593438148499\n",
      "Epoch: 805 / dLoss: 1.2069370746612549 / gLoss: 0.9795523285865784\n",
      "Epoch: 806 / dLoss: 1.322232723236084 / gLoss: 0.9246233105659485\n",
      "Epoch: 807 / dLoss: 1.2750056982040405 / gLoss: 0.9711406826972961\n",
      "Epoch: 808 / dLoss: 1.2143220901489258 / gLoss: 0.9687095880508423\n",
      "Epoch: 809 / dLoss: 1.2584205865859985 / gLoss: 0.906816303730011\n",
      "Epoch: 810 / dLoss: 1.1650813817977905 / gLoss: 1.026233196258545\n",
      "Epoch: 811 / dLoss: 1.24318528175354 / gLoss: 0.9538216590881348\n",
      "Epoch: 812 / dLoss: 1.259925365447998 / gLoss: 0.8532156348228455\n",
      "Epoch: 813 / dLoss: 1.1700048446655273 / gLoss: 0.9453439116477966\n",
      "Epoch: 814 / dLoss: 1.2258070707321167 / gLoss: 0.9759714603424072\n",
      "Epoch: 815 / dLoss: 1.2433874607086182 / gLoss: 0.9577081203460693\n",
      "Epoch: 816 / dLoss: 1.2395274639129639 / gLoss: 0.8947021961212158\n",
      "Epoch: 817 / dLoss: 1.2643439769744873 / gLoss: 0.9265390038490295\n",
      "Epoch: 818 / dLoss: 1.1960203647613525 / gLoss: 0.9454874396324158\n",
      "Epoch: 819 / dLoss: 1.2654896974563599 / gLoss: 0.9175460934638977\n",
      "Epoch: 820 / dLoss: 1.203810453414917 / gLoss: 0.9739224910736084\n",
      "Epoch: 821 / dLoss: 1.2009663581848145 / gLoss: 0.9526342749595642\n",
      "Epoch: 822 / dLoss: 1.220665454864502 / gLoss: 0.9728248715400696\n",
      "Epoch: 823 / dLoss: 1.1976608037948608 / gLoss: 0.9811411499977112\n",
      "Epoch: 824 / dLoss: 1.2257404327392578 / gLoss: 0.9039364457130432\n",
      "Epoch: 825 / dLoss: 1.2589037418365479 / gLoss: 1.0035120248794556\n",
      "Epoch: 826 / dLoss: 1.1411664485931396 / gLoss: 1.0237175226211548\n",
      "Epoch: 827 / dLoss: 1.2845709323883057 / gLoss: 0.9831117987632751\n",
      "Epoch: 828 / dLoss: 1.22005295753479 / gLoss: 1.092063546180725\n",
      "Epoch: 829 / dLoss: 1.2157108783721924 / gLoss: 1.0335668325424194\n",
      "Epoch: 830 / dLoss: 1.2365002632141113 / gLoss: 0.9597862958908081\n",
      "Epoch: 831 / dLoss: 1.252498984336853 / gLoss: 1.0489574670791626\n",
      "Epoch: 832 / dLoss: 1.194235920906067 / gLoss: 0.9975892901420593\n",
      "Epoch: 833 / dLoss: 1.34462571144104 / gLoss: 0.854866623878479\n",
      "Epoch: 834 / dLoss: 1.2325761318206787 / gLoss: 0.9375\n",
      "Epoch: 835 / dLoss: 1.2069005966186523 / gLoss: 1.0535976886749268\n",
      "Epoch: 836 / dLoss: 1.1601428985595703 / gLoss: 1.1145957708358765\n",
      "Epoch: 837 / dLoss: 1.265479326248169 / gLoss: 1.0014064311981201\n",
      "Epoch: 838 / dLoss: 1.2327244281768799 / gLoss: 0.961662769317627\n",
      "Epoch: 839 / dLoss: 1.28535795211792 / gLoss: 0.9590485095977783\n",
      "Epoch: 840 / dLoss: 1.2377647161483765 / gLoss: 0.9418525099754333\n",
      "Epoch: 841 / dLoss: 1.2053468227386475 / gLoss: 0.9611631035804749\n",
      "Epoch: 842 / dLoss: 1.2538574934005737 / gLoss: 0.8895356059074402\n",
      "Epoch: 843 / dLoss: 1.2384610176086426 / gLoss: 0.9617475271224976\n",
      "Epoch: 844 / dLoss: 1.2675217390060425 / gLoss: 0.9146322011947632\n",
      "Epoch: 845 / dLoss: 1.251574158668518 / gLoss: 0.9398283958435059\n",
      "Epoch: 846 / dLoss: 1.2014299631118774 / gLoss: 0.8875178694725037\n",
      "Epoch: 847 / dLoss: 1.2067351341247559 / gLoss: 0.9521350860595703\n",
      "Epoch: 848 / dLoss: 1.2266016006469727 / gLoss: 1.0238697528839111\n",
      "Epoch: 849 / dLoss: 1.2076714038848877 / gLoss: 1.0398132801055908\n",
      "Epoch: 850 / dLoss: 1.1879158020019531 / gLoss: 0.9392587542533875\n",
      "Epoch: 851 / dLoss: 1.2576971054077148 / gLoss: 1.0188122987747192\n",
      "Epoch: 852 / dLoss: 1.2440811395645142 / gLoss: 1.0193406343460083\n",
      "Epoch: 853 / dLoss: 1.276817798614502 / gLoss: 0.9464663863182068\n",
      "Epoch: 854 / dLoss: 1.1855820417404175 / gLoss: 1.0228997468948364\n",
      "Epoch: 855 / dLoss: 1.1702194213867188 / gLoss: 0.9942997694015503\n",
      "Epoch: 856 / dLoss: 1.1456358432769775 / gLoss: 0.9568032622337341\n",
      "Epoch: 857 / dLoss: 1.1547253131866455 / gLoss: 1.0573091506958008\n",
      "Epoch: 858 / dLoss: 1.0863006114959717 / gLoss: 1.0002977848052979\n",
      "Epoch: 859 / dLoss: 1.1659932136535645 / gLoss: 0.9228151440620422\n",
      "Epoch: 860 / dLoss: 1.192279577255249 / gLoss: 0.8632379174232483\n",
      "Epoch: 861 / dLoss: 1.2505414485931396 / gLoss: 0.9213864207267761\n",
      "Epoch: 862 / dLoss: 1.3329782485961914 / gLoss: 0.9036083817481995\n",
      "Epoch: 863 / dLoss: 1.211792230606079 / gLoss: 0.9355161786079407\n",
      "Epoch: 864 / dLoss: 1.2134028673171997 / gLoss: 1.049333930015564\n",
      "Epoch: 865 / dLoss: 1.2436423301696777 / gLoss: 1.0332375764846802\n",
      "Epoch: 866 / dLoss: 1.257781982421875 / gLoss: 1.119370460510254\n",
      "Epoch: 867 / dLoss: 1.270633578300476 / gLoss: 1.0270392894744873\n",
      "Epoch: 868 / dLoss: 1.2221381664276123 / gLoss: 1.105842113494873\n",
      "Epoch: 869 / dLoss: 1.19613778591156 / gLoss: 1.1128129959106445\n",
      "Epoch: 870 / dLoss: 1.2140138149261475 / gLoss: 0.9983199834823608\n",
      "Epoch: 871 / dLoss: 1.3320565223693848 / gLoss: 0.975899875164032\n",
      "Epoch: 872 / dLoss: 1.1931447982788086 / gLoss: 0.9580192565917969\n",
      "Epoch: 873 / dLoss: 1.2195736169815063 / gLoss: 1.2442251443862915\n",
      "Epoch: 874 / dLoss: 1.201562762260437 / gLoss: 1.1569825410842896\n",
      "Epoch: 875 / dLoss: 1.1529428958892822 / gLoss: 1.1482912302017212\n",
      "Epoch: 876 / dLoss: 1.1145384311676025 / gLoss: 1.0126765966415405\n",
      "Epoch: 877 / dLoss: 1.1947224140167236 / gLoss: 1.0473814010620117\n",
      "Epoch: 878 / dLoss: 1.1603446006774902 / gLoss: 0.9326351881027222\n",
      "Epoch: 879 / dLoss: 1.1867196559906006 / gLoss: 0.9036452770233154\n",
      "Epoch: 880 / dLoss: 1.1987059116363525 / gLoss: 1.059959888458252\n",
      "Epoch: 881 / dLoss: 1.1411981582641602 / gLoss: 0.9958828687667847\n",
      "Epoch: 882 / dLoss: 1.2490849494934082 / gLoss: 0.9111030697822571\n",
      "Epoch: 883 / dLoss: 1.220583438873291 / gLoss: 1.0232117176055908\n",
      "Epoch: 884 / dLoss: 1.2529821395874023 / gLoss: 0.9632688164710999\n",
      "Epoch: 885 / dLoss: 1.269636869430542 / gLoss: 0.9351758360862732\n",
      "Epoch: 886 / dLoss: 1.2031527757644653 / gLoss: 0.9782906174659729\n",
      "Epoch: 887 / dLoss: 1.1603543758392334 / gLoss: 1.0117324590682983\n",
      "Epoch: 888 / dLoss: 1.2354835271835327 / gLoss: 1.0135014057159424\n",
      "Epoch: 889 / dLoss: 1.2022708654403687 / gLoss: 0.86878502368927\n",
      "Epoch: 890 / dLoss: 1.204955816268921 / gLoss: 0.9366342425346375\n",
      "Epoch: 891 / dLoss: 1.1641323566436768 / gLoss: 0.9499411582946777\n",
      "Epoch: 892 / dLoss: 1.2200543880462646 / gLoss: 0.9286693334579468\n",
      "Epoch: 893 / dLoss: 1.15554940700531 / gLoss: 0.9929317235946655\n",
      "Epoch: 894 / dLoss: 1.3294997215270996 / gLoss: 0.9849223494529724\n",
      "Epoch: 895 / dLoss: 1.148329257965088 / gLoss: 1.0468132495880127\n",
      "Epoch: 896 / dLoss: 1.2455439567565918 / gLoss: 0.9015250205993652\n",
      "Epoch: 897 / dLoss: 1.179697036743164 / gLoss: 0.9989233613014221\n",
      "Epoch: 898 / dLoss: 1.2122366428375244 / gLoss: 0.939088761806488\n",
      "Epoch: 899 / dLoss: 1.1252551078796387 / gLoss: 0.9689551591873169\n",
      "Epoch: 900 / dLoss: 1.2291862964630127 / gLoss: 0.9477771520614624\n",
      "Epoch: 901 / dLoss: 1.1178531646728516 / gLoss: 0.9334063529968262\n",
      "Epoch: 902 / dLoss: 1.2896933555603027 / gLoss: 0.9507386684417725\n",
      "Epoch: 903 / dLoss: 1.2742173671722412 / gLoss: 1.015083909034729\n",
      "Epoch: 904 / dLoss: 1.1678400039672852 / gLoss: 0.8878152370452881\n",
      "Epoch: 905 / dLoss: 1.1748955249786377 / gLoss: 0.9252788424491882\n",
      "Epoch: 906 / dLoss: 1.234739899635315 / gLoss: 0.9808087944984436\n",
      "Epoch: 907 / dLoss: 1.1698875427246094 / gLoss: 1.1101603507995605\n",
      "Epoch: 908 / dLoss: 1.1765159368515015 / gLoss: 0.9383910894393921\n",
      "Epoch: 909 / dLoss: 1.1974339485168457 / gLoss: 0.9614563584327698\n",
      "Epoch: 910 / dLoss: 1.2567517757415771 / gLoss: 1.0481427907943726\n",
      "Epoch: 911 / dLoss: 1.1865845918655396 / gLoss: 1.0774877071380615\n",
      "Epoch: 912 / dLoss: 1.2452422380447388 / gLoss: 0.9338963031768799\n",
      "Epoch: 913 / dLoss: 1.2370744943618774 / gLoss: 1.0072321891784668\n",
      "Epoch: 914 / dLoss: 1.1615169048309326 / gLoss: 1.0444598197937012\n",
      "Epoch: 915 / dLoss: 1.226534128189087 / gLoss: 1.094109296798706\n",
      "Epoch: 916 / dLoss: 1.2451739311218262 / gLoss: 0.9841190576553345\n",
      "Epoch: 917 / dLoss: 1.155745029449463 / gLoss: 0.9812120795249939\n",
      "Epoch: 918 / dLoss: 1.2234470844268799 / gLoss: 0.9808112978935242\n",
      "Epoch: 919 / dLoss: 1.2589056491851807 / gLoss: 0.9624128937721252\n",
      "Epoch: 920 / dLoss: 1.2013633251190186 / gLoss: 0.8637690544128418\n",
      "Epoch: 921 / dLoss: 1.0824306011199951 / gLoss: 0.9556936621665955\n",
      "Epoch: 922 / dLoss: 1.1525540351867676 / gLoss: 0.9423620700836182\n",
      "Epoch: 923 / dLoss: 1.2140506505966187 / gLoss: 0.9392335414886475\n",
      "Epoch: 924 / dLoss: 1.2507102489471436 / gLoss: 0.8958436846733093\n",
      "Epoch: 925 / dLoss: 1.155064582824707 / gLoss: 1.0183632373809814\n",
      "Epoch: 926 / dLoss: 1.1781784296035767 / gLoss: 1.0489859580993652\n",
      "Epoch: 927 / dLoss: 1.2091765403747559 / gLoss: 0.8987962603569031\n",
      "Epoch: 928 / dLoss: 1.2245103120803833 / gLoss: 0.9239412546157837\n",
      "Epoch: 929 / dLoss: 1.2542433738708496 / gLoss: 0.8139169812202454\n",
      "Epoch: 930 / dLoss: 1.2742019891738892 / gLoss: 0.9261820912361145\n",
      "Epoch: 931 / dLoss: 1.2151460647583008 / gLoss: 0.9848752021789551\n",
      "Epoch: 932 / dLoss: 1.2831127643585205 / gLoss: 1.102657675743103\n",
      "Epoch: 933 / dLoss: 1.2264783382415771 / gLoss: 0.9351062178611755\n",
      "Epoch: 934 / dLoss: 1.124914526939392 / gLoss: 1.1552157402038574\n",
      "Epoch: 935 / dLoss: 1.2355897426605225 / gLoss: 1.1021651029586792\n",
      "Epoch: 936 / dLoss: 1.2983299493789673 / gLoss: 0.9917160868644714\n",
      "Epoch: 937 / dLoss: 1.1470024585723877 / gLoss: 0.9200249314308167\n",
      "Epoch: 938 / dLoss: 1.230773687362671 / gLoss: 0.9473793506622314\n",
      "Epoch: 939 / dLoss: 1.0989346504211426 / gLoss: 0.9137477874755859\n",
      "Epoch: 940 / dLoss: 1.1061160564422607 / gLoss: 0.9757143259048462\n",
      "Epoch: 941 / dLoss: 1.195563793182373 / gLoss: 0.9572514891624451\n",
      "Epoch: 942 / dLoss: 1.1704533100128174 / gLoss: 0.8999230265617371\n",
      "Epoch: 943 / dLoss: 1.1618516445159912 / gLoss: 0.8871288299560547\n",
      "Epoch: 944 / dLoss: 1.200355052947998 / gLoss: 1.035606861114502\n",
      "Epoch: 945 / dLoss: 1.1553481817245483 / gLoss: 1.1138643026351929\n",
      "Epoch: 946 / dLoss: 1.1828243732452393 / gLoss: 0.9431931376457214\n",
      "Epoch: 947 / dLoss: 1.112831473350525 / gLoss: 1.047229290008545\n",
      "Epoch: 948 / dLoss: 1.1696332693099976 / gLoss: 1.0111970901489258\n",
      "Epoch: 949 / dLoss: 1.2269294261932373 / gLoss: 0.9640494585037231\n",
      "Epoch: 950 / dLoss: 1.1548972129821777 / gLoss: 0.969086229801178\n",
      "Epoch: 951 / dLoss: 1.2070316076278687 / gLoss: 1.065976619720459\n",
      "Epoch: 952 / dLoss: 1.2489604949951172 / gLoss: 1.1389635801315308\n",
      "Epoch: 953 / dLoss: 1.1766223907470703 / gLoss: 1.0578370094299316\n",
      "Epoch: 954 / dLoss: 1.1065020561218262 / gLoss: 1.0377782583236694\n",
      "Epoch: 955 / dLoss: 1.2578318119049072 / gLoss: 1.0513384342193604\n",
      "Epoch: 956 / dLoss: 1.218296766281128 / gLoss: 1.0093449354171753\n",
      "Epoch: 957 / dLoss: 1.1605874300003052 / gLoss: 0.9380016922950745\n",
      "Epoch: 958 / dLoss: 1.2427891492843628 / gLoss: 1.0088647603988647\n",
      "Epoch: 959 / dLoss: 1.2260065078735352 / gLoss: 0.9315773248672485\n",
      "Epoch: 960 / dLoss: 1.1828033924102783 / gLoss: 0.9629875421524048\n",
      "Epoch: 961 / dLoss: 1.1913552284240723 / gLoss: 0.9303961396217346\n",
      "Epoch: 962 / dLoss: 1.239284873008728 / gLoss: 1.0232666730880737\n",
      "Epoch: 963 / dLoss: 1.1506086587905884 / gLoss: 1.0063261985778809\n",
      "Epoch: 964 / dLoss: 1.2251806259155273 / gLoss: 0.9339278340339661\n",
      "Epoch: 965 / dLoss: 1.1540822982788086 / gLoss: 0.980940043926239\n",
      "Epoch: 966 / dLoss: 1.1814641952514648 / gLoss: 1.0779364109039307\n",
      "Epoch: 967 / dLoss: 1.18546462059021 / gLoss: 1.0679086446762085\n",
      "Epoch: 968 / dLoss: 1.2418930530548096 / gLoss: 0.9885785579681396\n",
      "Epoch: 969 / dLoss: 1.2438678741455078 / gLoss: 0.9067034721374512\n",
      "Epoch: 970 / dLoss: 1.182470679283142 / gLoss: 0.9557037353515625\n",
      "Epoch: 971 / dLoss: 1.1953647136688232 / gLoss: 0.9991801381111145\n",
      "Epoch: 972 / dLoss: 1.1806225776672363 / gLoss: 1.0040477514266968\n",
      "Epoch: 973 / dLoss: 1.1560354232788086 / gLoss: 1.0207582712173462\n",
      "Epoch: 974 / dLoss: 1.1503247022628784 / gLoss: 1.0391887426376343\n",
      "Epoch: 975 / dLoss: 1.1928519010543823 / gLoss: 0.9953503608703613\n",
      "Epoch: 976 / dLoss: 1.166187047958374 / gLoss: 0.8700463175773621\n",
      "Epoch: 977 / dLoss: 1.1811575889587402 / gLoss: 0.9507893919944763\n",
      "Epoch: 978 / dLoss: 1.1432099342346191 / gLoss: 0.9095507860183716\n",
      "Epoch: 979 / dLoss: 1.173954963684082 / gLoss: 1.0387247800827026\n",
      "Epoch: 980 / dLoss: 1.1977958679199219 / gLoss: 0.9222322106361389\n",
      "Epoch: 981 / dLoss: 1.2154496908187866 / gLoss: 1.0252668857574463\n",
      "Epoch: 982 / dLoss: 1.2342007160186768 / gLoss: 0.9593415856361389\n",
      "Epoch: 983 / dLoss: 1.1752660274505615 / gLoss: 0.9897186756134033\n",
      "Epoch: 984 / dLoss: 1.1811398267745972 / gLoss: 0.9805548191070557\n",
      "Epoch: 985 / dLoss: 1.1536540985107422 / gLoss: 0.9517977237701416\n",
      "Epoch: 986 / dLoss: 1.1493496894836426 / gLoss: 1.0879111289978027\n",
      "Epoch: 987 / dLoss: 1.1924067735671997 / gLoss: 1.0809534788131714\n",
      "Epoch: 988 / dLoss: 1.2174780368804932 / gLoss: 1.0700150728225708\n",
      "Epoch: 989 / dLoss: 1.2189993858337402 / gLoss: 1.0685067176818848\n",
      "Epoch: 990 / dLoss: 1.2172735929489136 / gLoss: 0.9959284067153931\n",
      "Epoch: 991 / dLoss: 1.2884453535079956 / gLoss: 1.0102099180221558\n",
      "Epoch: 992 / dLoss: 1.118466854095459 / gLoss: 1.0327658653259277\n",
      "Epoch: 993 / dLoss: 1.2558350563049316 / gLoss: 1.0258039236068726\n",
      "Epoch: 994 / dLoss: 1.1478216648101807 / gLoss: 0.8246577382087708\n",
      "Epoch: 995 / dLoss: 1.1695454120635986 / gLoss: 1.0658549070358276\n",
      "Epoch: 996 / dLoss: 1.201757788658142 / gLoss: 0.9228809475898743\n",
      "Epoch: 997 / dLoss: 1.130054235458374 / gLoss: 0.9506111741065979\n",
      "Epoch: 998 / dLoss: 1.2064790725708008 / gLoss: 0.9906814694404602\n",
      "Epoch: 999 / dLoss: 1.1816754341125488 / gLoss: 0.9330583810806274\n",
      "Epoch: 1000 / dLoss: 1.2310878038406372 / gLoss: 0.9925668239593506\n",
      "Epoch: 1001 / dLoss: 1.2289384603500366 / gLoss: 1.0275938510894775\n",
      "Epoch: 1002 / dLoss: 1.168184757232666 / gLoss: 1.0079104900360107\n",
      "Epoch: 1003 / dLoss: 1.1706035137176514 / gLoss: 1.1107854843139648\n",
      "Epoch: 1004 / dLoss: 1.1474453210830688 / gLoss: 1.1099379062652588\n",
      "Epoch: 1005 / dLoss: 1.2134766578674316 / gLoss: 1.015048861503601\n",
      "Epoch: 1006 / dLoss: 1.200626254081726 / gLoss: 0.9102100133895874\n",
      "Epoch: 1007 / dLoss: 1.1292978525161743 / gLoss: 1.1328067779541016\n",
      "Epoch: 1008 / dLoss: 1.2238810062408447 / gLoss: 0.9283989667892456\n",
      "Epoch: 1009 / dLoss: 1.214057445526123 / gLoss: 1.0287182331085205\n",
      "Epoch: 1010 / dLoss: 1.1007405519485474 / gLoss: 1.0284736156463623\n",
      "Epoch: 1011 / dLoss: 1.2001703977584839 / gLoss: 0.9885475635528564\n",
      "Epoch: 1012 / dLoss: 1.1210280656814575 / gLoss: 0.9952160716056824\n",
      "Epoch: 1013 / dLoss: 1.0999560356140137 / gLoss: 1.0028196573257446\n",
      "Epoch: 1014 / dLoss: 1.1578428745269775 / gLoss: 0.9224428534507751\n",
      "Epoch: 1015 / dLoss: 1.1587443351745605 / gLoss: 0.9405754804611206\n",
      "Epoch: 1016 / dLoss: 1.1172322034835815 / gLoss: 0.95876145362854\n",
      "Epoch: 1017 / dLoss: 1.1364731788635254 / gLoss: 1.0220093727111816\n",
      "Epoch: 1018 / dLoss: 1.27329683303833 / gLoss: 0.9233493804931641\n",
      "Epoch: 1019 / dLoss: 1.20310640335083 / gLoss: 0.9592339396476746\n",
      "Epoch: 1020 / dLoss: 1.221418023109436 / gLoss: 1.0874991416931152\n",
      "Epoch: 1021 / dLoss: 1.1501774787902832 / gLoss: 1.0714243650436401\n",
      "Epoch: 1022 / dLoss: 1.1719574928283691 / gLoss: 0.9195184111595154\n",
      "Epoch: 1023 / dLoss: 1.1639338731765747 / gLoss: 0.9862761497497559\n",
      "Epoch: 1024 / dLoss: 1.2187976837158203 / gLoss: 1.0330731868743896\n",
      "Epoch: 1025 / dLoss: 1.1603693962097168 / gLoss: 0.9627052545547485\n",
      "Epoch: 1026 / dLoss: 1.1141502857208252 / gLoss: 1.0154142379760742\n",
      "Epoch: 1027 / dLoss: 1.261824369430542 / gLoss: 1.0252741575241089\n",
      "Epoch: 1028 / dLoss: 1.2052252292633057 / gLoss: 0.9330264925956726\n",
      "Epoch: 1029 / dLoss: 1.1575868129730225 / gLoss: 0.9920305609703064\n",
      "Epoch: 1030 / dLoss: 1.182676911354065 / gLoss: 1.094865083694458\n",
      "Epoch: 1031 / dLoss: 1.1944094896316528 / gLoss: 1.0679534673690796\n",
      "Epoch: 1032 / dLoss: 1.1865513324737549 / gLoss: 1.1742465496063232\n",
      "Epoch: 1033 / dLoss: 1.1390390396118164 / gLoss: 1.017753005027771\n",
      "Epoch: 1034 / dLoss: 1.2946431636810303 / gLoss: 0.9279087781906128\n",
      "Epoch: 1035 / dLoss: 1.1615111827850342 / gLoss: 0.9749400615692139\n",
      "Epoch: 1036 / dLoss: 1.2017438411712646 / gLoss: 0.9639864563941956\n",
      "Epoch: 1037 / dLoss: 1.1618082523345947 / gLoss: 0.9887098670005798\n",
      "Epoch: 1038 / dLoss: 1.2323670387268066 / gLoss: 1.0631020069122314\n",
      "Epoch: 1039 / dLoss: 1.0945875644683838 / gLoss: 1.0285258293151855\n",
      "Epoch: 1040 / dLoss: 1.1846728324890137 / gLoss: 1.0445210933685303\n",
      "Epoch: 1041 / dLoss: 1.2449908256530762 / gLoss: 1.0163164138793945\n",
      "Epoch: 1042 / dLoss: 1.1633734703063965 / gLoss: 1.0089547634124756\n",
      "Epoch: 1043 / dLoss: 1.1754488945007324 / gLoss: 0.9767952561378479\n",
      "Epoch: 1044 / dLoss: 1.1540812253952026 / gLoss: 0.9754741787910461\n",
      "Epoch: 1045 / dLoss: 1.1557786464691162 / gLoss: 0.9938919544219971\n",
      "Epoch: 1046 / dLoss: 1.2157926559448242 / gLoss: 1.0396713018417358\n",
      "Epoch: 1047 / dLoss: 1.1872458457946777 / gLoss: 1.0146510601043701\n",
      "Epoch: 1048 / dLoss: 1.1732773780822754 / gLoss: 0.9601181149482727\n",
      "Epoch: 1049 / dLoss: 1.2561092376708984 / gLoss: 0.9899716377258301\n",
      "Epoch: 1050 / dLoss: 1.1578313112258911 / gLoss: 1.0111216306686401\n",
      "Epoch: 1051 / dLoss: 1.2686799764633179 / gLoss: 0.9659773111343384\n",
      "Epoch: 1052 / dLoss: 1.1766211986541748 / gLoss: 1.03074312210083\n",
      "Epoch: 1053 / dLoss: 1.1256520748138428 / gLoss: 0.9491174817085266\n",
      "Epoch: 1054 / dLoss: 1.22396981716156 / gLoss: 1.0486444234848022\n",
      "Epoch: 1055 / dLoss: 1.1814703941345215 / gLoss: 0.9899203777313232\n",
      "Epoch: 1056 / dLoss: 1.1983205080032349 / gLoss: 0.9788685441017151\n",
      "Epoch: 1057 / dLoss: 1.1490259170532227 / gLoss: 0.9881016612052917\n",
      "Epoch: 1058 / dLoss: 1.1829900741577148 / gLoss: 1.0318983793258667\n",
      "Epoch: 1059 / dLoss: 1.2558190822601318 / gLoss: 0.9718579649925232\n",
      "Epoch: 1060 / dLoss: 1.1503536701202393 / gLoss: 0.9402283430099487\n",
      "Epoch: 1061 / dLoss: 1.1798481941223145 / gLoss: 1.0167478322982788\n",
      "Epoch: 1062 / dLoss: 1.2194693088531494 / gLoss: 0.9783711433410645\n",
      "Epoch: 1063 / dLoss: 1.087151050567627 / gLoss: 0.9155885577201843\n",
      "Epoch: 1064 / dLoss: 1.1394472122192383 / gLoss: 0.9611436724662781\n",
      "Epoch: 1065 / dLoss: 1.1973572969436646 / gLoss: 1.0138801336288452\n",
      "Epoch: 1066 / dLoss: 1.1290260553359985 / gLoss: 0.9487106800079346\n",
      "Epoch: 1067 / dLoss: 1.1214532852172852 / gLoss: 1.093253254890442\n",
      "Epoch: 1068 / dLoss: 1.2110517024993896 / gLoss: 1.0944584608078003\n",
      "Epoch: 1069 / dLoss: 1.1754469871520996 / gLoss: 1.2425545454025269\n",
      "Epoch: 1070 / dLoss: 1.1986258029937744 / gLoss: 1.0557008981704712\n",
      "Epoch: 1071 / dLoss: 1.1530652046203613 / gLoss: 1.1460254192352295\n",
      "Epoch: 1072 / dLoss: 1.1452796459197998 / gLoss: 1.0672543048858643\n",
      "Epoch: 1073 / dLoss: 1.163832664489746 / gLoss: 1.010865569114685\n",
      "Epoch: 1074 / dLoss: 1.1501185894012451 / gLoss: 0.9640254974365234\n",
      "Epoch: 1075 / dLoss: 1.1669273376464844 / gLoss: 1.021222710609436\n",
      "Epoch: 1076 / dLoss: 1.1855477094650269 / gLoss: 1.0115957260131836\n",
      "Epoch: 1077 / dLoss: 1.2171931266784668 / gLoss: 0.9474857449531555\n",
      "Epoch: 1078 / dLoss: 1.191884994506836 / gLoss: 1.0859689712524414\n",
      "Epoch: 1079 / dLoss: 1.1833584308624268 / gLoss: 1.0527963638305664\n",
      "Epoch: 1080 / dLoss: 1.145463228225708 / gLoss: 1.095060110092163\n",
      "Epoch: 1081 / dLoss: 1.0879732370376587 / gLoss: 1.0030906200408936\n",
      "Epoch: 1082 / dLoss: 1.2236666679382324 / gLoss: 1.0015840530395508\n",
      "Epoch: 1083 / dLoss: 1.14699125289917 / gLoss: 1.0313125848770142\n",
      "Epoch: 1084 / dLoss: 1.1148881912231445 / gLoss: 1.0971795320510864\n",
      "Epoch: 1085 / dLoss: 1.1162704229354858 / gLoss: 1.2538460493087769\n",
      "Epoch: 1086 / dLoss: 1.0785596370697021 / gLoss: 1.1679528951644897\n",
      "Epoch: 1087 / dLoss: 1.109412431716919 / gLoss: 1.0275418758392334\n",
      "Epoch: 1088 / dLoss: 1.1612493991851807 / gLoss: 0.9647554159164429\n",
      "Epoch: 1089 / dLoss: 1.2666244506835938 / gLoss: 1.0203139781951904\n",
      "Epoch: 1090 / dLoss: 1.1515107154846191 / gLoss: 0.9975966811180115\n",
      "Epoch: 1091 / dLoss: 1.2959082126617432 / gLoss: 0.9923381209373474\n",
      "Epoch: 1092 / dLoss: 1.1825437545776367 / gLoss: 0.9132739901542664\n",
      "Epoch: 1093 / dLoss: 1.0959570407867432 / gLoss: 1.1265615224838257\n",
      "Epoch: 1094 / dLoss: 1.11683988571167 / gLoss: 1.1152247190475464\n",
      "Epoch: 1095 / dLoss: 1.0910168886184692 / gLoss: 1.1624689102172852\n",
      "Epoch: 1096 / dLoss: 1.0522921085357666 / gLoss: 1.122761845588684\n",
      "Epoch: 1097 / dLoss: 1.1923596858978271 / gLoss: 1.0032031536102295\n",
      "Epoch: 1098 / dLoss: 1.1707775592803955 / gLoss: 1.0066224336624146\n",
      "Epoch: 1099 / dLoss: 1.1782758235931396 / gLoss: 1.037062168121338\n",
      "Epoch: 1100 / dLoss: 1.2839412689208984 / gLoss: 1.0435991287231445\n",
      "Epoch: 1101 / dLoss: 1.189204216003418 / gLoss: 1.1233491897583008\n",
      "Epoch: 1102 / dLoss: 1.0798046588897705 / gLoss: 1.2335426807403564\n",
      "Epoch: 1103 / dLoss: 1.1264927387237549 / gLoss: 1.0860482454299927\n",
      "Epoch: 1104 / dLoss: 1.0359644889831543 / gLoss: 0.9876875281333923\n",
      "Epoch: 1105 / dLoss: 1.1364845037460327 / gLoss: 1.1322906017303467\n",
      "Epoch: 1106 / dLoss: 1.0373725891113281 / gLoss: 1.1651713848114014\n",
      "Epoch: 1107 / dLoss: 1.258500576019287 / gLoss: 1.0376567840576172\n",
      "Epoch: 1108 / dLoss: 1.2514197826385498 / gLoss: 0.9569708108901978\n",
      "Epoch: 1109 / dLoss: 1.0772756338119507 / gLoss: 0.8821556568145752\n",
      "Epoch: 1110 / dLoss: 1.2328968048095703 / gLoss: 0.9488484263420105\n",
      "Epoch: 1111 / dLoss: 1.1405733823776245 / gLoss: 0.9918527007102966\n",
      "Epoch: 1112 / dLoss: 1.105823278427124 / gLoss: 1.2354506254196167\n",
      "Epoch: 1113 / dLoss: 1.089668869972229 / gLoss: 1.1851305961608887\n",
      "Epoch: 1114 / dLoss: 1.0560088157653809 / gLoss: 1.1027113199234009\n",
      "Epoch: 1115 / dLoss: 1.1974003314971924 / gLoss: 1.1741846799850464\n",
      "Epoch: 1116 / dLoss: 1.152097225189209 / gLoss: 1.044029951095581\n",
      "Epoch: 1117 / dLoss: 1.1964378356933594 / gLoss: 1.0891311168670654\n",
      "Epoch: 1118 / dLoss: 1.1384105682373047 / gLoss: 1.2651875019073486\n",
      "Epoch: 1119 / dLoss: 1.2174322605133057 / gLoss: 1.2296143770217896\n",
      "Epoch: 1120 / dLoss: 1.1512000560760498 / gLoss: 1.249693512916565\n",
      "Epoch: 1121 / dLoss: 1.0536229610443115 / gLoss: 1.318040132522583\n",
      "Epoch: 1122 / dLoss: 1.0461338758468628 / gLoss: 1.302187204360962\n",
      "Epoch: 1123 / dLoss: 1.1636728048324585 / gLoss: 1.2370219230651855\n",
      "Epoch: 1124 / dLoss: 1.0493476390838623 / gLoss: 1.2376936674118042\n",
      "Epoch: 1125 / dLoss: 1.0856847763061523 / gLoss: 0.9873704314231873\n",
      "Epoch: 1126 / dLoss: 1.2059011459350586 / gLoss: 0.9418937563896179\n",
      "Epoch: 1127 / dLoss: 1.3811073303222656 / gLoss: 0.9341574311256409\n",
      "Epoch: 1128 / dLoss: 1.2330379486083984 / gLoss: 0.8460468053817749\n",
      "Epoch: 1129 / dLoss: 1.3018360137939453 / gLoss: 0.8923478126525879\n",
      "Epoch: 1130 / dLoss: 1.1965073347091675 / gLoss: 0.9694037437438965\n",
      "Epoch: 1131 / dLoss: 1.0875613689422607 / gLoss: 1.1265146732330322\n",
      "Epoch: 1132 / dLoss: 1.101382851600647 / gLoss: 1.350260615348816\n",
      "Epoch: 1133 / dLoss: 1.121544361114502 / gLoss: 1.2384178638458252\n",
      "Epoch: 1134 / dLoss: 1.0949211120605469 / gLoss: 1.1405775547027588\n",
      "Epoch: 1135 / dLoss: 1.135076642036438 / gLoss: 1.120798110961914\n",
      "Epoch: 1136 / dLoss: 1.1439675092697144 / gLoss: 1.1173012256622314\n",
      "Epoch: 1137 / dLoss: 1.2769602537155151 / gLoss: 0.9872486591339111\n",
      "Epoch: 1138 / dLoss: 1.2515592575073242 / gLoss: 1.0879021883010864\n",
      "Epoch: 1139 / dLoss: 1.2527166604995728 / gLoss: 1.1190258264541626\n",
      "Epoch: 1140 / dLoss: 1.1601183414459229 / gLoss: 1.3877062797546387\n",
      "Epoch: 1141 / dLoss: 1.078590750694275 / gLoss: 1.453965425491333\n",
      "Epoch: 1142 / dLoss: 1.0218479633331299 / gLoss: 1.4403854608535767\n",
      "Epoch: 1143 / dLoss: 1.0825867652893066 / gLoss: 1.2490127086639404\n",
      "Epoch: 1144 / dLoss: 0.9072504639625549 / gLoss: 1.3193418979644775\n",
      "Epoch: 1145 / dLoss: 1.1785669326782227 / gLoss: 1.075708031654358\n",
      "Epoch: 1146 / dLoss: 1.096282958984375 / gLoss: 1.1522529125213623\n",
      "Epoch: 1147 / dLoss: 1.0830471515655518 / gLoss: 1.090222716331482\n",
      "Epoch: 1148 / dLoss: 1.2717616558074951 / gLoss: 0.9151933789253235\n",
      "Epoch: 1149 / dLoss: 1.272918462753296 / gLoss: 0.8017033338546753\n",
      "Epoch: 1150 / dLoss: 1.195116400718689 / gLoss: 0.8269100189208984\n",
      "Epoch: 1151 / dLoss: 1.1837661266326904 / gLoss: 0.8951445817947388\n",
      "Epoch: 1152 / dLoss: 1.0974397659301758 / gLoss: 1.250501275062561\n",
      "Epoch: 1153 / dLoss: 1.0789988040924072 / gLoss: 1.2111971378326416\n",
      "Epoch: 1154 / dLoss: 1.041268229484558 / gLoss: 1.1938481330871582\n",
      "Epoch: 1155 / dLoss: 1.0863244533538818 / gLoss: 1.1574665307998657\n",
      "Epoch: 1156 / dLoss: 1.047694206237793 / gLoss: 1.2519938945770264\n",
      "Epoch: 1157 / dLoss: 1.1051833629608154 / gLoss: 1.2171006202697754\n",
      "Epoch: 1158 / dLoss: 1.2228963375091553 / gLoss: 1.0414128303527832\n",
      "Epoch: 1159 / dLoss: 1.0998139381408691 / gLoss: 1.0965535640716553\n",
      "Epoch: 1160 / dLoss: 1.2233619689941406 / gLoss: 1.2125709056854248\n",
      "Epoch: 1161 / dLoss: 1.1276273727416992 / gLoss: 1.1088486909866333\n",
      "Epoch: 1162 / dLoss: 1.145448923110962 / gLoss: 1.1587923765182495\n",
      "Epoch: 1163 / dLoss: 1.2181544303894043 / gLoss: 0.996463418006897\n",
      "Epoch: 1164 / dLoss: 1.1332051753997803 / gLoss: 1.1547825336456299\n",
      "Epoch: 1165 / dLoss: 1.039487361907959 / gLoss: 1.1600925922393799\n",
      "Epoch: 1166 / dLoss: 1.0899957418441772 / gLoss: 1.083168625831604\n",
      "Epoch: 1167 / dLoss: 1.1504275798797607 / gLoss: 1.1913909912109375\n",
      "Epoch: 1168 / dLoss: 1.0537508726119995 / gLoss: 1.3041974306106567\n",
      "Epoch: 1169 / dLoss: 1.1615517139434814 / gLoss: 1.1285191774368286\n",
      "Epoch: 1170 / dLoss: 1.1507134437561035 / gLoss: 0.8872725963592529\n",
      "Epoch: 1171 / dLoss: 1.1166386604309082 / gLoss: 1.003986120223999\n",
      "Epoch: 1172 / dLoss: 1.1715797185897827 / gLoss: 0.9905470013618469\n",
      "Epoch: 1173 / dLoss: 1.1119178533554077 / gLoss: 0.9493557810783386\n",
      "Epoch: 1174 / dLoss: 1.116774320602417 / gLoss: 0.9527806043624878\n",
      "Epoch: 1175 / dLoss: 1.0754834413528442 / gLoss: 1.0999282598495483\n",
      "Epoch: 1176 / dLoss: 1.101480484008789 / gLoss: 1.1036607027053833\n",
      "Epoch: 1177 / dLoss: 1.038801908493042 / gLoss: 1.1425203084945679\n",
      "Epoch: 1178 / dLoss: 1.2344918251037598 / gLoss: 1.104071021080017\n",
      "Epoch: 1179 / dLoss: 1.0997265577316284 / gLoss: 1.0708235502243042\n",
      "Epoch: 1180 / dLoss: 1.1661230325698853 / gLoss: 1.1015342473983765\n",
      "Epoch: 1181 / dLoss: 1.1467607021331787 / gLoss: 1.0546042919158936\n",
      "Epoch: 1182 / dLoss: 1.1583715677261353 / gLoss: 1.086337924003601\n",
      "Epoch: 1183 / dLoss: 1.1506637334823608 / gLoss: 1.299232006072998\n",
      "Epoch: 1184 / dLoss: 1.1085740327835083 / gLoss: 1.3403106927871704\n",
      "Epoch: 1185 / dLoss: 1.1368067264556885 / gLoss: 1.2987529039382935\n",
      "Epoch: 1186 / dLoss: 1.0991113185882568 / gLoss: 1.180214762687683\n",
      "Epoch: 1187 / dLoss: 1.056645393371582 / gLoss: 1.179145336151123\n",
      "Epoch: 1188 / dLoss: 1.1063611507415771 / gLoss: 1.1330993175506592\n",
      "Epoch: 1189 / dLoss: 0.983971357345581 / gLoss: 1.2002918720245361\n",
      "Epoch: 1190 / dLoss: 1.1697499752044678 / gLoss: 1.0424022674560547\n",
      "Epoch: 1191 / dLoss: 1.1433624029159546 / gLoss: 1.0239784717559814\n",
      "Epoch: 1192 / dLoss: 1.1815502643585205 / gLoss: 0.9209824204444885\n",
      "Epoch: 1193 / dLoss: 1.1638472080230713 / gLoss: 0.9913811683654785\n",
      "Epoch: 1194 / dLoss: 1.091707706451416 / gLoss: 1.0653280019760132\n",
      "Epoch: 1195 / dLoss: 1.1319948434829712 / gLoss: 1.0160359144210815\n",
      "Epoch: 1196 / dLoss: 1.1307317018508911 / gLoss: 1.0635625123977661\n",
      "Epoch: 1197 / dLoss: 1.1047724485397339 / gLoss: 0.9742935299873352\n",
      "Epoch: 1198 / dLoss: 1.0731977224349976 / gLoss: 1.1355961561203003\n",
      "Epoch: 1199 / dLoss: 1.0974534749984741 / gLoss: 1.115272879600525\n",
      "Epoch: 1200 / dLoss: 1.1286605596542358 / gLoss: 1.0591288805007935\n",
      "Epoch: 1201 / dLoss: 1.165382742881775 / gLoss: 1.107398271560669\n",
      "Epoch: 1202 / dLoss: 1.1194558143615723 / gLoss: 1.0651439428329468\n",
      "Epoch: 1203 / dLoss: 1.1787995100021362 / gLoss: 1.1130226850509644\n",
      "Epoch: 1204 / dLoss: 1.129991054534912 / gLoss: 1.1599806547164917\n",
      "Epoch: 1205 / dLoss: 1.1992604732513428 / gLoss: 1.242305040359497\n",
      "Epoch: 1206 / dLoss: 1.1729326248168945 / gLoss: 1.1251659393310547\n",
      "Epoch: 1207 / dLoss: 1.1750171184539795 / gLoss: 1.095168948173523\n",
      "Epoch: 1208 / dLoss: 1.0849061012268066 / gLoss: 0.9876901507377625\n",
      "Epoch: 1209 / dLoss: 1.2076990604400635 / gLoss: 1.1785889863967896\n",
      "Epoch: 1210 / dLoss: 1.1385283470153809 / gLoss: 1.162093997001648\n",
      "Epoch: 1211 / dLoss: 1.1637437343597412 / gLoss: 0.9327396154403687\n",
      "Epoch: 1212 / dLoss: 1.200338363647461 / gLoss: 1.0923588275909424\n",
      "Epoch: 1213 / dLoss: 1.128199815750122 / gLoss: 1.108208417892456\n",
      "Epoch: 1214 / dLoss: 1.0870389938354492 / gLoss: 1.0753077268600464\n",
      "Epoch: 1215 / dLoss: 1.0790188312530518 / gLoss: 0.9536756277084351\n",
      "Epoch: 1216 / dLoss: 1.136634111404419 / gLoss: 1.0549969673156738\n",
      "Epoch: 1217 / dLoss: 1.1794118881225586 / gLoss: 0.9676143527030945\n",
      "Epoch: 1218 / dLoss: 1.1155059337615967 / gLoss: 1.0775870084762573\n",
      "Epoch: 1219 / dLoss: 1.1258800029754639 / gLoss: 1.1174825429916382\n",
      "Epoch: 1220 / dLoss: 1.071298360824585 / gLoss: 1.1589570045471191\n",
      "Epoch: 1221 / dLoss: 1.0808528661727905 / gLoss: 1.0883845090866089\n",
      "Epoch: 1222 / dLoss: 1.132336139678955 / gLoss: 1.2443031072616577\n",
      "Epoch: 1223 / dLoss: 1.1277849674224854 / gLoss: 1.0460362434387207\n",
      "Epoch: 1224 / dLoss: 1.1583166122436523 / gLoss: 1.073634386062622\n",
      "Epoch: 1225 / dLoss: 1.1133159399032593 / gLoss: 1.0435067415237427\n",
      "Epoch: 1226 / dLoss: 0.9898276329040527 / gLoss: 1.1848045587539673\n",
      "Epoch: 1227 / dLoss: 1.0530318021774292 / gLoss: 1.0622817277908325\n",
      "Epoch: 1228 / dLoss: 1.1164839267730713 / gLoss: 1.1963374614715576\n",
      "Epoch: 1229 / dLoss: 1.0694972276687622 / gLoss: 1.2732410430908203\n",
      "Epoch: 1230 / dLoss: 1.0963892936706543 / gLoss: 1.2586889266967773\n",
      "Epoch: 1231 / dLoss: 1.0615578889846802 / gLoss: 1.1172430515289307\n",
      "Epoch: 1232 / dLoss: 1.0582597255706787 / gLoss: 1.2182488441467285\n",
      "Epoch: 1233 / dLoss: 1.085397720336914 / gLoss: 1.1795334815979004\n",
      "Epoch: 1234 / dLoss: 1.1530191898345947 / gLoss: 1.0400530099868774\n",
      "Epoch: 1235 / dLoss: 1.1705585718154907 / gLoss: 1.039742112159729\n",
      "Epoch: 1236 / dLoss: 1.100738286972046 / gLoss: 1.0065605640411377\n",
      "Epoch: 1237 / dLoss: 1.1531699895858765 / gLoss: 1.0125118494033813\n",
      "Epoch: 1238 / dLoss: 1.05860435962677 / gLoss: 1.0579164028167725\n",
      "Epoch: 1239 / dLoss: 1.05330491065979 / gLoss: 1.0888876914978027\n",
      "Epoch: 1240 / dLoss: 1.1413904428482056 / gLoss: 1.0446795225143433\n",
      "Epoch: 1241 / dLoss: 1.0055382251739502 / gLoss: 1.1571879386901855\n",
      "Epoch: 1242 / dLoss: 1.0216388702392578 / gLoss: 1.1492856740951538\n",
      "Epoch: 1243 / dLoss: 1.1078479290008545 / gLoss: 1.0873043537139893\n",
      "Epoch: 1244 / dLoss: 1.246976375579834 / gLoss: 1.0892434120178223\n",
      "Epoch: 1245 / dLoss: 1.1026251316070557 / gLoss: 1.0625321865081787\n",
      "Epoch: 1246 / dLoss: 1.2024121284484863 / gLoss: 1.1926196813583374\n",
      "Epoch: 1247 / dLoss: 1.2621965408325195 / gLoss: 1.1393736600875854\n",
      "Epoch: 1248 / dLoss: 1.0456225872039795 / gLoss: 1.1584134101867676\n",
      "Epoch: 1249 / dLoss: 1.0989007949829102 / gLoss: 1.1576471328735352\n",
      "Epoch: 1250 / dLoss: 1.1249878406524658 / gLoss: 1.1551003456115723\n",
      "Epoch: 1251 / dLoss: 1.0209922790527344 / gLoss: 1.1016993522644043\n",
      "Epoch: 1252 / dLoss: 1.0818911790847778 / gLoss: 1.1935458183288574\n",
      "Epoch: 1253 / dLoss: 1.0123488903045654 / gLoss: 1.295040249824524\n",
      "Epoch: 1254 / dLoss: 1.1324191093444824 / gLoss: 1.1255906820297241\n",
      "Epoch: 1255 / dLoss: 1.1094557046890259 / gLoss: 0.9961406588554382\n",
      "Epoch: 1256 / dLoss: 1.0326610803604126 / gLoss: 1.0507208108901978\n",
      "Epoch: 1257 / dLoss: 1.1227848529815674 / gLoss: 1.0534292459487915\n",
      "Epoch: 1258 / dLoss: 1.1635539531707764 / gLoss: 1.1157642602920532\n",
      "Epoch: 1259 / dLoss: 1.030583143234253 / gLoss: 1.1334515810012817\n",
      "Epoch: 1260 / dLoss: 0.9875285625457764 / gLoss: 1.2401692867279053\n",
      "Epoch: 1261 / dLoss: 1.0276411771774292 / gLoss: 1.2362558841705322\n",
      "Epoch: 1262 / dLoss: 1.1553924083709717 / gLoss: 1.099316954612732\n",
      "Epoch: 1263 / dLoss: 1.089743971824646 / gLoss: 1.1846989393234253\n",
      "Epoch: 1264 / dLoss: 0.9988947510719299 / gLoss: 1.0831142663955688\n",
      "Epoch: 1265 / dLoss: 1.0542699098587036 / gLoss: 1.0668752193450928\n",
      "Epoch: 1266 / dLoss: 1.0217570066452026 / gLoss: 1.1183722019195557\n",
      "Epoch: 1267 / dLoss: 1.0977590084075928 / gLoss: 1.141229271888733\n",
      "Epoch: 1268 / dLoss: 1.2018957138061523 / gLoss: 1.0662198066711426\n",
      "Epoch: 1269 / dLoss: 1.20364248752594 / gLoss: 1.1212555170059204\n",
      "Epoch: 1270 / dLoss: 0.9797109365463257 / gLoss: 1.2168408632278442\n",
      "Epoch: 1271 / dLoss: 1.1387455463409424 / gLoss: 1.2647093534469604\n",
      "Epoch: 1272 / dLoss: 1.0450419187545776 / gLoss: 1.3240580558776855\n",
      "Epoch: 1273 / dLoss: 1.0894149541854858 / gLoss: 1.2154523134231567\n",
      "Epoch: 1274 / dLoss: 1.0882155895233154 / gLoss: 1.2598754167556763\n",
      "Epoch: 1275 / dLoss: 0.9946557283401489 / gLoss: 1.150099754333496\n",
      "Epoch: 1276 / dLoss: 0.9511804580688477 / gLoss: 1.0527082681655884\n",
      "Epoch: 1277 / dLoss: 1.1087309122085571 / gLoss: 1.1428191661834717\n",
      "Epoch: 1278 / dLoss: 1.1796058416366577 / gLoss: 0.9365352392196655\n",
      "Epoch: 1279 / dLoss: 1.2293972969055176 / gLoss: 1.00460946559906\n",
      "Epoch: 1280 / dLoss: 1.166987419128418 / gLoss: 1.0014415979385376\n",
      "Epoch: 1281 / dLoss: 1.1003899574279785 / gLoss: 1.1091550588607788\n",
      "Epoch: 1282 / dLoss: 1.0648117065429688 / gLoss: 1.2612553834915161\n",
      "Epoch: 1283 / dLoss: 1.0026382207870483 / gLoss: 1.2708189487457275\n",
      "Epoch: 1284 / dLoss: 1.1108365058898926 / gLoss: 1.2815757989883423\n",
      "Epoch: 1285 / dLoss: 1.096962332725525 / gLoss: 1.215326189994812\n",
      "Epoch: 1286 / dLoss: 1.1064527034759521 / gLoss: 1.2155548334121704\n",
      "Epoch: 1287 / dLoss: 1.001847267150879 / gLoss: 1.0892069339752197\n",
      "Epoch: 1288 / dLoss: 1.187599539756775 / gLoss: 1.1560332775115967\n",
      "Epoch: 1289 / dLoss: 1.1925783157348633 / gLoss: 1.1936001777648926\n",
      "Epoch: 1290 / dLoss: 1.0614782571792603 / gLoss: 1.316860556602478\n",
      "Epoch: 1291 / dLoss: 1.0417051315307617 / gLoss: 1.4223227500915527\n",
      "Epoch: 1292 / dLoss: 1.0400961637496948 / gLoss: 1.16660737991333\n",
      "Epoch: 1293 / dLoss: 1.0378901958465576 / gLoss: 1.1609820127487183\n",
      "Epoch: 1294 / dLoss: 0.9692237973213196 / gLoss: 1.2095515727996826\n",
      "Epoch: 1295 / dLoss: 1.0748953819274902 / gLoss: 1.0687248706817627\n",
      "Epoch: 1296 / dLoss: 1.1041467189788818 / gLoss: 1.3438811302185059\n",
      "Epoch: 1297 / dLoss: 1.0006881952285767 / gLoss: 1.084476351737976\n",
      "Epoch: 1298 / dLoss: 0.9738118648529053 / gLoss: 1.1960829496383667\n",
      "Epoch: 1299 / dLoss: 1.1366040706634521 / gLoss: 0.9688200950622559\n",
      "Epoch: 1300 / dLoss: 1.0910812616348267 / gLoss: 1.1257132291793823\n",
      "Epoch: 1301 / dLoss: 1.167250394821167 / gLoss: 1.0898089408874512\n",
      "Epoch: 1302 / dLoss: 1.110363245010376 / gLoss: 1.0654560327529907\n",
      "Epoch: 1303 / dLoss: 1.0221956968307495 / gLoss: 1.0165491104125977\n",
      "Epoch: 1304 / dLoss: 1.0302939414978027 / gLoss: 1.5298312902450562\n",
      "Epoch: 1305 / dLoss: 1.0390297174453735 / gLoss: 1.184058666229248\n",
      "Epoch: 1306 / dLoss: 1.0381214618682861 / gLoss: 1.3123955726623535\n",
      "Epoch: 1307 / dLoss: 1.183403491973877 / gLoss: 1.274335265159607\n",
      "Epoch: 1308 / dLoss: 1.0099337100982666 / gLoss: 1.2569739818572998\n",
      "Epoch: 1309 / dLoss: 1.0944381952285767 / gLoss: 1.1703354120254517\n",
      "Epoch: 1310 / dLoss: 1.09942626953125 / gLoss: 1.2400214672088623\n",
      "Epoch: 1311 / dLoss: 1.0988693237304688 / gLoss: 1.1470414400100708\n",
      "Epoch: 1312 / dLoss: 1.143503189086914 / gLoss: 1.293954610824585\n",
      "Epoch: 1313 / dLoss: 1.0612859725952148 / gLoss: 1.26543390750885\n",
      "Epoch: 1314 / dLoss: 0.9643456935882568 / gLoss: 1.4785200357437134\n",
      "Epoch: 1315 / dLoss: 1.1111003160476685 / gLoss: 1.2397500276565552\n",
      "Epoch: 1316 / dLoss: 1.0531914234161377 / gLoss: 1.2193753719329834\n",
      "Epoch: 1317 / dLoss: 1.0234525203704834 / gLoss: 1.1750909090042114\n",
      "Epoch: 1318 / dLoss: 1.0584205389022827 / gLoss: 1.1952821016311646\n",
      "Epoch: 1319 / dLoss: 1.1449168920516968 / gLoss: 1.0699093341827393\n",
      "Epoch: 1320 / dLoss: 1.0768396854400635 / gLoss: 1.0551503896713257\n",
      "Epoch: 1321 / dLoss: 1.1678779125213623 / gLoss: 1.097955584526062\n",
      "Epoch: 1322 / dLoss: 1.056233525276184 / gLoss: 1.1750342845916748\n",
      "Epoch: 1323 / dLoss: 1.0888288021087646 / gLoss: 1.181226372718811\n",
      "Epoch: 1324 / dLoss: 1.0165643692016602 / gLoss: 1.2622933387756348\n",
      "Epoch: 1325 / dLoss: 1.0468525886535645 / gLoss: 1.428756833076477\n",
      "Epoch: 1326 / dLoss: 1.040037989616394 / gLoss: 1.1699517965316772\n",
      "Epoch: 1327 / dLoss: 1.0441935062408447 / gLoss: 1.371253252029419\n",
      "Epoch: 1328 / dLoss: 1.0276851654052734 / gLoss: 1.3645609617233276\n",
      "Epoch: 1329 / dLoss: 1.1085197925567627 / gLoss: 1.2665321826934814\n",
      "Epoch: 1330 / dLoss: 1.0293726921081543 / gLoss: 1.1961462497711182\n",
      "Epoch: 1331 / dLoss: 1.1490283012390137 / gLoss: 1.2270283699035645\n",
      "Epoch: 1332 / dLoss: 1.1535115242004395 / gLoss: 1.239058494567871\n",
      "Epoch: 1333 / dLoss: 0.9741417169570923 / gLoss: 1.1950881481170654\n",
      "Epoch: 1334 / dLoss: 1.0203145742416382 / gLoss: 1.2477622032165527\n",
      "Epoch: 1335 / dLoss: 1.0070043802261353 / gLoss: 1.2532179355621338\n",
      "Epoch: 1336 / dLoss: 1.069519281387329 / gLoss: 1.2591201066970825\n",
      "Epoch: 1337 / dLoss: 1.0218993425369263 / gLoss: 1.2900478839874268\n",
      "Epoch: 1338 / dLoss: 1.035957932472229 / gLoss: 1.452002763748169\n",
      "Epoch: 1339 / dLoss: 1.032146692276001 / gLoss: 1.3213063478469849\n",
      "Epoch: 1340 / dLoss: 0.9773815870285034 / gLoss: 1.423998236656189\n",
      "Epoch: 1341 / dLoss: 1.0768240690231323 / gLoss: 0.9848414063453674\n",
      "Epoch: 1342 / dLoss: 1.0436689853668213 / gLoss: 1.0849295854568481\n",
      "Epoch: 1343 / dLoss: 1.0100938081741333 / gLoss: 1.2065362930297852\n",
      "Epoch: 1344 / dLoss: 1.0447089672088623 / gLoss: 1.1203086376190186\n",
      "Epoch: 1345 / dLoss: 1.0799760818481445 / gLoss: 1.247216820716858\n",
      "Epoch: 1346 / dLoss: 1.0438355207443237 / gLoss: 1.0386501550674438\n",
      "Epoch: 1347 / dLoss: 0.9551000595092773 / gLoss: 1.2336735725402832\n",
      "Epoch: 1348 / dLoss: 1.0062155723571777 / gLoss: 1.2533812522888184\n",
      "Epoch: 1349 / dLoss: 0.9946063756942749 / gLoss: 1.295562744140625\n",
      "Epoch: 1350 / dLoss: 1.0730841159820557 / gLoss: 1.2203081846237183\n",
      "Epoch: 1351 / dLoss: 1.1056395769119263 / gLoss: 1.1683677434921265\n",
      "Epoch: 1352 / dLoss: 1.006894588470459 / gLoss: 1.0980556011199951\n",
      "Epoch: 1353 / dLoss: 1.119675874710083 / gLoss: 1.2099635601043701\n",
      "Epoch: 1354 / dLoss: 1.0614941120147705 / gLoss: 1.1558912992477417\n",
      "Epoch: 1355 / dLoss: 1.0394561290740967 / gLoss: 1.3352793455123901\n",
      "Epoch: 1356 / dLoss: 0.9736422300338745 / gLoss: 1.4410622119903564\n",
      "Epoch: 1357 / dLoss: 0.9510067105293274 / gLoss: 1.2725228071212769\n",
      "Epoch: 1358 / dLoss: 1.0011062622070312 / gLoss: 1.22536039352417\n",
      "Epoch: 1359 / dLoss: 1.099109411239624 / gLoss: 1.3081507682800293\n",
      "Epoch: 1360 / dLoss: 1.0209394693374634 / gLoss: 1.3736028671264648\n",
      "Epoch: 1361 / dLoss: 1.0748004913330078 / gLoss: 1.2101317644119263\n",
      "Epoch: 1362 / dLoss: 1.0037814378738403 / gLoss: 1.1950734853744507\n",
      "Epoch: 1363 / dLoss: 1.0829181671142578 / gLoss: 1.1765429973602295\n",
      "Epoch: 1364 / dLoss: 1.080374002456665 / gLoss: 1.2520259618759155\n",
      "Epoch: 1365 / dLoss: 1.1040747165679932 / gLoss: 1.23772132396698\n",
      "Epoch: 1366 / dLoss: 1.0303676128387451 / gLoss: 1.2627875804901123\n",
      "Epoch: 1367 / dLoss: 1.1999139785766602 / gLoss: 1.168958067893982\n",
      "Epoch: 1368 / dLoss: 1.0880913734436035 / gLoss: 1.4477039575576782\n",
      "Epoch: 1369 / dLoss: 1.08467435836792 / gLoss: 1.3681442737579346\n",
      "Epoch: 1370 / dLoss: 1.0688066482543945 / gLoss: 1.338884711265564\n",
      "Epoch: 1371 / dLoss: 1.1033298969268799 / gLoss: 1.2014474868774414\n",
      "Epoch: 1372 / dLoss: 1.0667526721954346 / gLoss: 1.2775682210922241\n",
      "Epoch: 1373 / dLoss: 1.0248230695724487 / gLoss: 1.3364888429641724\n",
      "Epoch: 1374 / dLoss: 1.0037810802459717 / gLoss: 1.2147842645645142\n",
      "Epoch: 1375 / dLoss: 1.0782746076583862 / gLoss: 1.144697904586792\n",
      "Epoch: 1376 / dLoss: 1.044693112373352 / gLoss: 1.1742501258850098\n",
      "Epoch: 1377 / dLoss: 1.1043837070465088 / gLoss: 1.281463623046875\n",
      "Epoch: 1378 / dLoss: 0.9064565896987915 / gLoss: 1.2937580347061157\n",
      "Epoch: 1379 / dLoss: 1.0946705341339111 / gLoss: 1.2203694581985474\n",
      "Epoch: 1380 / dLoss: 1.0095493793487549 / gLoss: 1.232799768447876\n",
      "Epoch: 1381 / dLoss: 1.0846569538116455 / gLoss: 1.2202510833740234\n",
      "Epoch: 1382 / dLoss: 1.0650238990783691 / gLoss: 1.1015255451202393\n",
      "Epoch: 1383 / dLoss: 1.0935945510864258 / gLoss: 1.113694190979004\n",
      "Epoch: 1384 / dLoss: 1.0652987957000732 / gLoss: 1.089257836341858\n",
      "Epoch: 1385 / dLoss: 1.022872805595398 / gLoss: 1.1831450462341309\n",
      "Epoch: 1386 / dLoss: 1.0786182880401611 / gLoss: 1.2419484853744507\n",
      "Epoch: 1387 / dLoss: 0.9977803230285645 / gLoss: 1.1013178825378418\n",
      "Epoch: 1388 / dLoss: 0.980316698551178 / gLoss: 1.150492787361145\n",
      "Epoch: 1389 / dLoss: 1.041398048400879 / gLoss: 1.1550028324127197\n",
      "Epoch: 1390 / dLoss: 1.0178945064544678 / gLoss: 1.1456232070922852\n",
      "Epoch: 1391 / dLoss: 1.0975054502487183 / gLoss: 1.1496731042861938\n",
      "Epoch: 1392 / dLoss: 1.0070693492889404 / gLoss: 1.1111469268798828\n",
      "Epoch: 1393 / dLoss: 1.067209243774414 / gLoss: 1.2611021995544434\n",
      "Epoch: 1394 / dLoss: 0.9950515031814575 / gLoss: 1.2988351583480835\n",
      "Epoch: 1395 / dLoss: 1.0772614479064941 / gLoss: 1.261812686920166\n",
      "Epoch: 1396 / dLoss: 0.9812827706336975 / gLoss: 1.2658947706222534\n",
      "Epoch: 1397 / dLoss: 0.9833966493606567 / gLoss: 1.1836220026016235\n",
      "Epoch: 1398 / dLoss: 1.0077764987945557 / gLoss: 1.2533173561096191\n",
      "Epoch: 1399 / dLoss: 0.9878033995628357 / gLoss: 1.347874402999878\n",
      "Epoch: 1400 / dLoss: 0.9855676889419556 / gLoss: 1.2839469909667969\n",
      "Epoch: 1401 / dLoss: 0.9680002927780151 / gLoss: 1.2574875354766846\n",
      "Epoch: 1402 / dLoss: 1.0613899230957031 / gLoss: 1.1485133171081543\n",
      "Epoch: 1403 / dLoss: 1.1140155792236328 / gLoss: 1.183663010597229\n",
      "Epoch: 1404 / dLoss: 0.9731951951980591 / gLoss: 1.1558852195739746\n",
      "Epoch: 1405 / dLoss: 0.9907951951026917 / gLoss: 1.1224141120910645\n",
      "Epoch: 1406 / dLoss: 1.0140345096588135 / gLoss: 1.1104423999786377\n",
      "Epoch: 1407 / dLoss: 1.0838027000427246 / gLoss: 1.3946844339370728\n",
      "Epoch: 1408 / dLoss: 0.9243106245994568 / gLoss: 1.3499841690063477\n",
      "Epoch: 1409 / dLoss: 0.9171745181083679 / gLoss: 1.1323262453079224\n",
      "Epoch: 1410 / dLoss: 1.0027339458465576 / gLoss: 1.2967997789382935\n",
      "Epoch: 1411 / dLoss: 0.9962981343269348 / gLoss: 1.3510774374008179\n",
      "Epoch: 1412 / dLoss: 1.0145442485809326 / gLoss: 1.2669320106506348\n",
      "Epoch: 1413 / dLoss: 1.0297348499298096 / gLoss: 1.3110487461090088\n",
      "Epoch: 1414 / dLoss: 1.0699269771575928 / gLoss: 1.1946196556091309\n",
      "Epoch: 1415 / dLoss: 0.9620596170425415 / gLoss: 1.2935895919799805\n",
      "Epoch: 1416 / dLoss: 1.095173954963684 / gLoss: 1.4329534769058228\n",
      "Epoch: 1417 / dLoss: 1.014763593673706 / gLoss: 1.3575477600097656\n",
      "Epoch: 1418 / dLoss: 1.0347256660461426 / gLoss: 1.4255836009979248\n",
      "Epoch: 1419 / dLoss: 1.152750015258789 / gLoss: 1.337447166442871\n",
      "Epoch: 1420 / dLoss: 0.951709508895874 / gLoss: 1.294663667678833\n",
      "Epoch: 1421 / dLoss: 0.993338406085968 / gLoss: 1.263279676437378\n",
      "Epoch: 1422 / dLoss: 0.9206709265708923 / gLoss: 1.4632668495178223\n",
      "Epoch: 1423 / dLoss: 1.006093978881836 / gLoss: 1.4340602159500122\n",
      "Epoch: 1424 / dLoss: 1.1523027420043945 / gLoss: 1.1166393756866455\n",
      "Epoch: 1425 / dLoss: 1.0897202491760254 / gLoss: 1.1619104146957397\n",
      "Epoch: 1426 / dLoss: 1.0241687297821045 / gLoss: 1.1286287307739258\n",
      "Epoch: 1427 / dLoss: 0.9294174313545227 / gLoss: 1.1567977666854858\n",
      "Epoch: 1428 / dLoss: 1.0537898540496826 / gLoss: 1.1376482248306274\n",
      "Epoch: 1429 / dLoss: 1.0024957656860352 / gLoss: 1.3791507482528687\n",
      "Epoch: 1430 / dLoss: 0.9748197793960571 / gLoss: 1.3241006135940552\n",
      "Epoch: 1431 / dLoss: 1.0536682605743408 / gLoss: 1.1976609230041504\n",
      "Epoch: 1432 / dLoss: 1.00680673122406 / gLoss: 1.3785709142684937\n",
      "Epoch: 1433 / dLoss: 1.0106501579284668 / gLoss: 1.3556562662124634\n",
      "Epoch: 1434 / dLoss: 0.9827048778533936 / gLoss: 1.4254133701324463\n",
      "Epoch: 1435 / dLoss: 0.9698398113250732 / gLoss: 1.296483039855957\n",
      "Epoch: 1436 / dLoss: 1.0101943016052246 / gLoss: 1.2242603302001953\n",
      "Epoch: 1437 / dLoss: 1.020039677619934 / gLoss: 1.2443217039108276\n",
      "Epoch: 1438 / dLoss: 1.0253281593322754 / gLoss: 1.257273554801941\n",
      "Epoch: 1439 / dLoss: 1.116091251373291 / gLoss: 1.2116291522979736\n",
      "Epoch: 1440 / dLoss: 0.918060302734375 / gLoss: 1.283988356590271\n",
      "Epoch: 1441 / dLoss: 1.0093376636505127 / gLoss: 1.4712802171707153\n",
      "Epoch: 1442 / dLoss: 1.0213183164596558 / gLoss: 1.1965339183807373\n",
      "Epoch: 1443 / dLoss: 1.0778236389160156 / gLoss: 1.3970156908035278\n",
      "Epoch: 1444 / dLoss: 1.058788776397705 / gLoss: 1.335821509361267\n",
      "Epoch: 1445 / dLoss: 0.9237667918205261 / gLoss: 1.2140188217163086\n",
      "Epoch: 1446 / dLoss: 1.0312223434448242 / gLoss: 1.2607630491256714\n",
      "Epoch: 1447 / dLoss: 0.976374626159668 / gLoss: 1.3774094581604004\n",
      "Epoch: 1448 / dLoss: 1.034269094467163 / gLoss: 1.1806516647338867\n",
      "Epoch: 1449 / dLoss: 0.9291235208511353 / gLoss: 1.2309901714324951\n",
      "Epoch: 1450 / dLoss: 1.0576860904693604 / gLoss: 1.1282508373260498\n",
      "Epoch: 1451 / dLoss: 0.9408052563667297 / gLoss: 1.2911229133605957\n",
      "Epoch: 1452 / dLoss: 0.9213497638702393 / gLoss: 1.304781198501587\n",
      "Epoch: 1453 / dLoss: 0.9855812788009644 / gLoss: 1.2882193326950073\n",
      "Epoch: 1454 / dLoss: 1.1026675701141357 / gLoss: 1.2770501375198364\n",
      "Epoch: 1455 / dLoss: 1.0295524597167969 / gLoss: 1.2956221103668213\n",
      "Epoch: 1456 / dLoss: 1.0233542919158936 / gLoss: 1.2635763883590698\n",
      "Epoch: 1457 / dLoss: 1.0319039821624756 / gLoss: 1.3228527307510376\n",
      "Epoch: 1458 / dLoss: 0.9741414189338684 / gLoss: 1.3477277755737305\n",
      "Epoch: 1459 / dLoss: 0.9336806535720825 / gLoss: 1.2871872186660767\n",
      "Epoch: 1460 / dLoss: 0.9540113210678101 / gLoss: 1.0889102220535278\n",
      "Epoch: 1461 / dLoss: 1.0540580749511719 / gLoss: 1.3028552532196045\n",
      "Epoch: 1462 / dLoss: 0.973720133304596 / gLoss: 1.0916482210159302\n",
      "Epoch: 1463 / dLoss: 0.9661197662353516 / gLoss: 1.303306221961975\n",
      "Epoch: 1464 / dLoss: 0.9516488313674927 / gLoss: 1.3157998323440552\n",
      "Epoch: 1465 / dLoss: 0.9991036057472229 / gLoss: 1.3906750679016113\n",
      "Epoch: 1466 / dLoss: 1.0767035484313965 / gLoss: 1.1516109704971313\n",
      "Epoch: 1467 / dLoss: 0.8751129508018494 / gLoss: 1.4016603231430054\n",
      "Epoch: 1468 / dLoss: 0.9518116116523743 / gLoss: 1.253611445426941\n",
      "Epoch: 1469 / dLoss: 1.0165989398956299 / gLoss: 1.2653019428253174\n",
      "Epoch: 1470 / dLoss: 0.9639115929603577 / gLoss: 1.4260271787643433\n",
      "Epoch: 1471 / dLoss: 0.9455946087837219 / gLoss: 1.31162428855896\n",
      "Epoch: 1472 / dLoss: 0.985613226890564 / gLoss: 1.2509740591049194\n",
      "Epoch: 1473 / dLoss: 0.9873650074005127 / gLoss: 1.2267506122589111\n",
      "Epoch: 1474 / dLoss: 1.0094666481018066 / gLoss: 1.430812120437622\n",
      "Epoch: 1475 / dLoss: 0.9885374307632446 / gLoss: 1.3461048603057861\n",
      "Epoch: 1476 / dLoss: 0.8755713701248169 / gLoss: 1.286146640777588\n",
      "Epoch: 1477 / dLoss: 1.0282424688339233 / gLoss: 1.3472520112991333\n",
      "Epoch: 1478 / dLoss: 0.8702170848846436 / gLoss: 1.3680938482284546\n",
      "Epoch: 1479 / dLoss: 1.0004335641860962 / gLoss: 1.3901991844177246\n",
      "Epoch: 1480 / dLoss: 0.9958292841911316 / gLoss: 1.27324378490448\n",
      "Epoch: 1481 / dLoss: 1.0401417016983032 / gLoss: 1.243199348449707\n",
      "Epoch: 1482 / dLoss: 0.9434648752212524 / gLoss: 1.238537311553955\n",
      "Epoch: 1483 / dLoss: 1.0608354806900024 / gLoss: 1.2354583740234375\n",
      "Epoch: 1484 / dLoss: 0.9032539129257202 / gLoss: 1.261825442314148\n",
      "Epoch: 1485 / dLoss: 1.0124014616012573 / gLoss: 1.3090013265609741\n",
      "Epoch: 1486 / dLoss: 0.9267349243164062 / gLoss: 1.141575574874878\n",
      "Epoch: 1487 / dLoss: 0.9959743022918701 / gLoss: 1.2002005577087402\n",
      "Epoch: 1488 / dLoss: 1.0706729888916016 / gLoss: 1.2826179265975952\n",
      "Epoch: 1489 / dLoss: 1.1307857036590576 / gLoss: 1.2263495922088623\n",
      "Epoch: 1490 / dLoss: 1.0531854629516602 / gLoss: 1.3189021348953247\n",
      "Epoch: 1491 / dLoss: 0.9831346869468689 / gLoss: 1.3916897773742676\n",
      "Epoch: 1492 / dLoss: 0.9599330425262451 / gLoss: 1.355086326599121\n",
      "Epoch: 1493 / dLoss: 1.1132519245147705 / gLoss: 1.256272554397583\n",
      "Epoch: 1494 / dLoss: 0.9631012082099915 / gLoss: 1.1906671524047852\n",
      "Epoch: 1495 / dLoss: 0.9660977125167847 / gLoss: 1.5498443841934204\n",
      "Epoch: 1496 / dLoss: 0.9192262887954712 / gLoss: 1.4221408367156982\n",
      "Epoch: 1497 / dLoss: 1.046220064163208 / gLoss: 1.4430688619613647\n",
      "Epoch: 1498 / dLoss: 0.979065477848053 / gLoss: 1.3083049058914185\n",
      "Epoch: 1499 / dLoss: 1.0526769161224365 / gLoss: 1.444534182548523\n",
      "Epoch: 1500 / dLoss: 1.0410258769989014 / gLoss: 1.3037127256393433\n",
      "Epoch: 1501 / dLoss: 1.0498336553573608 / gLoss: 1.4535142183303833\n",
      "Epoch: 1502 / dLoss: 1.0825915336608887 / gLoss: 1.5056750774383545\n",
      "Epoch: 1503 / dLoss: 0.9508626461029053 / gLoss: 1.537331223487854\n",
      "Epoch: 1504 / dLoss: 1.0992109775543213 / gLoss: 1.3744146823883057\n",
      "Epoch: 1505 / dLoss: 1.1184766292572021 / gLoss: 1.2473113536834717\n",
      "Epoch: 1506 / dLoss: 0.9528260231018066 / gLoss: 1.5165760517120361\n",
      "Epoch: 1507 / dLoss: 0.9893919825553894 / gLoss: 1.3305954933166504\n",
      "Epoch: 1508 / dLoss: 0.9778004884719849 / gLoss: 1.226214051246643\n",
      "Epoch: 1509 / dLoss: 0.9605070352554321 / gLoss: 1.389927864074707\n",
      "Epoch: 1510 / dLoss: 0.9634121060371399 / gLoss: 1.3448972702026367\n",
      "Epoch: 1511 / dLoss: 0.9546931982040405 / gLoss: 1.527623176574707\n",
      "Epoch: 1512 / dLoss: 0.9905903339385986 / gLoss: 1.5557688474655151\n",
      "Epoch: 1513 / dLoss: 0.9285973310470581 / gLoss: 1.4840600490570068\n",
      "Epoch: 1514 / dLoss: 0.9933217763900757 / gLoss: 1.2850139141082764\n",
      "Epoch: 1515 / dLoss: 0.8092679977416992 / gLoss: 1.3264471292495728\n",
      "Epoch: 1516 / dLoss: 0.9545646905899048 / gLoss: 1.428517460823059\n",
      "Epoch: 1517 / dLoss: 0.8586534261703491 / gLoss: 1.3745037317276\n",
      "Epoch: 1518 / dLoss: 0.9075659513473511 / gLoss: 1.309751033782959\n",
      "Epoch: 1519 / dLoss: 0.8030501008033752 / gLoss: 1.428867220878601\n",
      "Epoch: 1520 / dLoss: 0.9687725305557251 / gLoss: 1.4214186668395996\n",
      "Epoch: 1521 / dLoss: 0.9430280923843384 / gLoss: 1.3873777389526367\n",
      "Epoch: 1522 / dLoss: 0.9054006338119507 / gLoss: 1.4214993715286255\n",
      "Epoch: 1523 / dLoss: 0.9974881410598755 / gLoss: 1.4206868410110474\n",
      "Epoch: 1524 / dLoss: 0.998080849647522 / gLoss: 1.5114338397979736\n",
      "Epoch: 1525 / dLoss: 0.931076169013977 / gLoss: 1.3582425117492676\n",
      "Epoch: 1526 / dLoss: 0.970511794090271 / gLoss: 1.363694190979004\n",
      "Epoch: 1527 / dLoss: 1.0065090656280518 / gLoss: 1.3469394445419312\n",
      "Epoch: 1528 / dLoss: 0.9205480813980103 / gLoss: 1.352098822593689\n",
      "Epoch: 1529 / dLoss: 0.9221833944320679 / gLoss: 1.4101654291152954\n",
      "Epoch: 1530 / dLoss: 1.0245661735534668 / gLoss: 1.5916666984558105\n",
      "Epoch: 1531 / dLoss: 0.9648498296737671 / gLoss: 1.4771114587783813\n",
      "Epoch: 1532 / dLoss: 1.0196921825408936 / gLoss: 1.444960594177246\n",
      "Epoch: 1533 / dLoss: 0.9079931974411011 / gLoss: 1.1402959823608398\n",
      "Epoch: 1534 / dLoss: 0.9189749956130981 / gLoss: 1.6055721044540405\n",
      "Epoch: 1535 / dLoss: 0.9844374656677246 / gLoss: 1.1956285238265991\n",
      "Epoch: 1536 / dLoss: 1.1064971685409546 / gLoss: 1.3850611448287964\n",
      "Epoch: 1537 / dLoss: 0.9956942200660706 / gLoss: 1.505009412765503\n",
      "Epoch: 1538 / dLoss: 1.017900824546814 / gLoss: 1.2923930883407593\n",
      "Epoch: 1539 / dLoss: 1.0245883464813232 / gLoss: 1.5555020570755005\n",
      "Epoch: 1540 / dLoss: 0.9414587020874023 / gLoss: 1.3638129234313965\n",
      "Epoch: 1541 / dLoss: 1.0106844902038574 / gLoss: 1.3908745050430298\n",
      "Epoch: 1542 / dLoss: 0.8616974353790283 / gLoss: 1.2784591913223267\n",
      "Epoch: 1543 / dLoss: 0.9413751363754272 / gLoss: 1.5478577613830566\n",
      "Epoch: 1544 / dLoss: 0.9996585845947266 / gLoss: 1.3558975458145142\n",
      "Epoch: 1545 / dLoss: 0.929339587688446 / gLoss: 1.4710416793823242\n",
      "Epoch: 1546 / dLoss: 0.9960619211196899 / gLoss: 1.5228809118270874\n",
      "Epoch: 1547 / dLoss: 0.9213659763336182 / gLoss: 1.4318863153457642\n",
      "Epoch: 1548 / dLoss: 0.9716390371322632 / gLoss: 1.1759146451950073\n",
      "Epoch: 1549 / dLoss: 0.8655027747154236 / gLoss: 1.2899786233901978\n",
      "Epoch: 1550 / dLoss: 0.9302371740341187 / gLoss: 1.3500561714172363\n",
      "Epoch: 1551 / dLoss: 0.964789867401123 / gLoss: 1.2862483263015747\n",
      "Epoch: 1552 / dLoss: 0.8744456768035889 / gLoss: 1.394914984703064\n",
      "Epoch: 1553 / dLoss: 1.0997905731201172 / gLoss: 1.3976874351501465\n",
      "Epoch: 1554 / dLoss: 1.0089179277420044 / gLoss: 1.1483155488967896\n",
      "Epoch: 1555 / dLoss: 0.9957688450813293 / gLoss: 1.4467872381210327\n",
      "Epoch: 1556 / dLoss: 1.092417597770691 / gLoss: 1.4780371189117432\n",
      "Epoch: 1557 / dLoss: 0.9384926557540894 / gLoss: 1.458114504814148\n",
      "Epoch: 1558 / dLoss: 0.898215651512146 / gLoss: 1.4613934755325317\n",
      "Epoch: 1559 / dLoss: 0.9535813331604004 / gLoss: 1.1946638822555542\n",
      "Epoch: 1560 / dLoss: 1.0071804523468018 / gLoss: 1.5441359281539917\n",
      "Epoch: 1561 / dLoss: 0.9227074384689331 / gLoss: 1.378434181213379\n",
      "Epoch: 1562 / dLoss: 0.9807302355766296 / gLoss: 1.3842267990112305\n",
      "Epoch: 1563 / dLoss: 0.9453935027122498 / gLoss: 1.419304370880127\n",
      "Epoch: 1564 / dLoss: 0.9643702507019043 / gLoss: 1.3386213779449463\n",
      "Epoch: 1565 / dLoss: 1.0094892978668213 / gLoss: 1.3891243934631348\n",
      "Epoch: 1566 / dLoss: 1.0027778148651123 / gLoss: 1.3333709239959717\n",
      "Epoch: 1567 / dLoss: 0.9711776971817017 / gLoss: 1.3870985507965088\n",
      "Epoch: 1568 / dLoss: 1.0170774459838867 / gLoss: 1.5318087339401245\n",
      "Epoch: 1569 / dLoss: 0.9567773342132568 / gLoss: 1.4167519807815552\n",
      "Epoch: 1570 / dLoss: 0.9605875015258789 / gLoss: 1.4601430892944336\n",
      "Epoch: 1571 / dLoss: 1.0238769054412842 / gLoss: 1.5082043409347534\n",
      "Epoch: 1572 / dLoss: 0.920134425163269 / gLoss: 1.3024085760116577\n",
      "Epoch: 1573 / dLoss: 1.0085371732711792 / gLoss: 1.3074729442596436\n",
      "Epoch: 1574 / dLoss: 0.9303021430969238 / gLoss: 1.48491632938385\n",
      "Epoch: 1575 / dLoss: 0.9994823932647705 / gLoss: 1.3706138134002686\n",
      "Epoch: 1576 / dLoss: 1.0057681798934937 / gLoss: 1.305052638053894\n",
      "Epoch: 1577 / dLoss: 0.9627471566200256 / gLoss: 1.2535032033920288\n",
      "Epoch: 1578 / dLoss: 1.0026627779006958 / gLoss: 1.529348611831665\n",
      "Epoch: 1579 / dLoss: 0.9080215692520142 / gLoss: 1.4748387336730957\n",
      "Epoch: 1580 / dLoss: 0.9013289213180542 / gLoss: 1.5823044776916504\n",
      "Epoch: 1581 / dLoss: 1.0540252923965454 / gLoss: 1.4785304069519043\n",
      "Epoch: 1582 / dLoss: 1.0432493686676025 / gLoss: 1.4205632209777832\n",
      "Epoch: 1583 / dLoss: 1.0093004703521729 / gLoss: 1.4883642196655273\n",
      "Epoch: 1584 / dLoss: 1.1015362739562988 / gLoss: 1.518627643585205\n",
      "Epoch: 1585 / dLoss: 0.8485236763954163 / gLoss: 1.3264237642288208\n",
      "Epoch: 1586 / dLoss: 1.053131103515625 / gLoss: 1.4437278509140015\n",
      "Epoch: 1587 / dLoss: 0.9353654384613037 / gLoss: 1.4335533380508423\n",
      "Epoch: 1588 / dLoss: 0.9377974271774292 / gLoss: 1.420163631439209\n",
      "Epoch: 1589 / dLoss: 0.9210206270217896 / gLoss: 1.4006167650222778\n",
      "Epoch: 1590 / dLoss: 0.9389036893844604 / gLoss: 1.3703876733779907\n",
      "Epoch: 1591 / dLoss: 0.9781728982925415 / gLoss: 1.2839598655700684\n",
      "Epoch: 1592 / dLoss: 0.8386275768280029 / gLoss: 1.2771046161651611\n",
      "Epoch: 1593 / dLoss: 0.9693073630332947 / gLoss: 1.5601191520690918\n",
      "Epoch: 1594 / dLoss: 0.9484826326370239 / gLoss: 1.373476505279541\n",
      "Epoch: 1595 / dLoss: 0.9567147493362427 / gLoss: 1.3796439170837402\n",
      "Epoch: 1596 / dLoss: 0.9774595499038696 / gLoss: 1.5164690017700195\n",
      "Epoch: 1597 / dLoss: 0.8949328064918518 / gLoss: 1.3777375221252441\n",
      "Epoch: 1598 / dLoss: 0.9404173493385315 / gLoss: 1.6389001607894897\n",
      "Epoch: 1599 / dLoss: 0.9631061553955078 / gLoss: 1.3276458978652954\n",
      "Epoch: 1600 / dLoss: 0.9287079572677612 / gLoss: 1.4497302770614624\n",
      "Epoch: 1601 / dLoss: 0.9741382598876953 / gLoss: 1.4727051258087158\n",
      "Epoch: 1602 / dLoss: 0.9628955125808716 / gLoss: 1.6723941564559937\n",
      "Epoch: 1603 / dLoss: 0.9293102025985718 / gLoss: 1.6101150512695312\n",
      "Epoch: 1604 / dLoss: 0.9820321798324585 / gLoss: 1.4202128648757935\n",
      "Epoch: 1605 / dLoss: 0.9044550657272339 / gLoss: 1.3753352165222168\n",
      "Epoch: 1606 / dLoss: 0.9553109407424927 / gLoss: 1.6102142333984375\n",
      "Epoch: 1607 / dLoss: 0.8406264185905457 / gLoss: 1.691516399383545\n",
      "Epoch: 1608 / dLoss: 0.9374439716339111 / gLoss: 1.4562430381774902\n",
      "Epoch: 1609 / dLoss: 0.9918324947357178 / gLoss: 1.470617651939392\n",
      "Epoch: 1610 / dLoss: 0.9923224449157715 / gLoss: 1.4602410793304443\n",
      "Epoch: 1611 / dLoss: 0.9521105289459229 / gLoss: 1.4311610460281372\n",
      "Epoch: 1612 / dLoss: 0.8888375759124756 / gLoss: 1.4980621337890625\n",
      "Epoch: 1613 / dLoss: 0.9204984903335571 / gLoss: 1.4214463233947754\n",
      "Epoch: 1614 / dLoss: 0.9718005657196045 / gLoss: 1.4194680452346802\n",
      "Epoch: 1615 / dLoss: 1.014554500579834 / gLoss: 1.3579710721969604\n",
      "Epoch: 1616 / dLoss: 1.0289393663406372 / gLoss: 1.641111135482788\n",
      "Epoch: 1617 / dLoss: 1.0141795873641968 / gLoss: 1.5820120573043823\n",
      "Epoch: 1618 / dLoss: 0.8499476909637451 / gLoss: 1.6727185249328613\n",
      "Epoch: 1619 / dLoss: 1.0210522413253784 / gLoss: 1.5157374143600464\n",
      "Epoch: 1620 / dLoss: 0.992154598236084 / gLoss: 1.4256706237792969\n",
      "Epoch: 1621 / dLoss: 0.9486502408981323 / gLoss: 1.5112004280090332\n",
      "Epoch: 1622 / dLoss: 0.8229045271873474 / gLoss: 1.3734450340270996\n",
      "Epoch: 1623 / dLoss: 1.0106009244918823 / gLoss: 1.7369143962860107\n",
      "Epoch: 1624 / dLoss: 1.0121277570724487 / gLoss: 1.5216976404190063\n",
      "Epoch: 1625 / dLoss: 1.0186388492584229 / gLoss: 1.5240685939788818\n",
      "Epoch: 1626 / dLoss: 0.991520345211029 / gLoss: 1.7229048013687134\n",
      "Epoch: 1627 / dLoss: 0.9097908139228821 / gLoss: 1.535531759262085\n",
      "Epoch: 1628 / dLoss: 0.8874004483222961 / gLoss: 1.3548997640609741\n",
      "Epoch: 1629 / dLoss: 0.8598695993423462 / gLoss: 1.350193977355957\n",
      "Epoch: 1630 / dLoss: 0.8802613019943237 / gLoss: 1.443395733833313\n",
      "Epoch: 1631 / dLoss: 1.0431263446807861 / gLoss: 1.2051231861114502\n",
      "Epoch: 1632 / dLoss: 0.8491400480270386 / gLoss: 1.6105952262878418\n",
      "Epoch: 1633 / dLoss: 0.9725451469421387 / gLoss: 1.6250807046890259\n",
      "Epoch: 1634 / dLoss: 0.9607117176055908 / gLoss: 1.382675290107727\n",
      "Epoch: 1635 / dLoss: 0.9754728078842163 / gLoss: 1.3428198099136353\n",
      "Epoch: 1636 / dLoss: 1.1176880598068237 / gLoss: 1.367030143737793\n",
      "Epoch: 1637 / dLoss: 1.1611255407333374 / gLoss: 1.5136768817901611\n",
      "Epoch: 1638 / dLoss: 0.9732812643051147 / gLoss: 1.6047706604003906\n",
      "Epoch: 1639 / dLoss: 0.8958158493041992 / gLoss: 1.5708985328674316\n",
      "Epoch: 1640 / dLoss: 0.9128894209861755 / gLoss: 1.4581272602081299\n",
      "Epoch: 1641 / dLoss: 0.9807332754135132 / gLoss: 1.5823804140090942\n",
      "Epoch: 1642 / dLoss: 0.8579229712486267 / gLoss: 1.5797441005706787\n",
      "Epoch: 1643 / dLoss: 0.8633365631103516 / gLoss: 1.6498427391052246\n",
      "Epoch: 1644 / dLoss: 0.8654516935348511 / gLoss: 1.4805288314819336\n",
      "Epoch: 1645 / dLoss: 0.7892606854438782 / gLoss: 1.492956519126892\n",
      "Epoch: 1646 / dLoss: 0.9379502534866333 / gLoss: 1.583195686340332\n",
      "Epoch: 1647 / dLoss: 1.0038560628890991 / gLoss: 1.6108609437942505\n",
      "Epoch: 1648 / dLoss: 0.9343905448913574 / gLoss: 1.3990962505340576\n",
      "Epoch: 1649 / dLoss: 0.953192949295044 / gLoss: 1.6034035682678223\n",
      "Epoch: 1650 / dLoss: 0.9326634407043457 / gLoss: 1.652347445487976\n",
      "Epoch: 1651 / dLoss: 0.9589927196502686 / gLoss: 1.4943263530731201\n",
      "Epoch: 1652 / dLoss: 0.9602122902870178 / gLoss: 1.3990890979766846\n",
      "Epoch: 1653 / dLoss: 0.9620511531829834 / gLoss: 1.4366874694824219\n",
      "Epoch: 1654 / dLoss: 0.9219022393226624 / gLoss: 1.5501681566238403\n",
      "Epoch: 1655 / dLoss: 0.9259464740753174 / gLoss: 1.5862212181091309\n",
      "Epoch: 1656 / dLoss: 0.9847095012664795 / gLoss: 1.717984676361084\n",
      "Epoch: 1657 / dLoss: 0.9281021356582642 / gLoss: 1.4439115524291992\n",
      "Epoch: 1658 / dLoss: 0.8597891330718994 / gLoss: 1.6472936868667603\n",
      "Epoch: 1659 / dLoss: 1.0349960327148438 / gLoss: 1.5076478719711304\n",
      "Epoch: 1660 / dLoss: 0.9834260940551758 / gLoss: 1.5479495525360107\n",
      "Epoch: 1661 / dLoss: 0.9619976282119751 / gLoss: 1.6347088813781738\n",
      "Epoch: 1662 / dLoss: 1.040418028831482 / gLoss: 1.7484228610992432\n",
      "Epoch: 1663 / dLoss: 0.980819582939148 / gLoss: 1.4855473041534424\n",
      "Epoch: 1664 / dLoss: 0.9319270849227905 / gLoss: 1.570193886756897\n",
      "Epoch: 1665 / dLoss: 0.9954408407211304 / gLoss: 1.6230624914169312\n",
      "Epoch: 1666 / dLoss: 0.9624247550964355 / gLoss: 1.5696908235549927\n",
      "Epoch: 1667 / dLoss: 0.8692669868469238 / gLoss: 1.7084968090057373\n",
      "Epoch: 1668 / dLoss: 0.8912699818611145 / gLoss: 1.6177233457565308\n",
      "Epoch: 1669 / dLoss: 0.9717370271682739 / gLoss: 1.3252910375595093\n",
      "Epoch: 1670 / dLoss: 0.8337141871452332 / gLoss: 1.4027605056762695\n",
      "Epoch: 1671 / dLoss: 0.8721710443496704 / gLoss: 1.3371953964233398\n",
      "Epoch: 1672 / dLoss: 0.804935872554779 / gLoss: 1.3877607583999634\n",
      "Epoch: 1673 / dLoss: 0.901496171951294 / gLoss: 1.5450067520141602\n",
      "Epoch: 1674 / dLoss: 1.0151389837265015 / gLoss: 1.8662375211715698\n",
      "Epoch: 1675 / dLoss: 0.8173679709434509 / gLoss: 1.718010663986206\n",
      "Epoch: 1676 / dLoss: 1.038439393043518 / gLoss: 1.8167393207550049\n",
      "Epoch: 1677 / dLoss: 0.9423857927322388 / gLoss: 1.4219939708709717\n",
      "Epoch: 1678 / dLoss: 0.9883760213851929 / gLoss: 1.4934979677200317\n",
      "Epoch: 1679 / dLoss: 0.9978166818618774 / gLoss: 1.4119833707809448\n",
      "Epoch: 1680 / dLoss: 0.903343915939331 / gLoss: 1.658774971961975\n",
      "Epoch: 1681 / dLoss: 1.0545629262924194 / gLoss: 1.94769287109375\n",
      "Epoch: 1682 / dLoss: 0.9687476754188538 / gLoss: 1.521417260169983\n",
      "Epoch: 1683 / dLoss: 0.8212651014328003 / gLoss: 1.7029856443405151\n",
      "Epoch: 1684 / dLoss: 0.9921025633811951 / gLoss: 1.3864374160766602\n",
      "Epoch: 1685 / dLoss: 0.8643616437911987 / gLoss: 1.8186695575714111\n",
      "Epoch: 1686 / dLoss: 0.9453096389770508 / gLoss: 1.5989238023757935\n",
      "Epoch: 1687 / dLoss: 0.9795542359352112 / gLoss: 1.2565444707870483\n",
      "Epoch: 1688 / dLoss: 1.01151704788208 / gLoss: 1.423072099685669\n",
      "Epoch: 1689 / dLoss: 1.0000652074813843 / gLoss: 1.5005488395690918\n",
      "Epoch: 1690 / dLoss: 1.044022560119629 / gLoss: 1.5942978858947754\n",
      "Epoch: 1691 / dLoss: 0.7948639392852783 / gLoss: 1.6938254833221436\n",
      "Epoch: 1692 / dLoss: 0.8849365711212158 / gLoss: 1.699483036994934\n",
      "Epoch: 1693 / dLoss: 0.8599792718887329 / gLoss: 1.5349371433258057\n",
      "Epoch: 1694 / dLoss: 0.794917106628418 / gLoss: 1.6370254755020142\n",
      "Epoch: 1695 / dLoss: 1.0623722076416016 / gLoss: 1.6264492273330688\n",
      "Epoch: 1696 / dLoss: 1.016831636428833 / gLoss: 1.3880865573883057\n",
      "Epoch: 1697 / dLoss: 1.1278595924377441 / gLoss: 1.4391201734542847\n",
      "Epoch: 1698 / dLoss: 0.9357261657714844 / gLoss: 1.6863807439804077\n",
      "Epoch: 1699 / dLoss: 0.9378440976142883 / gLoss: 1.7365310192108154\n",
      "Epoch: 1700 / dLoss: 0.9655323028564453 / gLoss: 1.7731651067733765\n",
      "Epoch: 1701 / dLoss: 0.9516628384590149 / gLoss: 1.6729627847671509\n",
      "Epoch: 1702 / dLoss: 1.060784101486206 / gLoss: 1.5450491905212402\n",
      "Epoch: 1703 / dLoss: 0.9033340215682983 / gLoss: 1.5450478792190552\n",
      "Epoch: 1704 / dLoss: 0.9081764221191406 / gLoss: 1.4784839153289795\n",
      "Epoch: 1705 / dLoss: 0.942947268486023 / gLoss: 1.6099086999893188\n",
      "Epoch: 1706 / dLoss: 1.0116567611694336 / gLoss: 1.533937931060791\n",
      "Epoch: 1707 / dLoss: 1.0173739194869995 / gLoss: 1.511532187461853\n",
      "Epoch: 1708 / dLoss: 0.9466390609741211 / gLoss: 1.516790509223938\n",
      "Epoch: 1709 / dLoss: 0.8537838459014893 / gLoss: 1.6666131019592285\n",
      "Epoch: 1710 / dLoss: 0.8759191036224365 / gLoss: 1.4925788640975952\n",
      "Epoch: 1711 / dLoss: 1.0359710454940796 / gLoss: 1.6351896524429321\n",
      "Epoch: 1712 / dLoss: 0.881657600402832 / gLoss: 1.6535496711730957\n",
      "Epoch: 1713 / dLoss: 0.9041416049003601 / gLoss: 1.643824815750122\n",
      "Epoch: 1714 / dLoss: 0.9513266682624817 / gLoss: 1.3355069160461426\n",
      "Epoch: 1715 / dLoss: 0.8938430547714233 / gLoss: 1.5161645412445068\n",
      "Epoch: 1716 / dLoss: 0.8063044548034668 / gLoss: 1.689603328704834\n",
      "Epoch: 1717 / dLoss: 0.9547093510627747 / gLoss: 1.6502981185913086\n",
      "Epoch: 1718 / dLoss: 0.9144473671913147 / gLoss: 1.8092042207717896\n",
      "Epoch: 1719 / dLoss: 0.8872250318527222 / gLoss: 1.8640121221542358\n",
      "Epoch: 1720 / dLoss: 0.892460823059082 / gLoss: 1.6334753036499023\n",
      "Epoch: 1721 / dLoss: 0.8844813108444214 / gLoss: 1.4693961143493652\n",
      "Epoch: 1722 / dLoss: 0.9559255838394165 / gLoss: 1.5601632595062256\n",
      "Epoch: 1723 / dLoss: 0.883568525314331 / gLoss: 1.4833790063858032\n",
      "Epoch: 1724 / dLoss: 0.9568657279014587 / gLoss: 1.5077698230743408\n",
      "Epoch: 1725 / dLoss: 0.910923957824707 / gLoss: 1.4431358575820923\n",
      "Epoch: 1726 / dLoss: 0.9343034029006958 / gLoss: 1.7196110486984253\n",
      "Epoch: 1727 / dLoss: 0.9501827359199524 / gLoss: 1.6279412508010864\n",
      "Epoch: 1728 / dLoss: 1.0133957862854004 / gLoss: 1.6034313440322876\n",
      "Epoch: 1729 / dLoss: 0.9449634552001953 / gLoss: 1.5312228202819824\n",
      "Epoch: 1730 / dLoss: 0.7982896566390991 / gLoss: 1.649454951286316\n",
      "Epoch: 1731 / dLoss: 0.8615447282791138 / gLoss: 1.6213089227676392\n",
      "Epoch: 1732 / dLoss: 0.9551506042480469 / gLoss: 1.5071351528167725\n",
      "Epoch: 1733 / dLoss: 0.8305109143257141 / gLoss: 1.5905663967132568\n",
      "Epoch: 1734 / dLoss: 0.9524102210998535 / gLoss: 1.5874007940292358\n",
      "Epoch: 1735 / dLoss: 0.9742408990859985 / gLoss: 1.6008758544921875\n",
      "Epoch: 1736 / dLoss: 0.958154559135437 / gLoss: 1.5587939023971558\n",
      "Epoch: 1737 / dLoss: 0.8908855319023132 / gLoss: 1.635825514793396\n",
      "Epoch: 1738 / dLoss: 0.9382525682449341 / gLoss: 1.6177520751953125\n",
      "Epoch: 1739 / dLoss: 0.9714717864990234 / gLoss: 1.7005715370178223\n",
      "Epoch: 1740 / dLoss: 0.806827187538147 / gLoss: 1.5513097047805786\n",
      "Epoch: 1741 / dLoss: 0.977104902267456 / gLoss: 1.6532936096191406\n",
      "Epoch: 1742 / dLoss: 0.907707929611206 / gLoss: 1.4623717069625854\n",
      "Epoch: 1743 / dLoss: 0.8901375532150269 / gLoss: 1.4541146755218506\n",
      "Epoch: 1744 / dLoss: 0.9368876814842224 / gLoss: 1.4699294567108154\n",
      "Epoch: 1745 / dLoss: 1.0006053447723389 / gLoss: 1.5458734035491943\n",
      "Epoch: 1746 / dLoss: 0.8801866769790649 / gLoss: 1.333328127861023\n",
      "Epoch: 1747 / dLoss: 0.9996271133422852 / gLoss: 1.5875169038772583\n",
      "Epoch: 1748 / dLoss: 0.8801591396331787 / gLoss: 1.697774052619934\n",
      "Epoch: 1749 / dLoss: 0.82792729139328 / gLoss: 1.8040918111801147\n",
      "Epoch: 1750 / dLoss: 0.8331500887870789 / gLoss: 1.5811810493469238\n",
      "Epoch: 1751 / dLoss: 0.9900573492050171 / gLoss: 1.822161078453064\n",
      "Epoch: 1752 / dLoss: 0.891736626625061 / gLoss: 1.8190022706985474\n",
      "Epoch: 1753 / dLoss: 0.8734334707260132 / gLoss: 1.779531717300415\n",
      "Epoch: 1754 / dLoss: 0.955007791519165 / gLoss: 1.6920280456542969\n",
      "Epoch: 1755 / dLoss: 0.935456395149231 / gLoss: 1.7099316120147705\n",
      "Epoch: 1756 / dLoss: 1.0580730438232422 / gLoss: 1.7475271224975586\n",
      "Epoch: 1757 / dLoss: 0.9546927213668823 / gLoss: 2.0205583572387695\n",
      "Epoch: 1758 / dLoss: 0.9475318193435669 / gLoss: 1.982712984085083\n",
      "Epoch: 1759 / dLoss: 0.9415963888168335 / gLoss: 1.695239543914795\n",
      "Epoch: 1760 / dLoss: 0.8630317449569702 / gLoss: 1.7658079862594604\n",
      "Epoch: 1761 / dLoss: 1.0226601362228394 / gLoss: 1.6462469100952148\n",
      "Epoch: 1762 / dLoss: 0.9802646040916443 / gLoss: 1.608178734779358\n",
      "Epoch: 1763 / dLoss: 0.9668123126029968 / gLoss: 1.4335434436798096\n",
      "Epoch: 1764 / dLoss: 1.0235838890075684 / gLoss: 1.4300363063812256\n",
      "Epoch: 1765 / dLoss: 1.0064146518707275 / gLoss: 1.4438575506210327\n",
      "Epoch: 1766 / dLoss: 0.9803668856620789 / gLoss: 1.426626443862915\n",
      "Epoch: 1767 / dLoss: 0.9623157978057861 / gLoss: 1.5822006464004517\n",
      "Epoch: 1768 / dLoss: 0.8976628184318542 / gLoss: 1.7969890832901\n",
      "Epoch: 1769 / dLoss: 0.9875158667564392 / gLoss: 1.5026217699050903\n",
      "Epoch: 1770 / dLoss: 0.8641541004180908 / gLoss: 1.5071674585342407\n",
      "Epoch: 1771 / dLoss: 0.9471243619918823 / gLoss: 1.5626835823059082\n",
      "Epoch: 1772 / dLoss: 0.8961924314498901 / gLoss: 1.495711326599121\n",
      "Epoch: 1773 / dLoss: 0.9663335680961609 / gLoss: 1.7697831392288208\n",
      "Epoch: 1774 / dLoss: 0.9515448808670044 / gLoss: 1.647093653678894\n",
      "Epoch: 1775 / dLoss: 0.9844184517860413 / gLoss: 1.6601454019546509\n",
      "Epoch: 1776 / dLoss: 0.9531581401824951 / gLoss: 1.9527851343154907\n",
      "Epoch: 1777 / dLoss: 0.9006258845329285 / gLoss: 1.7834670543670654\n",
      "Epoch: 1778 / dLoss: 0.9452309608459473 / gLoss: 1.8048020601272583\n",
      "Epoch: 1779 / dLoss: 0.8022035360336304 / gLoss: 1.8359777927398682\n",
      "Epoch: 1780 / dLoss: 0.9304078817367554 / gLoss: 1.4682639837265015\n",
      "Epoch: 1781 / dLoss: 0.9670171141624451 / gLoss: 1.5013258457183838\n",
      "Epoch: 1782 / dLoss: 0.8433166146278381 / gLoss: 1.7385140657424927\n",
      "Epoch: 1783 / dLoss: 0.9286602735519409 / gLoss: 1.5949592590332031\n",
      "Epoch: 1784 / dLoss: 1.0949510335922241 / gLoss: 1.60103440284729\n",
      "Epoch: 1785 / dLoss: 0.9376267790794373 / gLoss: 1.5526673793792725\n",
      "Epoch: 1786 / dLoss: 0.9771628379821777 / gLoss: 1.4500794410705566\n",
      "Epoch: 1787 / dLoss: 0.8526115417480469 / gLoss: 1.7293806076049805\n",
      "Epoch: 1788 / dLoss: 1.026383638381958 / gLoss: 1.5140206813812256\n",
      "Epoch: 1789 / dLoss: 0.8787029981613159 / gLoss: 1.6532081365585327\n",
      "Epoch: 1790 / dLoss: 0.8743051290512085 / gLoss: 1.6118474006652832\n",
      "Epoch: 1791 / dLoss: 0.9974696040153503 / gLoss: 1.7339951992034912\n",
      "Epoch: 1792 / dLoss: 0.9515348672866821 / gLoss: 1.4387544393539429\n",
      "Epoch: 1793 / dLoss: 1.0183967351913452 / gLoss: 1.3592365980148315\n",
      "Epoch: 1794 / dLoss: 0.993441104888916 / gLoss: 1.5435246229171753\n",
      "Epoch: 1795 / dLoss: 1.0770933628082275 / gLoss: 1.5229231119155884\n",
      "Epoch: 1796 / dLoss: 1.032472848892212 / gLoss: 1.6531286239624023\n",
      "Epoch: 1797 / dLoss: 0.9158508777618408 / gLoss: 2.000791549682617\n",
      "Epoch: 1798 / dLoss: 0.9128139019012451 / gLoss: 2.2762818336486816\n",
      "Epoch: 1799 / dLoss: 0.8717820644378662 / gLoss: 2.121863603591919\n",
      "Epoch: 1800 / dLoss: 0.9553599953651428 / gLoss: 2.1171469688415527\n",
      "Epoch: 1801 / dLoss: 0.8266772031784058 / gLoss: 1.6442396640777588\n",
      "Epoch: 1802 / dLoss: 0.7243328094482422 / gLoss: 1.7044018507003784\n",
      "Epoch: 1803 / dLoss: 0.85337233543396 / gLoss: 1.9353715181350708\n",
      "Epoch: 1804 / dLoss: 0.935702383518219 / gLoss: 1.6129342317581177\n",
      "Epoch: 1805 / dLoss: 1.0337014198303223 / gLoss: 1.5812541246414185\n",
      "Epoch: 1806 / dLoss: 1.1212884187698364 / gLoss: 1.5690439939498901\n",
      "Epoch: 1807 / dLoss: 0.9633913636207581 / gLoss: 1.6725879907608032\n",
      "Epoch: 1808 / dLoss: 1.0195682048797607 / gLoss: 1.5950751304626465\n",
      "Epoch: 1809 / dLoss: 0.8823376893997192 / gLoss: 1.575189232826233\n",
      "Epoch: 1810 / dLoss: 0.9513039588928223 / gLoss: 1.8436872959136963\n",
      "Epoch: 1811 / dLoss: 0.8826322555541992 / gLoss: 1.851708173751831\n",
      "Epoch: 1812 / dLoss: 0.7441396713256836 / gLoss: 1.833614706993103\n",
      "Epoch: 1813 / dLoss: 0.781466007232666 / gLoss: 1.8380792140960693\n",
      "Epoch: 1814 / dLoss: 0.850145697593689 / gLoss: 1.7836449146270752\n",
      "Epoch: 1815 / dLoss: 0.9622372388839722 / gLoss: 1.6371647119522095\n",
      "Epoch: 1816 / dLoss: 1.0163443088531494 / gLoss: 1.640188455581665\n",
      "Epoch: 1817 / dLoss: 1.006258249282837 / gLoss: 1.607225775718689\n",
      "Epoch: 1818 / dLoss: 1.1138086318969727 / gLoss: 1.5678305625915527\n",
      "Epoch: 1819 / dLoss: 1.1863036155700684 / gLoss: 1.5338091850280762\n",
      "Epoch: 1820 / dLoss: 1.0415678024291992 / gLoss: 1.4607561826705933\n",
      "Epoch: 1821 / dLoss: 0.9038207530975342 / gLoss: 2.224987268447876\n",
      "Epoch: 1822 / dLoss: 1.0556542873382568 / gLoss: 2.0932180881500244\n",
      "Epoch: 1823 / dLoss: 0.9136109948158264 / gLoss: 1.9631690979003906\n",
      "Epoch: 1824 / dLoss: 0.8472970724105835 / gLoss: 2.0347461700439453\n",
      "Epoch: 1825 / dLoss: 0.9414392709732056 / gLoss: 1.806755781173706\n",
      "Epoch: 1826 / dLoss: 0.6753607988357544 / gLoss: 2.011914014816284\n",
      "Epoch: 1827 / dLoss: 0.9672011733055115 / gLoss: 2.0110487937927246\n",
      "Epoch: 1828 / dLoss: 0.9129539728164673 / gLoss: 1.5398677587509155\n",
      "Epoch: 1829 / dLoss: 0.8769741058349609 / gLoss: 1.3694120645523071\n",
      "Epoch: 1830 / dLoss: 1.0129368305206299 / gLoss: 1.4881761074066162\n",
      "Epoch: 1831 / dLoss: 1.0836505889892578 / gLoss: 1.398698091506958\n",
      "Epoch: 1832 / dLoss: 1.1433963775634766 / gLoss: 1.258139967918396\n",
      "Epoch: 1833 / dLoss: 1.1792457103729248 / gLoss: 1.3167766332626343\n",
      "Epoch: 1834 / dLoss: 0.924729585647583 / gLoss: 1.7424923181533813\n",
      "Epoch: 1835 / dLoss: 0.9677039384841919 / gLoss: 1.9878002405166626\n",
      "Epoch: 1836 / dLoss: 0.9368377923965454 / gLoss: 2.215949773788452\n",
      "Epoch: 1837 / dLoss: 0.8110551238059998 / gLoss: 2.190513849258423\n",
      "Epoch: 1838 / dLoss: 0.7664653062820435 / gLoss: 1.8335754871368408\n",
      "Epoch: 1839 / dLoss: 0.8883881568908691 / gLoss: 1.8282023668289185\n",
      "Epoch: 1840 / dLoss: 0.9809738397598267 / gLoss: 1.5849162340164185\n",
      "Epoch: 1841 / dLoss: 1.1648306846618652 / gLoss: 1.7261898517608643\n",
      "Epoch: 1842 / dLoss: 1.1546742916107178 / gLoss: 1.6891597509384155\n",
      "Epoch: 1843 / dLoss: 1.1463629007339478 / gLoss: 1.6728466749191284\n",
      "Epoch: 1844 / dLoss: 1.1796537637710571 / gLoss: 1.6698631048202515\n",
      "Epoch: 1845 / dLoss: 1.089925765991211 / gLoss: 1.6888740062713623\n",
      "Epoch: 1846 / dLoss: 1.0795416831970215 / gLoss: 1.8475080728530884\n",
      "Epoch: 1847 / dLoss: 0.890785813331604 / gLoss: 2.0831685066223145\n",
      "Epoch: 1848 / dLoss: 0.8866778612136841 / gLoss: 1.8836774826049805\n",
      "Epoch: 1849 / dLoss: 0.8390144109725952 / gLoss: 2.369283676147461\n",
      "Epoch: 1850 / dLoss: 0.7552820444107056 / gLoss: 2.315523624420166\n",
      "Epoch: 1851 / dLoss: 0.7642526626586914 / gLoss: 1.9395922422409058\n",
      "Epoch: 1852 / dLoss: 0.990748405456543 / gLoss: 1.790432095527649\n",
      "Epoch: 1853 / dLoss: 0.9171630144119263 / gLoss: 1.4363088607788086\n",
      "Epoch: 1854 / dLoss: 1.0584783554077148 / gLoss: 1.2961361408233643\n",
      "Epoch: 1855 / dLoss: 1.0198683738708496 / gLoss: 1.5653762817382812\n",
      "Epoch: 1856 / dLoss: 1.0680550336837769 / gLoss: 1.3376057147979736\n",
      "Epoch: 1857 / dLoss: 1.1047283411026 / gLoss: 1.4790300130844116\n",
      "Epoch: 1858 / dLoss: 1.1324619054794312 / gLoss: 1.7442080974578857\n",
      "Epoch: 1859 / dLoss: 1.0714304447174072 / gLoss: 1.5777031183242798\n",
      "Epoch: 1860 / dLoss: 1.0050827264785767 / gLoss: 2.1812119483947754\n",
      "Epoch: 1861 / dLoss: 0.8457383513450623 / gLoss: 2.079902172088623\n",
      "Epoch: 1862 / dLoss: 0.9007924795150757 / gLoss: 1.9315612316131592\n",
      "Epoch: 1863 / dLoss: 0.9750601053237915 / gLoss: 1.7375435829162598\n",
      "Epoch: 1864 / dLoss: 0.894729733467102 / gLoss: 1.7796275615692139\n",
      "Epoch: 1865 / dLoss: 0.9899660348892212 / gLoss: 1.692106008529663\n",
      "Epoch: 1866 / dLoss: 1.0031208992004395 / gLoss: 1.8578453063964844\n",
      "Epoch: 1867 / dLoss: 1.0479190349578857 / gLoss: 1.7357776165008545\n",
      "Epoch: 1868 / dLoss: 0.9933913946151733 / gLoss: 1.7037864923477173\n",
      "Epoch: 1869 / dLoss: 1.128434419631958 / gLoss: 1.6468521356582642\n",
      "Epoch: 1870 / dLoss: 0.9619632959365845 / gLoss: 1.9545265436172485\n",
      "Epoch: 1871 / dLoss: 1.087565302848816 / gLoss: 1.8717094659805298\n",
      "Epoch: 1872 / dLoss: 0.9252616167068481 / gLoss: 1.8642197847366333\n",
      "Epoch: 1873 / dLoss: 0.8672360181808472 / gLoss: 1.7467772960662842\n",
      "Epoch: 1874 / dLoss: 0.9000521302223206 / gLoss: 2.127971887588501\n",
      "Epoch: 1875 / dLoss: 0.9469449520111084 / gLoss: 2.104562282562256\n",
      "Epoch: 1876 / dLoss: 0.8815677165985107 / gLoss: 2.014606475830078\n",
      "Epoch: 1877 / dLoss: 0.8845717310905457 / gLoss: 1.899612545967102\n",
      "Epoch: 1878 / dLoss: 0.9685488939285278 / gLoss: 1.6080307960510254\n",
      "Epoch: 1879 / dLoss: 0.9646660089492798 / gLoss: 1.4356982707977295\n",
      "Epoch: 1880 / dLoss: 0.9571411609649658 / gLoss: 1.8986111879348755\n",
      "Epoch: 1881 / dLoss: 1.0089703798294067 / gLoss: 1.4568804502487183\n",
      "Epoch: 1882 / dLoss: 0.9553301334381104 / gLoss: 1.425384521484375\n",
      "Epoch: 1883 / dLoss: 1.0191278457641602 / gLoss: 1.5189534425735474\n",
      "Epoch: 1884 / dLoss: 0.951827883720398 / gLoss: 1.5590003728866577\n",
      "Epoch: 1885 / dLoss: 0.9689910411834717 / gLoss: 1.607428789138794\n",
      "Epoch: 1886 / dLoss: 0.9246877431869507 / gLoss: 1.83827543258667\n",
      "Epoch: 1887 / dLoss: 0.9423242807388306 / gLoss: 1.8440850973129272\n",
      "Epoch: 1888 / dLoss: 0.9930237531661987 / gLoss: 2.1945977210998535\n",
      "Epoch: 1889 / dLoss: 0.8847881555557251 / gLoss: 1.4952317476272583\n",
      "Epoch: 1890 / dLoss: 0.926466703414917 / gLoss: 1.8596171140670776\n",
      "Epoch: 1891 / dLoss: 0.9229806661605835 / gLoss: 1.8105723857879639\n",
      "Epoch: 1892 / dLoss: 0.8830851316452026 / gLoss: 1.5528563261032104\n",
      "Epoch: 1893 / dLoss: 0.9801629781723022 / gLoss: 1.7677305936813354\n",
      "Epoch: 1894 / dLoss: 1.0063425302505493 / gLoss: 1.7558090686798096\n",
      "Epoch: 1895 / dLoss: 1.072672724723816 / gLoss: 1.5882272720336914\n",
      "Epoch: 1896 / dLoss: 1.034441590309143 / gLoss: 1.6935991048812866\n",
      "Epoch: 1897 / dLoss: 0.9273074865341187 / gLoss: 1.752323865890503\n",
      "Epoch: 1898 / dLoss: 0.9581582546234131 / gLoss: 1.91969633102417\n",
      "Epoch: 1899 / dLoss: 0.87618088722229 / gLoss: 1.7865777015686035\n",
      "Epoch: 1900 / dLoss: 0.9756550192832947 / gLoss: 1.97416090965271\n",
      "Epoch: 1901 / dLoss: 0.9111658334732056 / gLoss: 1.6582086086273193\n",
      "Epoch: 1902 / dLoss: 0.8502625823020935 / gLoss: 1.7921212911605835\n",
      "Epoch: 1903 / dLoss: 0.8612905740737915 / gLoss: 1.5905981063842773\n",
      "Epoch: 1904 / dLoss: 0.7835991382598877 / gLoss: 1.8068064451217651\n",
      "Epoch: 1905 / dLoss: 0.9333585500717163 / gLoss: 1.729723334312439\n",
      "Epoch: 1906 / dLoss: 0.969338595867157 / gLoss: 1.701391577720642\n",
      "Epoch: 1907 / dLoss: 1.0641652345657349 / gLoss: 1.3133649826049805\n",
      "Epoch: 1908 / dLoss: 1.131072759628296 / gLoss: 1.505185604095459\n",
      "Epoch: 1909 / dLoss: 0.9332984685897827 / gLoss: 1.6290867328643799\n",
      "Epoch: 1910 / dLoss: 0.8891074657440186 / gLoss: 1.5983059406280518\n",
      "Epoch: 1911 / dLoss: 0.9784367084503174 / gLoss: 1.549049735069275\n",
      "Epoch: 1912 / dLoss: 0.960437536239624 / gLoss: 1.5294280052185059\n",
      "Epoch: 1913 / dLoss: 0.8785561919212341 / gLoss: 2.001335859298706\n",
      "Epoch: 1914 / dLoss: 0.8884235620498657 / gLoss: 1.7302467823028564\n",
      "Epoch: 1915 / dLoss: 0.7984271049499512 / gLoss: 1.8589798212051392\n",
      "Epoch: 1916 / dLoss: 0.8963683843612671 / gLoss: 1.9113268852233887\n",
      "Epoch: 1917 / dLoss: 0.9630534648895264 / gLoss: 1.5596646070480347\n",
      "Epoch: 1918 / dLoss: 0.9094760417938232 / gLoss: 1.587699294090271\n",
      "Epoch: 1919 / dLoss: 0.9004634618759155 / gLoss: 1.7017725706100464\n",
      "Epoch: 1920 / dLoss: 0.8730406761169434 / gLoss: 1.4137567281723022\n",
      "Epoch: 1921 / dLoss: 0.8682222366333008 / gLoss: 1.8158015012741089\n",
      "Epoch: 1922 / dLoss: 0.957825779914856 / gLoss: 1.8175597190856934\n",
      "Epoch: 1923 / dLoss: 0.9556248784065247 / gLoss: 1.7767612934112549\n",
      "Epoch: 1924 / dLoss: 0.9335120916366577 / gLoss: 1.7040421962738037\n",
      "Epoch: 1925 / dLoss: 0.8538804054260254 / gLoss: 1.8730260133743286\n",
      "Epoch: 1926 / dLoss: 0.8674747943878174 / gLoss: 1.7242776155471802\n",
      "Epoch: 1927 / dLoss: 0.8278714418411255 / gLoss: 1.770849585533142\n",
      "Epoch: 1928 / dLoss: 0.800161600112915 / gLoss: 1.8565833568572998\n",
      "Epoch: 1929 / dLoss: 0.8373311758041382 / gLoss: 1.9971145391464233\n",
      "Epoch: 1930 / dLoss: 0.8903548717498779 / gLoss: 1.8249377012252808\n",
      "Epoch: 1931 / dLoss: 0.9126541018486023 / gLoss: 1.7233607769012451\n",
      "Epoch: 1932 / dLoss: 0.8480879068374634 / gLoss: 1.7131092548370361\n",
      "Epoch: 1933 / dLoss: 0.8899316787719727 / gLoss: 1.5765900611877441\n",
      "Epoch: 1934 / dLoss: 0.8386664390563965 / gLoss: 1.6726473569869995\n",
      "Epoch: 1935 / dLoss: 0.8781676292419434 / gLoss: 1.6070656776428223\n",
      "Epoch: 1936 / dLoss: 0.9817046523094177 / gLoss: 1.5617135763168335\n",
      "Epoch: 1937 / dLoss: 0.9483942985534668 / gLoss: 1.5383573770523071\n",
      "Epoch: 1938 / dLoss: 0.88267982006073 / gLoss: 1.610545039176941\n",
      "Epoch: 1939 / dLoss: 0.91206955909729 / gLoss: 1.526914358139038\n",
      "Epoch: 1940 / dLoss: 0.8564013242721558 / gLoss: 1.9701859951019287\n",
      "Epoch: 1941 / dLoss: 0.9827913045883179 / gLoss: 1.8148128986358643\n",
      "Epoch: 1942 / dLoss: 0.9046036005020142 / gLoss: 1.4467520713806152\n",
      "Epoch: 1943 / dLoss: 0.7688536047935486 / gLoss: 1.6922399997711182\n",
      "Epoch: 1944 / dLoss: 0.7707920670509338 / gLoss: 2.0013911724090576\n",
      "Epoch: 1945 / dLoss: 0.9118753671646118 / gLoss: 1.6874722242355347\n",
      "Epoch: 1946 / dLoss: 0.8419706225395203 / gLoss: 1.8598366975784302\n",
      "Epoch: 1947 / dLoss: 0.9834942817687988 / gLoss: 1.7026523351669312\n",
      "Epoch: 1948 / dLoss: 1.2061057090759277 / gLoss: 1.6064475774765015\n",
      "Epoch: 1949 / dLoss: 1.0282245874404907 / gLoss: 1.4131665229797363\n",
      "Epoch: 1950 / dLoss: 0.9580014944076538 / gLoss: 1.5866912603378296\n",
      "Epoch: 1951 / dLoss: 0.9395657777786255 / gLoss: 1.881505012512207\n",
      "Epoch: 1952 / dLoss: 0.8317407965660095 / gLoss: 1.9024286270141602\n",
      "Epoch: 1953 / dLoss: 0.8747010231018066 / gLoss: 1.8360956907272339\n",
      "Epoch: 1954 / dLoss: 0.9191438555717468 / gLoss: 1.8215510845184326\n",
      "Epoch: 1955 / dLoss: 0.8544741868972778 / gLoss: 1.8559514284133911\n",
      "Epoch: 1956 / dLoss: 0.85161292552948 / gLoss: 1.8836256265640259\n",
      "Epoch: 1957 / dLoss: 0.9394018054008484 / gLoss: 1.9467602968215942\n",
      "Epoch: 1958 / dLoss: 0.8546829223632812 / gLoss: 1.921846628189087\n",
      "Epoch: 1959 / dLoss: 0.7732879519462585 / gLoss: 1.494883418083191\n",
      "Epoch: 1960 / dLoss: 1.018913984298706 / gLoss: 1.766782283782959\n",
      "Epoch: 1961 / dLoss: 0.9170424938201904 / gLoss: 1.499208688735962\n",
      "Epoch: 1962 / dLoss: 0.8891744613647461 / gLoss: 1.7762761116027832\n",
      "Epoch: 1963 / dLoss: 1.0014395713806152 / gLoss: 1.6462790966033936\n",
      "Epoch: 1964 / dLoss: 1.0061802864074707 / gLoss: 1.5345900058746338\n",
      "Epoch: 1965 / dLoss: 0.9766129851341248 / gLoss: 1.4312422275543213\n",
      "Epoch: 1966 / dLoss: 0.7623798251152039 / gLoss: 1.7457355260849\n",
      "Epoch: 1967 / dLoss: 0.9063236713409424 / gLoss: 1.5590600967407227\n",
      "Epoch: 1968 / dLoss: 0.9108630418777466 / gLoss: 1.5555461645126343\n",
      "Epoch: 1969 / dLoss: 0.9520543813705444 / gLoss: 1.9087646007537842\n",
      "Epoch: 1970 / dLoss: 0.9475263953208923 / gLoss: 1.8387138843536377\n",
      "Epoch: 1971 / dLoss: 0.8327919244766235 / gLoss: 1.8341293334960938\n",
      "Epoch: 1972 / dLoss: 0.8473960161209106 / gLoss: 1.9001988172531128\n",
      "Epoch: 1973 / dLoss: 0.9056335687637329 / gLoss: 1.6525367498397827\n",
      "Epoch: 1974 / dLoss: 0.8968806862831116 / gLoss: 1.599103569984436\n",
      "Epoch: 1975 / dLoss: 1.0004017353057861 / gLoss: 2.009979009628296\n",
      "Epoch: 1976 / dLoss: 1.0040535926818848 / gLoss: 1.4808006286621094\n",
      "Epoch: 1977 / dLoss: 0.9674017429351807 / gLoss: 1.9421970844268799\n",
      "Epoch: 1978 / dLoss: 0.8730562925338745 / gLoss: 1.8554469347000122\n",
      "Epoch: 1979 / dLoss: 0.9320271015167236 / gLoss: 1.5793366432189941\n",
      "Epoch: 1980 / dLoss: 0.8897733688354492 / gLoss: 1.9060081243515015\n",
      "Epoch: 1981 / dLoss: 0.8856436014175415 / gLoss: 1.944200873374939\n",
      "Epoch: 1982 / dLoss: 0.8040496110916138 / gLoss: 1.8599879741668701\n",
      "Epoch: 1983 / dLoss: 0.8776122331619263 / gLoss: 1.9036500453948975\n",
      "Epoch: 1984 / dLoss: 0.8271933794021606 / gLoss: 1.4491456747055054\n",
      "Epoch: 1985 / dLoss: 0.8113497495651245 / gLoss: 1.6861701011657715\n",
      "Epoch: 1986 / dLoss: 0.8989081978797913 / gLoss: 1.6822375059127808\n",
      "Epoch: 1987 / dLoss: 0.865283191204071 / gLoss: 1.8007538318634033\n",
      "Epoch: 1988 / dLoss: 0.9990575313568115 / gLoss: 1.9432181119918823\n",
      "Epoch: 1989 / dLoss: 0.8635379076004028 / gLoss: 1.6939812898635864\n",
      "Epoch: 1990 / dLoss: 0.9444441199302673 / gLoss: 1.618991732597351\n",
      "Epoch: 1991 / dLoss: 1.0229392051696777 / gLoss: 1.9656448364257812\n",
      "Epoch: 1992 / dLoss: 0.8071194887161255 / gLoss: 1.3457294702529907\n",
      "Epoch: 1993 / dLoss: 0.9166001081466675 / gLoss: 1.8400769233703613\n",
      "Epoch: 1994 / dLoss: 0.8903672695159912 / gLoss: 1.7589186429977417\n",
      "Epoch: 1995 / dLoss: 1.0180413722991943 / gLoss: 1.785525918006897\n",
      "Epoch: 1996 / dLoss: 0.8686343431472778 / gLoss: 1.502240777015686\n",
      "Epoch: 1997 / dLoss: 0.8507468700408936 / gLoss: 1.9277310371398926\n",
      "Epoch: 1998 / dLoss: 0.949697732925415 / gLoss: 2.000875234603882\n",
      "Epoch: 1999 / dLoss: 0.9225300550460815 / gLoss: 1.6341313123703003\n",
      "Epoch: 2000 / dLoss: 0.943557858467102 / gLoss: 1.651160478591919\n",
      "Epoch: 2001 / dLoss: 1.0097122192382812 / gLoss: 1.5346266031265259\n",
      "Epoch: 2002 / dLoss: 0.8948372602462769 / gLoss: 1.762325644493103\n",
      "Epoch: 2003 / dLoss: 0.9308863282203674 / gLoss: 1.9453641176223755\n",
      "Epoch: 2004 / dLoss: 0.85420823097229 / gLoss: 1.9312880039215088\n",
      "Epoch: 2005 / dLoss: 1.0692713260650635 / gLoss: 1.735425591468811\n",
      "Epoch: 2006 / dLoss: 0.8901238441467285 / gLoss: 1.9790822267532349\n",
      "Epoch: 2007 / dLoss: 1.001342535018921 / gLoss: 1.6785495281219482\n",
      "Epoch: 2008 / dLoss: 0.8529541492462158 / gLoss: 1.6590909957885742\n",
      "Epoch: 2009 / dLoss: 0.9016989469528198 / gLoss: 1.6742424964904785\n",
      "Epoch: 2010 / dLoss: 1.011833906173706 / gLoss: 1.7192301750183105\n",
      "Epoch: 2011 / dLoss: 0.8922203183174133 / gLoss: 1.7275326251983643\n",
      "Epoch: 2012 / dLoss: 0.9376956224441528 / gLoss: 1.75275456905365\n",
      "Epoch: 2013 / dLoss: 1.0844089984893799 / gLoss: 1.6426398754119873\n",
      "Epoch: 2014 / dLoss: 1.0307495594024658 / gLoss: 1.6197874546051025\n",
      "Epoch: 2015 / dLoss: 0.908666729927063 / gLoss: 1.6884276866912842\n",
      "Epoch: 2016 / dLoss: 0.875988245010376 / gLoss: 1.8510947227478027\n",
      "Epoch: 2017 / dLoss: 0.846297025680542 / gLoss: 1.7763183116912842\n",
      "Epoch: 2018 / dLoss: 0.8174495697021484 / gLoss: 1.70461106300354\n",
      "Epoch: 2019 / dLoss: 0.9151882529258728 / gLoss: 1.7381435632705688\n",
      "Epoch: 2020 / dLoss: 0.8933380842208862 / gLoss: 1.9031786918640137\n",
      "Epoch: 2021 / dLoss: 0.8501197099685669 / gLoss: 1.6443899869918823\n",
      "Epoch: 2022 / dLoss: 0.8548352718353271 / gLoss: 2.096489667892456\n",
      "Epoch: 2023 / dLoss: 0.9910445809364319 / gLoss: 1.7996517419815063\n",
      "Epoch: 2024 / dLoss: 1.038962960243225 / gLoss: 1.7643591165542603\n",
      "Epoch: 2025 / dLoss: 1.0552325248718262 / gLoss: 1.6437991857528687\n",
      "Epoch: 2026 / dLoss: 1.048123836517334 / gLoss: 1.6981364488601685\n",
      "Epoch: 2027 / dLoss: 0.9580821990966797 / gLoss: 2.0445399284362793\n",
      "Epoch: 2028 / dLoss: 1.0559508800506592 / gLoss: 1.9272493124008179\n",
      "Epoch: 2029 / dLoss: 1.0144407749176025 / gLoss: 1.7341439723968506\n",
      "Epoch: 2030 / dLoss: 0.7936056852340698 / gLoss: 1.8883923292160034\n",
      "Epoch: 2031 / dLoss: 0.871958315372467 / gLoss: 1.935410976409912\n",
      "Epoch: 2032 / dLoss: 0.8581269979476929 / gLoss: 2.100297212600708\n",
      "Epoch: 2033 / dLoss: 0.8934715986251831 / gLoss: 1.767212986946106\n",
      "Epoch: 2034 / dLoss: 0.920445442199707 / gLoss: 1.9005937576293945\n",
      "Epoch: 2035 / dLoss: 0.8471299409866333 / gLoss: 1.862778663635254\n",
      "Epoch: 2036 / dLoss: 0.9347646236419678 / gLoss: 1.6906262636184692\n",
      "Epoch: 2037 / dLoss: 1.020171880722046 / gLoss: 1.7420154809951782\n",
      "Epoch: 2038 / dLoss: 0.9039534330368042 / gLoss: 1.7495728731155396\n",
      "Epoch: 2039 / dLoss: 0.9618197679519653 / gLoss: 1.757667899131775\n",
      "Epoch: 2040 / dLoss: 0.9292446374893188 / gLoss: 1.8185806274414062\n",
      "Epoch: 2041 / dLoss: 0.931814432144165 / gLoss: 1.8123464584350586\n",
      "Epoch: 2042 / dLoss: 0.9180310964584351 / gLoss: 1.9112695455551147\n",
      "Epoch: 2043 / dLoss: 0.7858126759529114 / gLoss: 1.9336658716201782\n",
      "Epoch: 2044 / dLoss: 0.839851975440979 / gLoss: 1.5256357192993164\n",
      "Epoch: 2045 / dLoss: 0.8992786407470703 / gLoss: 1.9764460325241089\n",
      "Epoch: 2046 / dLoss: 0.962404727935791 / gLoss: 1.8272156715393066\n",
      "Epoch: 2047 / dLoss: 0.946621298789978 / gLoss: 1.836625099182129\n",
      "Epoch: 2048 / dLoss: 0.8619633913040161 / gLoss: 1.6153620481491089\n",
      "Epoch: 2049 / dLoss: 0.8820186853408813 / gLoss: 1.7578696012496948\n",
      "Epoch: 2050 / dLoss: 0.9445716738700867 / gLoss: 1.9735229015350342\n",
      "Epoch: 2051 / dLoss: 1.0229289531707764 / gLoss: 1.6944268941879272\n",
      "Epoch: 2052 / dLoss: 0.9132443070411682 / gLoss: 1.6812890768051147\n",
      "Epoch: 2053 / dLoss: 0.7232241034507751 / gLoss: 2.0396604537963867\n",
      "Epoch: 2054 / dLoss: 0.7557113170623779 / gLoss: 1.6408261060714722\n",
      "Epoch: 2055 / dLoss: 0.9309253692626953 / gLoss: 1.6735960245132446\n",
      "Epoch: 2056 / dLoss: 0.8519341945648193 / gLoss: 2.138742446899414\n",
      "Epoch: 2057 / dLoss: 0.8379877805709839 / gLoss: 1.6151552200317383\n",
      "Epoch: 2058 / dLoss: 0.9303377866744995 / gLoss: 1.9631942510604858\n",
      "Epoch: 2059 / dLoss: 0.873288631439209 / gLoss: 1.827130913734436\n",
      "Epoch: 2060 / dLoss: 0.8977511525154114 / gLoss: 1.8522748947143555\n",
      "Epoch: 2061 / dLoss: 0.8441025018692017 / gLoss: 1.741326928138733\n",
      "Epoch: 2062 / dLoss: 0.8938224911689758 / gLoss: 1.647128939628601\n",
      "Epoch: 2063 / dLoss: 0.8406362533569336 / gLoss: 1.510866403579712\n",
      "Epoch: 2064 / dLoss: 0.9270882606506348 / gLoss: 1.7314785718917847\n",
      "Epoch: 2065 / dLoss: 0.9160253405570984 / gLoss: 1.6299530267715454\n",
      "Epoch: 2066 / dLoss: 0.9355149269104004 / gLoss: 1.540632963180542\n",
      "Epoch: 2067 / dLoss: 0.8791870474815369 / gLoss: 1.4968664646148682\n",
      "Epoch: 2068 / dLoss: 0.8332139253616333 / gLoss: 1.875481128692627\n",
      "Epoch: 2069 / dLoss: 0.8555647134780884 / gLoss: 2.0142455101013184\n",
      "Epoch: 2070 / dLoss: 0.872849702835083 / gLoss: 1.6006858348846436\n",
      "Epoch: 2071 / dLoss: 0.905540943145752 / gLoss: 1.7390743494033813\n",
      "Epoch: 2072 / dLoss: 0.7868162393569946 / gLoss: 1.9124702215194702\n",
      "Epoch: 2073 / dLoss: 0.845461368560791 / gLoss: 1.8628894090652466\n",
      "Epoch: 2074 / dLoss: 0.887212336063385 / gLoss: 1.4759950637817383\n",
      "Epoch: 2075 / dLoss: 0.8798363208770752 / gLoss: 1.8195937871932983\n",
      "Epoch: 2076 / dLoss: 1.0091944932937622 / gLoss: 1.8413681983947754\n",
      "Epoch: 2077 / dLoss: 0.8380169868469238 / gLoss: 1.5791376829147339\n",
      "Epoch: 2078 / dLoss: 1.0386488437652588 / gLoss: 1.6002475023269653\n",
      "Epoch: 2079 / dLoss: 0.8463724851608276 / gLoss: 1.7545733451843262\n",
      "Epoch: 2080 / dLoss: 0.9946106672286987 / gLoss: 1.923948049545288\n",
      "Epoch: 2081 / dLoss: 0.9180625677108765 / gLoss: 1.6203241348266602\n",
      "Epoch: 2082 / dLoss: 0.9097182154655457 / gLoss: 1.6931931972503662\n",
      "Epoch: 2083 / dLoss: 0.8930799961090088 / gLoss: 1.8397005796432495\n",
      "Epoch: 2084 / dLoss: 0.9398354291915894 / gLoss: 1.5730360746383667\n",
      "Epoch: 2085 / dLoss: 0.9805209636688232 / gLoss: 1.6847249269485474\n",
      "Epoch: 2086 / dLoss: 0.988469123840332 / gLoss: 1.599436640739441\n",
      "Epoch: 2087 / dLoss: 0.9334038496017456 / gLoss: 1.6834821701049805\n",
      "Epoch: 2088 / dLoss: 0.7859171628952026 / gLoss: 1.5574373006820679\n",
      "Epoch: 2089 / dLoss: 0.8792585730552673 / gLoss: 1.891265869140625\n",
      "Epoch: 2090 / dLoss: 0.8314160108566284 / gLoss: 1.9594436883926392\n",
      "Epoch: 2091 / dLoss: 0.870928168296814 / gLoss: 1.7798871994018555\n",
      "Epoch: 2092 / dLoss: 0.9261260032653809 / gLoss: 1.7857011556625366\n",
      "Epoch: 2093 / dLoss: 0.9315394759178162 / gLoss: 1.5373142957687378\n",
      "Epoch: 2094 / dLoss: 0.8786520957946777 / gLoss: 1.677419900894165\n",
      "Epoch: 2095 / dLoss: 0.7980118989944458 / gLoss: 2.0739834308624268\n",
      "Epoch: 2096 / dLoss: 0.9694771766662598 / gLoss: 1.8343931436538696\n",
      "Epoch: 2097 / dLoss: 0.9209600687026978 / gLoss: 2.0003297328948975\n",
      "Epoch: 2098 / dLoss: 0.9604206085205078 / gLoss: 1.9213794469833374\n",
      "Epoch: 2099 / dLoss: 0.8688874244689941 / gLoss: 1.7009774446487427\n",
      "Epoch: 2100 / dLoss: 0.8853208422660828 / gLoss: 1.6853785514831543\n",
      "Epoch: 2101 / dLoss: 0.8874449729919434 / gLoss: 1.4601026773452759\n",
      "Epoch: 2102 / dLoss: 0.8674717545509338 / gLoss: 2.0750391483306885\n",
      "Epoch: 2103 / dLoss: 0.9695411920547485 / gLoss: 1.91424560546875\n",
      "Epoch: 2104 / dLoss: 0.7892332673072815 / gLoss: 1.7603905200958252\n",
      "Epoch: 2105 / dLoss: 0.9349943995475769 / gLoss: 1.8676846027374268\n",
      "Epoch: 2106 / dLoss: 0.8753821849822998 / gLoss: 1.6350370645523071\n",
      "Epoch: 2107 / dLoss: 0.832311749458313 / gLoss: 1.702089786529541\n",
      "Epoch: 2108 / dLoss: 0.9713919758796692 / gLoss: 1.7200055122375488\n",
      "Epoch: 2109 / dLoss: 0.9439281225204468 / gLoss: 1.717598557472229\n",
      "Epoch: 2110 / dLoss: 0.9148996472358704 / gLoss: 1.5497013330459595\n",
      "Epoch: 2111 / dLoss: 0.8637174367904663 / gLoss: 1.7066268920898438\n",
      "Epoch: 2112 / dLoss: 0.8657935857772827 / gLoss: 1.6818323135375977\n",
      "Epoch: 2113 / dLoss: 1.005579948425293 / gLoss: 1.7810161113739014\n",
      "Epoch: 2114 / dLoss: 0.7947381734848022 / gLoss: 1.6081753969192505\n",
      "Epoch: 2115 / dLoss: 0.9473528861999512 / gLoss: 1.6084545850753784\n",
      "Epoch: 2116 / dLoss: 0.8764851093292236 / gLoss: 1.5951869487762451\n",
      "Epoch: 2117 / dLoss: 0.8898235559463501 / gLoss: 1.7519323825836182\n",
      "Epoch: 2118 / dLoss: 0.872745156288147 / gLoss: 1.996228814125061\n",
      "Epoch: 2119 / dLoss: 0.919410228729248 / gLoss: 1.956079363822937\n",
      "Epoch: 2120 / dLoss: 0.8352689743041992 / gLoss: 1.883499264717102\n",
      "Epoch: 2121 / dLoss: 0.8997427225112915 / gLoss: 1.6104618310928345\n",
      "Epoch: 2122 / dLoss: 0.947120189666748 / gLoss: 1.7179635763168335\n",
      "Epoch: 2123 / dLoss: 0.8812305927276611 / gLoss: 1.7963544130325317\n",
      "Epoch: 2124 / dLoss: 0.8390408754348755 / gLoss: 1.7697227001190186\n",
      "Epoch: 2125 / dLoss: 0.9772490859031677 / gLoss: 1.8850946426391602\n",
      "Epoch: 2126 / dLoss: 0.8883994817733765 / gLoss: 1.5454221963882446\n",
      "Epoch: 2127 / dLoss: 0.7458304762840271 / gLoss: 1.8821730613708496\n",
      "Epoch: 2128 / dLoss: 0.8629142045974731 / gLoss: 1.5372936725616455\n",
      "Epoch: 2129 / dLoss: 0.8440861701965332 / gLoss: 1.612700343132019\n",
      "Epoch: 2130 / dLoss: 0.8960549831390381 / gLoss: 1.8757846355438232\n",
      "Epoch: 2131 / dLoss: 0.9326810240745544 / gLoss: 1.8196134567260742\n",
      "Epoch: 2132 / dLoss: 1.0221723318099976 / gLoss: 1.6586967706680298\n",
      "Epoch: 2133 / dLoss: 0.8497127890586853 / gLoss: 1.7365314960479736\n",
      "Epoch: 2134 / dLoss: 0.8804187774658203 / gLoss: 1.6312986612319946\n",
      "Epoch: 2135 / dLoss: 0.9083884954452515 / gLoss: 1.7631818056106567\n",
      "Epoch: 2136 / dLoss: 0.9414331912994385 / gLoss: 1.6040440797805786\n",
      "Epoch: 2137 / dLoss: 0.8639812469482422 / gLoss: 1.8068629503250122\n",
      "Epoch: 2138 / dLoss: 0.8745297789573669 / gLoss: 1.9384161233901978\n",
      "Epoch: 2139 / dLoss: 0.9414811730384827 / gLoss: 1.7328360080718994\n",
      "Epoch: 2140 / dLoss: 0.9346866607666016 / gLoss: 1.781875491142273\n",
      "Epoch: 2141 / dLoss: 0.9792895317077637 / gLoss: 1.6817718744277954\n",
      "Epoch: 2142 / dLoss: 0.8215448260307312 / gLoss: 1.5543937683105469\n",
      "Epoch: 2143 / dLoss: 0.982999861240387 / gLoss: 1.7905925512313843\n",
      "Epoch: 2144 / dLoss: 0.9684963226318359 / gLoss: 1.768222689628601\n",
      "Epoch: 2145 / dLoss: 0.8325682878494263 / gLoss: 1.8233489990234375\n",
      "Epoch: 2146 / dLoss: 0.8756167888641357 / gLoss: 2.1089279651641846\n",
      "Epoch: 2147 / dLoss: 0.8284081816673279 / gLoss: 1.896673560142517\n",
      "Epoch: 2148 / dLoss: 0.9151053428649902 / gLoss: 1.542445182800293\n",
      "Epoch: 2149 / dLoss: 0.7474590539932251 / gLoss: 1.4956375360488892\n",
      "Epoch: 2150 / dLoss: 1.0855348110198975 / gLoss: 1.4777107238769531\n",
      "Epoch: 2151 / dLoss: 0.8375332951545715 / gLoss: 1.9100342988967896\n",
      "Epoch: 2152 / dLoss: 0.9029699563980103 / gLoss: 1.9629267454147339\n",
      "Epoch: 2153 / dLoss: 0.8595276474952698 / gLoss: 1.7264496088027954\n",
      "Epoch: 2154 / dLoss: 0.8562901616096497 / gLoss: 1.9792863130569458\n",
      "Epoch: 2155 / dLoss: 1.0605741739273071 / gLoss: 1.7837693691253662\n",
      "Epoch: 2156 / dLoss: 0.7799063920974731 / gLoss: 1.8270752429962158\n",
      "Epoch: 2157 / dLoss: 0.92104572057724 / gLoss: 1.5430659055709839\n",
      "Epoch: 2158 / dLoss: 0.7711964845657349 / gLoss: 1.7193347215652466\n",
      "Epoch: 2159 / dLoss: 1.064198613166809 / gLoss: 1.6592830419540405\n",
      "Epoch: 2160 / dLoss: 0.8832083344459534 / gLoss: 1.9638899564743042\n",
      "Epoch: 2161 / dLoss: 0.8917953968048096 / gLoss: 1.7078161239624023\n",
      "Epoch: 2162 / dLoss: 0.9126038551330566 / gLoss: 1.919136643409729\n",
      "Epoch: 2163 / dLoss: 0.842251181602478 / gLoss: 1.6689846515655518\n",
      "Epoch: 2164 / dLoss: 0.9282560348510742 / gLoss: 2.0227365493774414\n",
      "Epoch: 2165 / dLoss: 0.8987689018249512 / gLoss: 1.7663503885269165\n",
      "Epoch: 2166 / dLoss: 0.922059178352356 / gLoss: 1.9118320941925049\n",
      "Epoch: 2167 / dLoss: 0.855133056640625 / gLoss: 1.7210822105407715\n",
      "Epoch: 2168 / dLoss: 0.8981010913848877 / gLoss: 2.015233278274536\n",
      "Epoch: 2169 / dLoss: 0.8028720617294312 / gLoss: 1.784178614616394\n",
      "Epoch: 2170 / dLoss: 0.9191049337387085 / gLoss: 2.0454752445220947\n",
      "Epoch: 2171 / dLoss: 0.8443339467048645 / gLoss: 1.6604427099227905\n",
      "Epoch: 2172 / dLoss: 0.8164002895355225 / gLoss: 1.8948619365692139\n",
      "Epoch: 2173 / dLoss: 0.8150393962860107 / gLoss: 2.1005091667175293\n",
      "Epoch: 2174 / dLoss: 0.8607991933822632 / gLoss: 1.9547033309936523\n",
      "Epoch: 2175 / dLoss: 0.9206497073173523 / gLoss: 1.8149113655090332\n",
      "Epoch: 2176 / dLoss: 0.9646496772766113 / gLoss: 2.0089311599731445\n",
      "Epoch: 2177 / dLoss: 0.8754125833511353 / gLoss: 1.9530715942382812\n",
      "Epoch: 2178 / dLoss: 0.9505411982536316 / gLoss: 1.894987940788269\n",
      "Epoch: 2179 / dLoss: 0.7810027599334717 / gLoss: 1.6713768243789673\n",
      "Epoch: 2180 / dLoss: 0.927810788154602 / gLoss: 2.0472803115844727\n",
      "Epoch: 2181 / dLoss: 0.9651914834976196 / gLoss: 1.7529301643371582\n",
      "Epoch: 2182 / dLoss: 0.9646667242050171 / gLoss: 1.7848585844039917\n",
      "Epoch: 2183 / dLoss: 0.9006801247596741 / gLoss: 2.009584903717041\n",
      "Epoch: 2184 / dLoss: 0.7931762933731079 / gLoss: 2.070265293121338\n",
      "Epoch: 2185 / dLoss: 0.9632874727249146 / gLoss: 1.9170966148376465\n",
      "Epoch: 2186 / dLoss: 0.9638426303863525 / gLoss: 1.5273555517196655\n",
      "Epoch: 2187 / dLoss: 0.755597710609436 / gLoss: 1.6484898328781128\n",
      "Epoch: 2188 / dLoss: 0.8532201647758484 / gLoss: 1.5275921821594238\n",
      "Epoch: 2189 / dLoss: 0.8504642248153687 / gLoss: 1.6543354988098145\n",
      "Epoch: 2190 / dLoss: 0.8768205046653748 / gLoss: 1.7192018032073975\n",
      "Epoch: 2191 / dLoss: 0.8761464953422546 / gLoss: 1.787160038948059\n",
      "Epoch: 2192 / dLoss: 0.9213141202926636 / gLoss: 1.576634168624878\n",
      "Epoch: 2193 / dLoss: 0.8525654673576355 / gLoss: 1.7310328483581543\n",
      "Epoch: 2194 / dLoss: 0.7545598745346069 / gLoss: 1.9169954061508179\n",
      "Epoch: 2195 / dLoss: 0.8872913122177124 / gLoss: 1.9511147737503052\n",
      "Epoch: 2196 / dLoss: 0.8181641697883606 / gLoss: 1.837802767753601\n",
      "Epoch: 2197 / dLoss: 0.8682301044464111 / gLoss: 1.8343608379364014\n",
      "Epoch: 2198 / dLoss: 0.9066793918609619 / gLoss: 1.8080852031707764\n",
      "Epoch: 2199 / dLoss: 0.8768424987792969 / gLoss: 1.8133164644241333\n",
      "Epoch: 2200 / dLoss: 0.8846913576126099 / gLoss: 1.8947011232376099\n",
      "Epoch: 2201 / dLoss: 0.9118009209632874 / gLoss: 1.4427679777145386\n",
      "Epoch: 2202 / dLoss: 0.9209468960762024 / gLoss: 1.8189395666122437\n",
      "Epoch: 2203 / dLoss: 1.0688831806182861 / gLoss: 1.64042329788208\n",
      "Epoch: 2204 / dLoss: 0.9291291236877441 / gLoss: 1.8494045734405518\n",
      "Epoch: 2205 / dLoss: 0.8693257570266724 / gLoss: 1.716270923614502\n",
      "Epoch: 2206 / dLoss: 0.7913280725479126 / gLoss: 1.7608954906463623\n",
      "Epoch: 2207 / dLoss: 0.8601305484771729 / gLoss: 1.885535478591919\n",
      "Epoch: 2208 / dLoss: 0.7773475050926208 / gLoss: 1.735093355178833\n",
      "Epoch: 2209 / dLoss: 0.855206310749054 / gLoss: 1.8121474981307983\n",
      "Epoch: 2210 / dLoss: 0.8603395223617554 / gLoss: 1.7264106273651123\n",
      "Epoch: 2211 / dLoss: 0.8439092636108398 / gLoss: 1.6572543382644653\n",
      "Epoch: 2212 / dLoss: 0.9790337085723877 / gLoss: 2.1265664100646973\n",
      "Epoch: 2213 / dLoss: 0.882707953453064 / gLoss: 1.8620110750198364\n",
      "Epoch: 2214 / dLoss: 0.8333642482757568 / gLoss: 1.8864549398422241\n",
      "Epoch: 2215 / dLoss: 0.970039963722229 / gLoss: 1.9162691831588745\n",
      "Epoch: 2216 / dLoss: 0.9120575785636902 / gLoss: 2.0293686389923096\n",
      "Epoch: 2217 / dLoss: 0.8913754224777222 / gLoss: 1.9691705703735352\n",
      "Epoch: 2218 / dLoss: 0.9560356736183167 / gLoss: 1.692233681678772\n",
      "Epoch: 2219 / dLoss: 0.90113365650177 / gLoss: 1.8068389892578125\n",
      "Epoch: 2220 / dLoss: 0.9395012855529785 / gLoss: 1.9404090642929077\n",
      "Epoch: 2221 / dLoss: 0.8846409916877747 / gLoss: 1.6910113096237183\n",
      "Epoch: 2222 / dLoss: 0.9082292318344116 / gLoss: 1.8596947193145752\n",
      "Epoch: 2223 / dLoss: 0.8156594038009644 / gLoss: 1.93452787399292\n",
      "Epoch: 2224 / dLoss: 0.8728080987930298 / gLoss: 2.0274720191955566\n",
      "Epoch: 2225 / dLoss: 0.8924096822738647 / gLoss: 1.9678030014038086\n",
      "Epoch: 2226 / dLoss: 0.9338955879211426 / gLoss: 1.6649086475372314\n",
      "Epoch: 2227 / dLoss: 0.9286268353462219 / gLoss: 1.6796015501022339\n",
      "Epoch: 2228 / dLoss: 0.8688965439796448 / gLoss: 2.083131790161133\n",
      "Epoch: 2229 / dLoss: 1.0098109245300293 / gLoss: 1.6731584072113037\n",
      "Epoch: 2230 / dLoss: 0.8392961621284485 / gLoss: 1.7678738832473755\n",
      "Epoch: 2231 / dLoss: 0.8109335899353027 / gLoss: 1.7605500221252441\n",
      "Epoch: 2232 / dLoss: 0.9783816933631897 / gLoss: 1.7105326652526855\n",
      "Epoch: 2233 / dLoss: 0.9172358512878418 / gLoss: 2.0691938400268555\n",
      "Epoch: 2234 / dLoss: 0.8119144439697266 / gLoss: 1.8671820163726807\n",
      "Epoch: 2235 / dLoss: 0.8697285652160645 / gLoss: 1.937437891960144\n",
      "Epoch: 2236 / dLoss: 0.8248640298843384 / gLoss: 1.910534381866455\n",
      "Epoch: 2237 / dLoss: 0.911338210105896 / gLoss: 2.063934087753296\n",
      "Epoch: 2238 / dLoss: 0.8051624298095703 / gLoss: 1.6491496562957764\n",
      "Epoch: 2239 / dLoss: 0.8192786574363708 / gLoss: 2.0618162155151367\n",
      "Epoch: 2240 / dLoss: 0.8095805644989014 / gLoss: 1.9723433256149292\n",
      "Epoch: 2241 / dLoss: 0.8969852924346924 / gLoss: 1.745671033859253\n",
      "Epoch: 2242 / dLoss: 0.8214094042778015 / gLoss: 1.8897334337234497\n",
      "Epoch: 2243 / dLoss: 0.8831542134284973 / gLoss: 1.882124662399292\n",
      "Epoch: 2244 / dLoss: 0.91927170753479 / gLoss: 1.976442575454712\n",
      "Epoch: 2245 / dLoss: 0.8446792364120483 / gLoss: 1.859279751777649\n",
      "Epoch: 2246 / dLoss: 0.9694277048110962 / gLoss: 1.585964560508728\n",
      "Epoch: 2247 / dLoss: 0.839064359664917 / gLoss: 1.9844317436218262\n",
      "Epoch: 2248 / dLoss: 0.8859606981277466 / gLoss: 1.719864845275879\n",
      "Epoch: 2249 / dLoss: 0.9735457897186279 / gLoss: 1.9374226331710815\n",
      "Epoch: 2250 / dLoss: 0.9707860946655273 / gLoss: 1.6030653715133667\n",
      "Epoch: 2251 / dLoss: 0.8724926710128784 / gLoss: 1.670984148979187\n",
      "Epoch: 2252 / dLoss: 0.8355821371078491 / gLoss: 1.7494163513183594\n",
      "Epoch: 2253 / dLoss: 0.8366802930831909 / gLoss: 1.7374932765960693\n",
      "Epoch: 2254 / dLoss: 0.9153537750244141 / gLoss: 1.5676661729812622\n",
      "Epoch: 2255 / dLoss: 0.8187762498855591 / gLoss: 2.0234570503234863\n",
      "Epoch: 2256 / dLoss: 0.82680344581604 / gLoss: 1.7906492948532104\n",
      "Epoch: 2257 / dLoss: 0.9254189729690552 / gLoss: 1.8987982273101807\n",
      "Epoch: 2258 / dLoss: 0.7943975925445557 / gLoss: 1.9075740575790405\n",
      "Epoch: 2259 / dLoss: 0.8663067817687988 / gLoss: 2.212850332260132\n",
      "Epoch: 2260 / dLoss: 1.0513916015625 / gLoss: 1.820236325263977\n",
      "Epoch: 2261 / dLoss: 0.8592900037765503 / gLoss: 1.7816071510314941\n",
      "Epoch: 2262 / dLoss: 0.8975527286529541 / gLoss: 2.0711510181427\n",
      "Epoch: 2263 / dLoss: 0.8612790107727051 / gLoss: 1.5885788202285767\n",
      "Epoch: 2264 / dLoss: 0.8816109895706177 / gLoss: 1.9471796751022339\n",
      "Epoch: 2265 / dLoss: 0.9410582780838013 / gLoss: 1.7888728380203247\n",
      "Epoch: 2266 / dLoss: 0.8324597477912903 / gLoss: 1.8794540166854858\n",
      "Epoch: 2267 / dLoss: 0.865302324295044 / gLoss: 2.0493733882904053\n",
      "Epoch: 2268 / dLoss: 0.8620169162750244 / gLoss: 1.931401252746582\n",
      "Epoch: 2269 / dLoss: 0.822931170463562 / gLoss: 1.8777269124984741\n",
      "Epoch: 2270 / dLoss: 0.8152199387550354 / gLoss: 1.7835081815719604\n",
      "Epoch: 2271 / dLoss: 0.8554141521453857 / gLoss: 1.9434568881988525\n",
      "Epoch: 2272 / dLoss: 0.8558860421180725 / gLoss: 1.8832767009735107\n",
      "Epoch: 2273 / dLoss: 0.8924239873886108 / gLoss: 1.976933240890503\n",
      "Epoch: 2274 / dLoss: 0.850458025932312 / gLoss: 1.5900816917419434\n",
      "Epoch: 2275 / dLoss: 0.9075250625610352 / gLoss: 1.9751166105270386\n",
      "Epoch: 2276 / dLoss: 0.9243412017822266 / gLoss: 1.4731210470199585\n",
      "Epoch: 2277 / dLoss: 0.8971835374832153 / gLoss: 1.778834342956543\n",
      "Epoch: 2278 / dLoss: 0.7892301082611084 / gLoss: 1.9669109582901\n",
      "Epoch: 2279 / dLoss: 0.8979106545448303 / gLoss: 1.8584544658660889\n",
      "Epoch: 2280 / dLoss: 0.8724126219749451 / gLoss: 1.91005539894104\n",
      "Epoch: 2281 / dLoss: 0.8408544063568115 / gLoss: 1.7375688552856445\n",
      "Epoch: 2282 / dLoss: 0.8956651091575623 / gLoss: 1.7954150438308716\n",
      "Epoch: 2283 / dLoss: 0.8746592402458191 / gLoss: 1.995438814163208\n",
      "Epoch: 2284 / dLoss: 0.8920420408248901 / gLoss: 1.943421483039856\n",
      "Epoch: 2285 / dLoss: 0.905212938785553 / gLoss: 1.9129481315612793\n",
      "Epoch: 2286 / dLoss: 0.8597602844238281 / gLoss: 1.6765458583831787\n",
      "Epoch: 2287 / dLoss: 0.9034897089004517 / gLoss: 1.9982593059539795\n",
      "Epoch: 2288 / dLoss: 0.8328137993812561 / gLoss: 1.7012648582458496\n",
      "Epoch: 2289 / dLoss: 0.8748637437820435 / gLoss: 1.741787314414978\n",
      "Epoch: 2290 / dLoss: 0.9245659112930298 / gLoss: 1.9658515453338623\n",
      "Epoch: 2291 / dLoss: 0.8695459365844727 / gLoss: 1.9388563632965088\n",
      "Epoch: 2292 / dLoss: 0.8432424068450928 / gLoss: 1.9468046426773071\n",
      "Epoch: 2293 / dLoss: 0.8344058990478516 / gLoss: 1.408819556236267\n",
      "Epoch: 2294 / dLoss: 0.9542975425720215 / gLoss: 1.9895176887512207\n",
      "Epoch: 2295 / dLoss: 0.899636447429657 / gLoss: 2.0098037719726562\n",
      "Epoch: 2296 / dLoss: 0.7966339588165283 / gLoss: 2.0021026134490967\n",
      "Epoch: 2297 / dLoss: 0.8287220597267151 / gLoss: 1.8507673740386963\n",
      "Epoch: 2298 / dLoss: 0.8946762681007385 / gLoss: 1.732266902923584\n",
      "Epoch: 2299 / dLoss: 0.9133644104003906 / gLoss: 1.7706892490386963\n",
      "Epoch: 2300 / dLoss: 0.9034179449081421 / gLoss: 1.9454046487808228\n",
      "Epoch: 2301 / dLoss: 0.7095308303833008 / gLoss: 2.1006617546081543\n",
      "Epoch: 2302 / dLoss: 0.8349119424819946 / gLoss: 1.9646209478378296\n",
      "Epoch: 2303 / dLoss: 0.9798413515090942 / gLoss: 1.8072117567062378\n",
      "Epoch: 2304 / dLoss: 0.7910206317901611 / gLoss: 1.820518970489502\n",
      "Epoch: 2305 / dLoss: 0.8282008767127991 / gLoss: 1.9405882358551025\n",
      "Epoch: 2306 / dLoss: 0.8167417645454407 / gLoss: 1.536409616470337\n",
      "Epoch: 2307 / dLoss: 0.8573364019393921 / gLoss: 1.743552803993225\n",
      "Epoch: 2308 / dLoss: 0.7764376997947693 / gLoss: 1.9507359266281128\n",
      "Epoch: 2309 / dLoss: 0.8616852760314941 / gLoss: 1.9228936433792114\n",
      "Epoch: 2310 / dLoss: 0.8260778784751892 / gLoss: 1.8100786209106445\n",
      "Epoch: 2311 / dLoss: 0.8577002286911011 / gLoss: 1.5133306980133057\n",
      "Epoch: 2312 / dLoss: 0.9858808517456055 / gLoss: 1.7512582540512085\n",
      "Epoch: 2313 / dLoss: 1.0029289722442627 / gLoss: 1.6811572313308716\n",
      "Epoch: 2314 / dLoss: 0.8763526678085327 / gLoss: 1.8497875928878784\n",
      "Epoch: 2315 / dLoss: 0.7692604660987854 / gLoss: 1.8491207361221313\n",
      "Epoch: 2316 / dLoss: 0.9018985629081726 / gLoss: 1.7478547096252441\n",
      "Epoch: 2317 / dLoss: 0.8401055932044983 / gLoss: 1.644738793373108\n",
      "Epoch: 2318 / dLoss: 0.8551642894744873 / gLoss: 1.8420275449752808\n",
      "Epoch: 2319 / dLoss: 0.9118438959121704 / gLoss: 1.8860244750976562\n",
      "Epoch: 2320 / dLoss: 0.860031247138977 / gLoss: 1.6579867601394653\n",
      "Epoch: 2321 / dLoss: 0.847213864326477 / gLoss: 2.2209513187408447\n",
      "Epoch: 2322 / dLoss: 0.8615631461143494 / gLoss: 1.7098780870437622\n",
      "Epoch: 2323 / dLoss: 0.8541406989097595 / gLoss: 1.7429592609405518\n",
      "Epoch: 2324 / dLoss: 0.9270288944244385 / gLoss: 1.6073803901672363\n",
      "Epoch: 2325 / dLoss: 0.8687805533409119 / gLoss: 1.7444146871566772\n",
      "Epoch: 2326 / dLoss: 0.8548794984817505 / gLoss: 1.9442079067230225\n",
      "Epoch: 2327 / dLoss: 0.9182764291763306 / gLoss: 1.6983747482299805\n",
      "Epoch: 2328 / dLoss: 0.8621934652328491 / gLoss: 1.798719882965088\n",
      "Epoch: 2329 / dLoss: 0.7689861059188843 / gLoss: 1.78923761844635\n",
      "Epoch: 2330 / dLoss: 0.8824525475502014 / gLoss: 1.7303122282028198\n",
      "Epoch: 2331 / dLoss: 0.9542577266693115 / gLoss: 1.7691682577133179\n",
      "Epoch: 2332 / dLoss: 0.8623161315917969 / gLoss: 1.7984864711761475\n",
      "Epoch: 2333 / dLoss: 0.9587222337722778 / gLoss: 1.953192949295044\n",
      "Epoch: 2334 / dLoss: 0.9347193241119385 / gLoss: 1.947878122329712\n",
      "Epoch: 2335 / dLoss: 0.897577702999115 / gLoss: 2.0807573795318604\n",
      "Epoch: 2336 / dLoss: 0.8596493601799011 / gLoss: 1.954088568687439\n",
      "Epoch: 2337 / dLoss: 0.8312877416610718 / gLoss: 1.9628510475158691\n",
      "Epoch: 2338 / dLoss: 0.9461287260055542 / gLoss: 1.9452147483825684\n",
      "Epoch: 2339 / dLoss: 0.9232038259506226 / gLoss: 1.7325172424316406\n",
      "Epoch: 2340 / dLoss: 0.9141079783439636 / gLoss: 2.126755952835083\n",
      "Epoch: 2341 / dLoss: 0.7932140827178955 / gLoss: 2.0321249961853027\n",
      "Epoch: 2342 / dLoss: 0.8546805381774902 / gLoss: 2.08770489692688\n",
      "Epoch: 2343 / dLoss: 0.8978068828582764 / gLoss: 1.793358325958252\n",
      "Epoch: 2344 / dLoss: 0.8736945390701294 / gLoss: 1.7400619983673096\n",
      "Epoch: 2345 / dLoss: 0.9966261386871338 / gLoss: 1.9497425556182861\n",
      "Epoch: 2346 / dLoss: 0.9268851280212402 / gLoss: 1.7102259397506714\n",
      "Epoch: 2347 / dLoss: 0.7975002527236938 / gLoss: 2.145451545715332\n",
      "Epoch: 2348 / dLoss: 0.9257493019104004 / gLoss: 1.7872735261917114\n",
      "Epoch: 2349 / dLoss: 0.945460319519043 / gLoss: 1.6586813926696777\n",
      "Epoch: 2350 / dLoss: 0.8262571096420288 / gLoss: 2.021000623703003\n",
      "Epoch: 2351 / dLoss: 0.8113099336624146 / gLoss: 1.721105933189392\n",
      "Epoch: 2352 / dLoss: 0.7936986684799194 / gLoss: 1.9296149015426636\n",
      "Epoch: 2353 / dLoss: 0.95196533203125 / gLoss: 1.79557204246521\n",
      "Epoch: 2354 / dLoss: 0.8467200398445129 / gLoss: 1.7297204732894897\n",
      "Epoch: 2355 / dLoss: 0.8110220432281494 / gLoss: 1.8210117816925049\n",
      "Epoch: 2356 / dLoss: 0.8666553497314453 / gLoss: 1.7916783094406128\n",
      "Epoch: 2357 / dLoss: 0.7774561047554016 / gLoss: 1.9478687047958374\n",
      "Epoch: 2358 / dLoss: 0.9090186357498169 / gLoss: 2.018054246902466\n",
      "Epoch: 2359 / dLoss: 0.8900412321090698 / gLoss: 1.928544521331787\n",
      "Epoch: 2360 / dLoss: 0.7171630859375 / gLoss: 1.8446186780929565\n",
      "Epoch: 2361 / dLoss: 0.9318826198577881 / gLoss: 2.0687859058380127\n",
      "Epoch: 2362 / dLoss: 0.9386457800865173 / gLoss: 1.6879464387893677\n",
      "Epoch: 2363 / dLoss: 0.840141773223877 / gLoss: 1.9524693489074707\n",
      "Epoch: 2364 / dLoss: 0.8825337290763855 / gLoss: 1.9609042406082153\n",
      "Epoch: 2365 / dLoss: 0.8888855576515198 / gLoss: 1.5833264589309692\n",
      "Epoch: 2366 / dLoss: 0.8429374694824219 / gLoss: 1.8352488279342651\n",
      "Epoch: 2367 / dLoss: 1.0124952793121338 / gLoss: 1.505813717842102\n",
      "Epoch: 2368 / dLoss: 0.8867248296737671 / gLoss: 2.139727830886841\n",
      "Epoch: 2369 / dLoss: 0.7639532089233398 / gLoss: 1.892058253288269\n",
      "Epoch: 2370 / dLoss: 0.9211348295211792 / gLoss: 1.981978416442871\n",
      "Epoch: 2371 / dLoss: 0.9313029050827026 / gLoss: 1.8465958833694458\n",
      "Epoch: 2372 / dLoss: 0.9530791640281677 / gLoss: 2.104179620742798\n",
      "Epoch: 2373 / dLoss: 0.7961022853851318 / gLoss: 1.9037349224090576\n",
      "Epoch: 2374 / dLoss: 0.7745081782341003 / gLoss: 2.00475811958313\n",
      "Epoch: 2375 / dLoss: 0.9477039575576782 / gLoss: 1.7651963233947754\n",
      "Epoch: 2376 / dLoss: 0.955204963684082 / gLoss: 1.881088376045227\n",
      "Epoch: 2377 / dLoss: 0.8558183908462524 / gLoss: 2.021777868270874\n",
      "Epoch: 2378 / dLoss: 0.7897661924362183 / gLoss: 2.1645772457122803\n",
      "Epoch: 2379 / dLoss: 0.9482043981552124 / gLoss: 1.9690660238265991\n",
      "Epoch: 2380 / dLoss: 1.0020695924758911 / gLoss: 2.086296796798706\n",
      "Epoch: 2381 / dLoss: 0.8434771299362183 / gLoss: 1.9950698614120483\n",
      "Epoch: 2382 / dLoss: 0.7844499349594116 / gLoss: 1.866084337234497\n",
      "Epoch: 2383 / dLoss: 0.8122265338897705 / gLoss: 2.2950196266174316\n",
      "Epoch: 2384 / dLoss: 0.7453023195266724 / gLoss: 2.3396904468536377\n",
      "Epoch: 2385 / dLoss: 0.7896022796630859 / gLoss: 1.9521992206573486\n",
      "Epoch: 2386 / dLoss: 0.8893269896507263 / gLoss: 1.918244481086731\n",
      "Epoch: 2387 / dLoss: 0.9229015111923218 / gLoss: 2.1311981678009033\n",
      "Epoch: 2388 / dLoss: 0.8892033100128174 / gLoss: 1.8000662326812744\n",
      "Epoch: 2389 / dLoss: 0.8463298678398132 / gLoss: 1.863570213317871\n",
      "Epoch: 2390 / dLoss: 0.8325585722923279 / gLoss: 1.8246768712997437\n",
      "Epoch: 2391 / dLoss: 0.9872338771820068 / gLoss: 1.702864408493042\n",
      "Epoch: 2392 / dLoss: 0.8824169635772705 / gLoss: 2.1525254249572754\n",
      "Epoch: 2393 / dLoss: 0.8709001541137695 / gLoss: 1.68315851688385\n",
      "Epoch: 2394 / dLoss: 0.8118822574615479 / gLoss: 1.9493728876113892\n",
      "Epoch: 2395 / dLoss: 0.9434428215026855 / gLoss: 2.074958562850952\n",
      "Epoch: 2396 / dLoss: 0.8408260345458984 / gLoss: 1.6370141506195068\n",
      "Epoch: 2397 / dLoss: 0.7445449829101562 / gLoss: 1.88747239112854\n",
      "Epoch: 2398 / dLoss: 0.8741598725318909 / gLoss: 1.9838427305221558\n",
      "Epoch: 2399 / dLoss: 0.8992922306060791 / gLoss: 2.213315486907959\n",
      "Epoch: 2400 / dLoss: 0.9262254238128662 / gLoss: 1.9833979606628418\n",
      "Epoch: 2401 / dLoss: 0.9678502082824707 / gLoss: 1.8428186178207397\n",
      "Epoch: 2402 / dLoss: 0.8767364025115967 / gLoss: 2.0758657455444336\n",
      "Epoch: 2403 / dLoss: 0.8551428318023682 / gLoss: 1.9647340774536133\n",
      "Epoch: 2404 / dLoss: 0.8375089168548584 / gLoss: 1.9147125482559204\n",
      "Epoch: 2405 / dLoss: 0.9115181565284729 / gLoss: 2.067685604095459\n",
      "Epoch: 2406 / dLoss: 0.9159500598907471 / gLoss: 2.075810194015503\n",
      "Epoch: 2407 / dLoss: 0.8341091871261597 / gLoss: 1.715128779411316\n",
      "Epoch: 2408 / dLoss: 0.8560335636138916 / gLoss: 1.7540876865386963\n",
      "Epoch: 2409 / dLoss: 1.0044209957122803 / gLoss: 1.9987295866012573\n",
      "Epoch: 2410 / dLoss: 0.8696286082267761 / gLoss: 1.910502314567566\n",
      "Epoch: 2411 / dLoss: 0.9327734708786011 / gLoss: 2.190372943878174\n",
      "Epoch: 2412 / dLoss: 0.6556186079978943 / gLoss: 1.7955844402313232\n",
      "Epoch: 2413 / dLoss: 0.9041610956192017 / gLoss: 1.9657052755355835\n",
      "Epoch: 2414 / dLoss: 0.8897740244865417 / gLoss: 1.9786477088928223\n",
      "Epoch: 2415 / dLoss: 0.8769077062606812 / gLoss: 1.7452735900878906\n",
      "Epoch: 2416 / dLoss: 0.9371855854988098 / gLoss: 1.7408766746520996\n",
      "Epoch: 2417 / dLoss: 0.7125709056854248 / gLoss: 1.8364098072052002\n",
      "Epoch: 2418 / dLoss: 0.9837803840637207 / gLoss: 1.9843285083770752\n",
      "Epoch: 2419 / dLoss: 0.7702318429946899 / gLoss: 2.2143077850341797\n",
      "Epoch: 2420 / dLoss: 0.7718077301979065 / gLoss: 1.8517036437988281\n",
      "Epoch: 2421 / dLoss: 0.7254789471626282 / gLoss: 2.2570457458496094\n",
      "Epoch: 2422 / dLoss: 0.8253451585769653 / gLoss: 1.9522038698196411\n",
      "Epoch: 2423 / dLoss: 0.8156911134719849 / gLoss: 1.867547631263733\n",
      "Epoch: 2424 / dLoss: 0.7514796257019043 / gLoss: 1.9625948667526245\n",
      "Epoch: 2425 / dLoss: 0.7913417816162109 / gLoss: 1.9744062423706055\n",
      "Epoch: 2426 / dLoss: 0.8465313911437988 / gLoss: 2.3629648685455322\n",
      "Epoch: 2427 / dLoss: 0.8438553214073181 / gLoss: 2.1316003799438477\n",
      "Epoch: 2428 / dLoss: 1.0698981285095215 / gLoss: 2.3250768184661865\n",
      "Epoch: 2429 / dLoss: 0.8162705898284912 / gLoss: 2.2434520721435547\n",
      "Epoch: 2430 / dLoss: 0.8766969442367554 / gLoss: 1.9522392749786377\n",
      "Epoch: 2431 / dLoss: 0.8976558446884155 / gLoss: 2.0404255390167236\n",
      "Epoch: 2432 / dLoss: 0.8357307314872742 / gLoss: 1.7959434986114502\n",
      "Epoch: 2433 / dLoss: 0.8772501945495605 / gLoss: 1.884705662727356\n",
      "Epoch: 2434 / dLoss: 0.8801878094673157 / gLoss: 2.1660497188568115\n",
      "Epoch: 2435 / dLoss: 0.8519326448440552 / gLoss: 1.8132357597351074\n",
      "Epoch: 2436 / dLoss: 0.9030887484550476 / gLoss: 2.0392379760742188\n",
      "Epoch: 2437 / dLoss: 0.9527804255485535 / gLoss: 2.194584369659424\n",
      "Epoch: 2438 / dLoss: 0.809137761592865 / gLoss: 2.1991591453552246\n",
      "Epoch: 2439 / dLoss: 0.8604616522789001 / gLoss: 1.8315072059631348\n",
      "Epoch: 2440 / dLoss: 0.8784810304641724 / gLoss: 2.105837106704712\n",
      "Epoch: 2441 / dLoss: 0.7682657241821289 / gLoss: 1.9415611028671265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/zhongsheng/anaconda2/envs/ganRegression/lib/python3.6/site-packages/keras/engine/training.py:493: UserWarning:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    }
   ],
   "source": [
    "regcgan = reg_cgan_model.RegCGAN(exp_config)\n",
    "d_loss_err, d_loss_true, d_loss_fake, g_loss_err, g_pred, g_true = regcgan.train(X_train, y_train, \n",
    "                                                                              epochs=exp_config.training.n_epochs,\n",
    "                                                                              batch_size=exp_config.training.batch_size,\n",
    "                                                                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotting.plots(d_loss_err, d_loss_true, d_loss_fake, g_loss_err, g_pred, g_true, fig_dir, exp_config.run.save_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CVT samples: taking as input parts of a paired virtual  samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "X_CVT = np.load(f\"{fig_dir}/{basename(fig_dir)}_CVT_samples.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% True y for CVT inputs\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from dataset import _magical_sinus\n",
    "y = _magical_sinus(X_CVT[:,0], X_CVT[:,1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ypred_single_cgan_cvt = cgan.predict(X_CVT)\n",
    "ypred_mean_cgan_cvt, _, _ = cgan.sample(X_CVT, exp_config.training.n_samples)\n",
    "ypred_gp_cvt, cov_cvt = gpr.predict(X_CVT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Predictions using single or mean of CGAN along with GP\n",
    "plotting.plot_ypred_joint(X_CVT, y, ypred_single_cgan_cvt, ypred_mean_cgan_cvt, ypred_gp_cvt,\n",
    "                          fig_dir=fig_dir, prefix=\"single_mean_cgan_gp\", save_fig=exp_config.run.save_fig,\n",
    "                          alpha=0.5, elevation=30, azimuth=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalized density of P(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plotting.plot_densities_joint(y, ypred_single_cgan_cvt, ypred_mean_cgan_cvt, ypred_gp_cvt,\n",
    "                              title=r\"Marginalized $P(y)$\", fig_dir=fig_dir,\n",
    "                              prefix=\"marginalized_P(y)\", save_fig=exp_config.run.save_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from P(y|x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_sampling = 200 # increase sample_size to have gaussian densities for GP\n",
    "ypred_mean_cgan, _, ypred_mean_cgan_sampling = cgan.sample(X_CVT, num_sampling)\n",
    "\n",
    "ypred_gp, cov = gpr.predict(X_CVT)\n",
    "ypred_gp_sampling = np.random.normal(ypred_gp, np.sqrt(cov))\n",
    "for i in range(1, num_sampling):\n",
    "    ypred_gp_sampling = np.hstack([ypred_gp_sampling, np.random.normal(ypred_gp, np.sqrt(cov))])\n",
    "\n",
    "ypred_single_cgan_sampling = cgan.predict(X_CVT)\n",
    "for i in range(1, num_sampling):\n",
    "    ypred_single_cgan_sampling = np.hstack([ypred_single_cgan_sampling, cgan.predict(X_CVT)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "bins =[[0.12, 0.36], [0.49, 0.64], [0.13, 0.53]]\n",
    "for (x1, x2) in bins:\n",
    "    y_true = dataset.get_true_y_given_x(given_x1=x1, given_x2=x2,\n",
    "                                        X=np.tile(X_CVT, (num_sampling, 1)),\n",
    "                                        y=np.tile(y, (num_sampling, 1)))\n",
    "\n",
    "    ypred_single_cgan_cond = dataset.get_true_y_given_x(given_x1=x1, given_x2=x2,\n",
    "                                        X=np.tile(X_CVT, (num_sampling, 1)),\n",
    "                                        y=np.ravel(ypred_single_cgan_sampling.T).reshape(-1, 1))\n",
    "\n",
    "    ypred_mean_cgan_cond = dataset.get_true_y_given_x(given_x1=x1, given_x2=x2,\n",
    "                                        X=np.tile(X_CVT, (num_sampling, 1)),\n",
    "                                        y=np.ravel(ypred_mean_cgan_sampling.T).reshape(-1, 1))\n",
    "\n",
    "    ypred_gp_cond = dataset.get_true_y_given_x(given_x1=x1, given_x2=x2,\n",
    "                                        X=np.tile(X_CVT, (num_sampling, 1)),\n",
    "                                        y=np.ravel(ypred_gp_sampling.T).reshape(-1, 1))\n",
    "\n",
    "    (y_true, ypred_mean_cgan_cond, ypred_gp_cond) = (y_true.reshape(-1, 1),\n",
    "                                                      ypred_mean_cgan_cond.reshape(-1, 1),\n",
    "                                                      ypred_gp_cond.reshape(-1, 1))\n",
    "\n",
    "#     plotting.plot_densities_joint(y_true.reshape(-1,1),\n",
    "#                                   ypred_single_cgan_cond.reshape(-1, 1),\n",
    "#                                   ypred_mean_cgan_cond.reshape(-1, 1),\n",
    "#                                   ypred_gp_cond.reshape(-1, 1),\n",
    "#                                   title = f\"conditional density of $P(y|x_1={x1},x_2={x2})$ \", fig_dir=fig_dir,\n",
    "#                                   prefix=f\"condition_density_P(y|x1={x1},x2={x2})\", save_fig=exp_config.run.save_fig)\n",
    "    plotting.plot_densities_joint(y_true.reshape(-1,1),\n",
    "                                      ypred_single_cgan_cond.reshape(-1, 1),\n",
    "                                      ypred_mean_cgan_cond.reshape(-1, 1),\n",
    "                                      ypred_gp_cond.reshape(-1, 1),\n",
    "                                      title = f\"conditional density of $P(y|x_1={x1},x_2={x2})$ \", fig_dir=fig_dir,\n",
    "                                      prefix=f\"condition_density_P(y|x1={x1},x2={x2})\", save_fig=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "single_cgan_mae_list = []\n",
    "single_cgan_mse_list= []\n",
    "\n",
    "mean_cgan_mae_list = []\n",
    "mean_cgan_mse_list = []\n",
    "\n",
    "gp_mae_list = []\n",
    "gp_mse_list =[]\n",
    "\n",
    "n_eval_runs = 50\n",
    "for i in range(n_eval_runs):\n",
    "    ypred_single_cgan = cgan.predict(X_CVT)\n",
    "    single_cgan_mae_list.append(mean_absolute_error(y, ypred_single_cgan))\n",
    "    single_cgan_mse_list.append(mean_squared_error(y, ypred_single_cgan))\n",
    "\n",
    "    ypred_mean_cgan, _, _ = cgan.sample(X_CVT, exp_config.training.n_samples)\n",
    "    mean_cgan_mae_list.append(mean_absolute_error(y, ypred_mean_cgan))\n",
    "    mean_cgan_mse_list.append(mean_squared_error(y, ypred_mean_cgan))\n",
    "\n",
    "    yped_gp = np.random.normal(ypred_gp_cvt, np.sqrt(cov_cvt))\n",
    "    gp_mae_list.append(mean_absolute_error(y, yped_gp))\n",
    "    gp_mse_list.append(mean_squared_error(y, yped_gp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "single_cgan_mae_mean, single_cgan_mae_std = np.mean(np.asarray(single_cgan_mae_list)), np.std(np.asarray(single_cgan_mae_list))\n",
    "single_cgan_mse_mean, single_cgan_mse_std = np.mean(np.asarray(single_cgan_mse_list)), np.std(np.asarray(single_cgan_mse_list))\n",
    "\n",
    "print(f\"Single CGAN validation MAE: {single_cgan_mae_mean:.3f} +- {single_cgan_mae_std:.5f}\")\n",
    "print(f\"Single CGAN validation MSE: {single_cgan_mse_mean:.3f} +- {single_cgan_mse_std:.5f}\")\n",
    "\n",
    "mean_cgan_mae_mean, mean_cgan_mae_std = np.mean(np.asarray(mean_cgan_mae_list)), np.std(np.asarray(mean_cgan_mae_list))\n",
    "mean_cgan_mse_mean, mean_cgan_mse_std = np.mean(np.asarray(mean_cgan_mse_list)), np.std(np.asarray(mean_cgan_mse_list))\n",
    "\n",
    "print(f\"Mean CGAN validation MAE: {mean_cgan_mae_mean:.3f} +- {mean_cgan_mae_std:.5f}\")\n",
    "print(f\"Mean CGAN validation MSE: {mean_cgan_mse_mean:.3f} +- {mean_cgan_mse_std:.5f}\")\n",
    "\n",
    "gp_mae_mean, gp_mae_std = np.mean(np.asarray(gp_mae_list)), np.std(np.asarray(gp_mae_list))\n",
    "gp_mse_mean, gp_mse_std = np.mean(np.asarray(gp_mse_list)), np.std(np.asarray(gp_mse_list))\n",
    "print(f\"GP validation MAE: {gp_mae_mean:.3f} +- {gp_mae_std:.5f}\")\n",
    "print(f\"GP validation MSE: {gp_mse_mean:.3f} +- {gp_mse_std:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nlpd_list = []\n",
    "for i in range(n_eval_runs):\n",
    "    nlpd_list.append(metrics.gaussian_NLPD(y, np.random.normal(ypred_gp_cvt, np.sqrt(cov_cvt)), cov_cvt))\n",
    "gp_nlpd_mean = np.mean(nlpd_list)\n",
    "gp_nlpd_std = np.std(nlpd_list)\n",
    "print(f\"GP validation NLPD: {gp_nlpd_mean:.3f} +- {gp_nlpd_std:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nlpd_list = []\n",
    "for i in range(n_eval_runs):\n",
    "    ypred_single_cgan_cvt = cgan.predict(X_CVT)\n",
    "    cov = np.ones(len(ypred_single_cgan_cvt))* np.mean((y - ypred_single_cgan_cvt)**2)\n",
    "    nlpd_list.append(metrics.gaussian_NLPD(y, ypred_single_cgan_cvt, cov))\n",
    "single_cgan_nlpd_mean = np.mean(nlpd_list)\n",
    "single_cgan_nlpd_std = np.std(nlpd_list)\n",
    "print(f\"Single CGAN Validation NLPD: {single_cgan_nlpd_mean:.3f} +- {single_cgan_nlpd_std:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_, w, _ = metrics.Parzen(cgan, X_train, y_train)\n",
    "nlpd_list = []\n",
    "for i in range(n_eval_runs):\n",
    "    nlpd_list.append(metrics.Parzen_test(cgan, X_CVT, y, w, exp_config.training.n_samples))\n",
    "mean_cgan_nlpd_mean = np.mean(nlpd_list)\n",
    "mean_cgan_nlpd_std = np.std(nlpd_list)\n",
    "print(f\"Mean CGAN Validation NLPD: {mean_cgan_nlpd_mean:.3f} +- {mean_cgan_nlpd_std:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if exp_config.run.save_fig:\n",
    "\n",
    "    try:\n",
    "        file = open(f\"{fig_dir}/metrics_on_CVT.txt\",\"w\")\n",
    "    except FileExistsError:\n",
    "        print(f\" {fig_dir} was failed to create.\")\n",
    "\n",
    "    file.write(f\"===Validation MAE===\\n\")\n",
    "    file.write(f\"GP validation MAE: {gp_mae_mean} +- {gp_mae_std}\\n\")\n",
    "    file.write(f\"Single CGAN validation MAE: {single_cgan_mae_mean} +- {single_cgan_mae_std}\\n\")\n",
    "    file.write(f\"Mean CGAN validation MAE: {mean_cgan_mae_mean} +- {mean_cgan_mae_std}\\n\")\n",
    "    file.write(f\"===Validation MSE===\\n\")\n",
    "    file.write(f\"GP validation MSE: {gp_mse_mean} +- {gp_mse_std}\\n\")\n",
    "    file.write(f\"Single CGAN validation MSE: {single_cgan_mse_mean} +- {single_cgan_mse_std}\\n\")\n",
    "    file.write(f\"CGAN validation MSE: {mean_cgan_mse_mean} +- {mean_cgan_mse_std}\\n\")\n",
    "    file.write(f\"===Validation NLPD===\\n\")\n",
    "    file.write(f\"GP Gaussian NLPD: {gp_nlpd_mean} +- {gp_nlpd_std}\\n\")\n",
    "    file.write(f\"Single CGAN NLPD: {single_cgan_nlpd_mean} +- {single_cgan_nlpd_std}\\n\")\n",
    "    file.write(f\"Mean CGAN NLPD: {mean_cgan_nlpd_mean} +- {mean_cgan_nlpd_std}\\n\")\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ganRegression)",
   "language": "python",
   "name": "pycharm-25d74700"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}